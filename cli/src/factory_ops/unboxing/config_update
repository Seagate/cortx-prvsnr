#!/bin/sh
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#



# Functions in this file address following:
#   1. Updates /root/.ssh/config file 
#   2. Update cluster.sls with hostnames obtained for node-1 and node-2 
#   3. Update /etc/salt/minion for hostname
#   4. Start rabbitmq cluster   <= Currently handled in init
set -euE

export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/unboxing_config_update.log}"
mkdir -p $(dirname "${LOG_FILE}")
#truncate -s 0 ${LOG_FILE}

BASEDIR=$(dirname "${BASH_SOURCE}")

. ${BASEDIR}/../../common_utils/utility_scripts.sh
. ${BASEDIR}/../../common_utils/functions.sh


PRVSNR_ROOT="/opt/seagate/cortx/provisioner"
salt_opts="--no-color --out-file=${LOG_FILE} --out-file-append"

function trap_handler {
    echo "***** ERROR! *****"
    echo "For detailed error logs, please see: $LOG_FILE"
    echo "******************"
}
trap trap_handler ERR

export gfs_vol_prvsnr_data="${gfs_vol_prvsnr_data:-"volume_prvsnr_data"}"
export gfs_vol_salt_cache_jobs="${gfs_vol_salt_cache_jobs:-"volume_salt_cache_jobs"}"
export mount_dir_salt_cache="${mount_dir_salt_cache:-"/srv/glusterfs/volume_salt_cache_jobs"}"
export mountpt_salt_cache_vol="${mountpt_salt_cache_vol:-"/var/cache/salt/master/jobs"}"
export mountpt_prvsnr_data_vol="${mountpt_prvsnr_data_vol:-"/var/lib/seagate/cortx/provisioner/shared"}"
export mount_dir_prvsnr_data="${mount_dir_prvsnr_data:-"/srv/glusterfs/volume_prvsnr_data"}"


pvt_ip_a=$(get_pillar_data cluster:srvnode-1:network:data:private_ip)
pvt_ip_b=$(get_pillar_data cluster:srvnode-2:network:data:private_ip)


function update_ssh_config {
    if [[ "srvnode-1" == "$(cat /etc/salt/minion_id)" ]]; then
        echo -n "Updating server A hostname in ssh config file of server A..................." 2>&1|tee -a ${LOG_FILE}
        # Replace node-1 entry
        local primary_host=$(hostname)
        # echo ${primary_host}
        local line_to_replace=$(grep -m1 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-1.*|Host srvnode-1 ${primary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${primary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}

        # Replace node-2 entry
        echo "Private n/w IP (Server B): $pvt_ip_b" | tee -a ${LOG_FILE}
        echo -n "Updating server B hostname in ssh config file of server A..................." | tee -a ${LOG_FILE}
        local secondary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_b} "hostname")
        # echo ${secondary_host}
        local line_to_replace=$(grep -m2 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-2.*|Host srvnode-2 ${secondary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${secondary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}
    else
        echo "Private n/w IP (Server A): $pvt_ip_a" | tee -a ${LOG_FILE}
        echo -n "Updating server B hostname in ssh config file of server B..................." 2>&1|tee -a ${LOG_FILE}
        # Replace node-1 entry
        local primary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_a} "hostname")
        # echo ${primary_host}
        local line_to_replace=$(grep -m1 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-1.*|Host srvnode-1 ${primary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${primary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}

        # Replace node-2 entry
        echo -n "Updating server A hostname in ssh config file of server B..................." 2>&1|tee -a ${LOG_FILE}
        local secondary_host=$(hostname)
        # echo ${secondary_host}
        local line_to_replace=$(grep -m2 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-2.*|Host srvnode-2 ${secondary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${secondary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}
    fi
}

function stop_salt_services() {
    echo -n "Stoping salt-minion on Server A.........................................." |tee -a ${LOG_FILE}
    systemctl stop salt-minion
    echo "Ok." | tee -a ${LOG_FILE}

    echo -n "Stoping salt-minion on Server B.........................................." |tee -a ${LOG_FILE}
    ${ssh_cmd} ${pvt_ip_b} "systemctl stop salt-minion"
    echo "Ok." | tee -a ${LOG_FILE}

    sleep 5

    echo -n "Stoping salt-master on Server A.........................................." 2>&1 | tee -a ${LOG_FILE}
    systemctl stop salt-master
    echo "Ok." | tee -a ${LOG_FILE}

    echo -n "Stoping salt-master on Server B.........................................." 2>&1|tee -a ${LOG_FILE}
    ${ssh_cmd} ${pvt_ip_b} "systemctl stop salt-master"
    echo "Ok." | tee -a ${LOG_FILE}
}

function configure_gluster_vols() {

    #1. Add peer
    #2. create bricks
    #3. start volume
    #4. mount volume
    #5. add entry in fstab

    echo "Stopping the Salt services before updating gluster configuration" | tee -a ${LOG_FILE}
    stop_salt_services

    echo "Adding peer in gluster configuration" | tee -a ${LOG_FILE}
    if ! gluster peer status | grep ${hostname_B}; then
        echo "adding gluser peer" >> ${LOG_FILE}
        echo y | gluster peer probe ${hostname_B} 2>&1 | tee -a ${LOG_FILE}
        sleep 5
    else
        echo "peer ${hostname_B} already added in gluser configuration" >> ${LOG_FILE}
    fi

    #volume_salt_cache_jobs:
    echo -e "\nCreating gluster volume brick for ${gfs_vol_salt_cache_jobs}" | tee -a ${LOG_FILE}
    if ! gluster volume list | grep ${gfs_vol_salt_cache_jobs}; then
        echo "creating gluster volume ${gfs_vol_salt_cache_jobs}" >> ${LOG_FILE}
        echo y | gluster volume create ${gfs_vol_salt_cache_jobs} replica 2 ${hostname_A}:${mount_dir_salt_cache} ${hostname_B}:${mount_dir_salt_cache} force
        sleep 5
    else
        echo "gluster volume ${gfs_vol_salt_cache_jobs} already created, ignoring" >> ${LOG_FILE}
    fi

    echo -e "\nStarting the gluster volume ${gfs_vol_salt_cache_jobs}" | tee -a ${LOG_FILE}
    start_status=$(gluster volume info ${gfs_vol_salt_cache_jobs} | grep Status: | awk '{ print $2}')
    if [[ "${start_status}" == "Started" ]]; then
        echo "gluster volume ${gfs_vol_salt_cache_jobs} is already started, ignoring" >> ${LOG_FILE}
    else
        echo "starting the volume ${gfs_vol_salt_cache_jobs}" >> ${LOG_FILE}
        echo y | gluster volume start ${gfs_vol_salt_cache_jobs}
        sleep 5
    fi

    echo -e "\nMounting gluster volume ${hostname_A}:${gfs_vol_salt_cache_jobs} at ${mountpt_salt_cache_vol} on Server A" | tee -a ${LOG_FILE}
    if ! mount | grep ${gfs_vol_salt_cache_jobs}; then
        echo "mounting the gluster volume ${gfs_vol_salt_cache_jobs} on Server A" >> ${LOG_FILE}
        mount -t glusterfs ${hostname_A}:/${gfs_vol_salt_cache_jobs} ${mountpt_salt_cache_vol}
    else
        echo "gluster volume ${gfs_vol_salt_cache_jobs} is already mounted on Server A, ignoring" >> ${LOG_FILE}
    fi

    echo -e "\nMounting gluster volume ${hostname_B}:${gfs_vol_salt_cache_jobs} at ${mountpt_salt_cache_vol} on Server B" | tee -a ${LOG_FILE}
    if ! $ssh_cmd $pvt_ip_b mount | grep ${gfs_vol_salt_cache_jobs}; then
        echo "mounting the gluster volume ${gfs_vol_salt_cache_jobs} on Server B" >> ${LOG_FILE}
        $ssh_cmd $pvt_ip_b "mount -t glusterfs ${hostname_B}:/${gfs_vol_salt_cache_jobs} ${mountpt_salt_cache_vol}"
    else
        echo "gluster volume ${gfs_vol_salt_cache_jobs} is already mounted on Server B, ignoring" >> ${LOG_FILE}
    fi

    #volume_prvsnr_data:
    echo -e "\nCreating gluster volume brick for ${gfs_vol_prvsnr_data}" | tee -a ${LOG_FILE}
    if ! gluster volume list | grep ${gfs_vol_prvsnr_data}; then
        echo "creating gluster volume ${gfs_vol_prvsnr_data}" >> ${LOG_FILE}
        echo y | gluster volume create ${gfs_vol_prvsnr_data} replica 2 ${hostname_A}:${mount_dir_prvsnr_data} ${hostname_B}:${mount_dir_prvsnr_data} force
        sleep 5
    else
        echo "gluster volume ${gfs_vol_prvsnr_data} already created, ignoring" >> ${LOG_FILE}
    fi

    echo -e "\nStarting the gluster volume ${gfs_vol_prvsnr_data}" | tee -a ${LOG_FILE}
    start_status=$(gluster volume info ${gfs_vol_prvsnr_data} | grep Status: | awk '{ print $2}')
    if [[ "${start_status}" == "Started" ]]; then
        echo "gluster volume ${gfs_vol_prvsnr_data} is already started, ignoring" >> ${LOG_FILE}
    else
        echo "starting the volume ${gfs_vol_prvsnr_data}" >> ${LOG_FILE}
        echo y | gluster volume start ${gfs_vol_prvsnr_data}
        sleep 5
    fi

    echo -e "\nMounting gluster volume ${hostname_A}:/${gfs_vol_prvsnr_data} at ${mountpt_prvsnr_data_vol} on Server A" | tee -a ${LOG_FILE}
    if ! mount | grep ${gfs_vol_prvsnr_data}; then
        echo "mounting the gluster volume ${gfs_vol_prvsnr_data} on Server A" >> ${LOG_FILE}
        mount -t glusterfs ${hostname_A}:/${gfs_vol_prvsnr_data} ${mountpt_prvsnr_data_vol}
    else
        echo "gluster volume ${gfs_vol_prvsnr_data} is already mounted on Server A, ignoring" >> ${LOG_FILE}
    fi

    echo -e "\nMounting gluster volume ${hostname_A}:/${gfs_vol_prvsnr_data} at ${mountpt_prvsnr_data_vol} on Server B" | tee -a ${LOG_FILE}
    if ! $ssh_cmd $pvt_ip_b mount | grep ${gfs_vol_prvsnr_data}; then
        echo "mounting the gluster volume ${gfs_vol_prvsnr_data} on Server B" >> ${LOG_FILE}
        $ssh_cmd $pvt_ip_b "mount -t glusterfs ${hostname_A}:/${gfs_vol_prvsnr_data} ${mountpt_prvsnr_data_vol}"
    else
        echo "gluster volume ${gfs_vol_prvsnr_data} is already mounted on Server B, ignoring" >> ${LOG_FILE}
    fi

    # add entries in fstab server A
    time_stamp=$(date "+%Y.%m.%d-%H.%M.%S")
    echo -e "\nAdding entry for gluster volume (${gfs_vol_salt_cache_jobs}) in fstab of Server A" | tee -a ${LOG_FILE}
    yes | cp -f /etc/fstab /opt/seagate/cortx_configs/provisioner_generated/fstab_a_unboxing_${time_stamp}
    if grep -q ${gfs_vol_salt_cache_jobs} /etc/fstab; then
        echo "/etc/fstab already has the entry for ${gfs_vol_salt_cache_jobs} on Server A" >> ${LOG_FILE}
        #Comment the existing entry and add the new one
        sed -i "/${gfs_vol_salt_cache_jobs}/ s/^/#/" /etc/fstab 2>&1 | tee -a ${LOG_FILE}
    fi
    echo "${hostname_A}:${gfs_vol_salt_cache_jobs}      ${mountpt_salt_cache_vol}    glusterfs    _netdev,defaults,acl    0 0" >> /etc/fstab

    echo "Adding entry for gluster volume (${gfs_vol_prvsnr_data}) in fstab of Server A" | tee -a ${LOG_FILE}
    if grep -q ${gfs_vol_prvsnr_data} /etc/fstab; then
        echo "/etc/fstab already has the entry for ${gfs_vol_prvsnr_data} on Server A" >> ${LOG_FILE}
        #Comment the existing entry and add the new one
        sed -i "/${gfs_vol_prvsnr_data}/ s/^/#/" /etc/fstab 2>&1 | tee -a ${LOG_FILE}
    fi
    echo "${hostname_A}:${gfs_vol_prvsnr_data}      ${mountpt_prvsnr_data_vol}    glusterfs    _netdev,defaults,acl    0 0" >> /etc/fstab

    # add entries in fstab server B
    echo "Adding entry for gluster volume (${gfs_vol_salt_cache_jobs}) in fstab of Server B" | tee -a ${LOG_FILE}
    fstab_b="/tmp/fstab_b_unboxing"
    echo "Copying the fstab from Server B to Server A for update" >> ${LOG_FILE}
    ${scp_cmd} ${pvt_ip_b}:/etc/fstab ${fstab_b}
    yes | cp -f ${fstab_b} /opt/seagate/cortx_configs/provisioner_generated/fstab_b_unboxing_${time_stamp}
    if grep -q ${gfs_vol_salt_cache_jobs} ${fstab_b}; then
        echo "/etc/fstab already has the entry for ${gfs_vol_salt_cache_jobs} on Server B" >> ${LOG_FILE}
        #Comment the existing entry and add the new one
        sed -i "/${gfs_vol_salt_cache_jobs}/ s/^/#/" ${fstab_b} 2>&1 | tee -a ${LOG_FILE}
    fi
    echo "${hostname_A}:${gfs_vol_salt_cache_jobs}      ${mountpt_salt_cache_vol}    glusterfs    _netdev,defaults,acl    0 0" >> ${fstab_b}

    echo "Adding entry for gluster volume (${gfs_vol_prvsnr_data}) in fstab of Server B" | tee -a ${LOG_FILE}
    if grep -q ${gfs_vol_prvsnr_data} ${fstab_b}; then
        echo "/etc/fstab already has the entry for ${gfs_vol_prvsnr_data} on Server B" >> ${LOG_FILE}
        #Comment the existing entry and add the new one
        sed -i "/${gfs_vol_prvsnr_data}/ s/^/#/" ${fstab_b} 2>&1 | tee -a ${LOG_FILE}
    fi
    echo "${hostname_A}:${gfs_vol_prvsnr_data}      ${mountpt_prvsnr_data_vol}    glusterfs    _netdev,defaults,acl    0 0" >> ${fstab_b}

    echo "Copying the fstab from Server A to Server B after update" >> ${LOG_FILE}
    ${scp_cmd} ${fstab_b} ${pvt_ip_b}:/etc/fstab

}

function update_salt_minion {

    if [[ "srvnode-1" == "$(cat /etc/salt/minion_id)" ]]; then
        # Getting '127.0.0.1'. This can be hard-coded also
        local localhost_ip=$(awk '{print $1; exit}' /etc/hosts)
        local primary_host=$(hostname)
        local secondary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_b} "hostname")
        # echo ${secondary_host}
        local line_to_replace=$(grep -m1 -noP "master: " /etc/salt/minion|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        
        echo -n "Setting salt-master on server A (primary node).............................." 2>&1|tee -a ${LOG_FILE} 
        sed -i "${line_to_replace}s|^master:.*|master: ['${localhost_ip}', '${secondary_host}']|" /etc/salt/minion
        echo "Ok." | tee -a ${LOG_FILE}

        echo -n "Setting salt-master on server B (secondary node)............................" 2>&1|tee -a ${LOG_FILE}
        ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_b} "sed -i \"${line_to_replace}s|^master:.*|master: \['${primary_host}', '${localhost_ip}'\]|\" /etc/salt/minion"
        echo "Ok." | tee -a ${LOG_FILE}

        # It's safe to restart service on both nodes
        echo -n "Restarting salt-minion on Server A.........................................." 2>&1|tee -a ${LOG_FILE}
        systemctl restart salt-minion
        echo "Ok." | tee -a ${LOG_FILE}

        echo -n "Restarting salt-minion on Server B.........................................." 2>&1|tee -a ${LOG_FILE}
        ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_b} "systemctl restart salt-minion"
        echo "Ok." | tee -a ${LOG_FILE}

        sleep 5

        echo -n "Restarting salt-master on Server A.........................................." 2>&1 | tee -a ${LOG_FILE}
        systemctl restart salt-master
        echo "Ok." | tee -a ${LOG_FILE}

        echo -n "Restarting salt-master on Server B.........................................." 2>&1|tee -a ${LOG_FILE}
        ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_b} "systemctl restart salt-master"
        echo "Ok." | tee -a ${LOG_FILE}

#        echo -n "Listing salt keys..........................................................." 2>&1 | tee -a ${LOG_FILE}
#        salt-key -L ${salt_opts}
#        echo "Ok." | tee -a ${LOG_FILE}
#        echo -n "Accepting salt keys........................................................." 2>&1 | tee -a ${LOG_FILE}
#        salt-key -A -y >> ${LOG_FILE}
#        echo "Ok." | tee -a ${LOG_FILE}

        sleep 5
    fi
}


# Copy local pillar to shared pillar and Cleanup the local pillar files
function update_pillar_from_local {
    # Copy public management network parameters
    # Server Node A
    srv1_public_mgmt_ip=$(salt-call --local pillar.get cluster:srvnode-1:network:mgmt:public_ip --output=newline_values_only)
    if [[ "${srv1_public_mgmt_ip}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-1/network/mgmt/public_ip PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-1/network/mgmt/public_ip \"${srv1_public_mgmt_ip}\"
    fi

    srv1_public_gateway=$(salt-call --local pillar.get cluster:srvnode-1:network:mgmt:gateway --output=newline_values_only)
    if [[ "${srv1_public_gateway}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-1/network/mgmt/gateway PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-1/network/mgmt/gateway \"${srv1_public_gateway}\"
    fi

    srv1_public_netmask=$(salt-call --local pillar.get cluster:srvnode-1:network:mgmt:netmask --output=newline_values_only)
    if [[ "${srv1_public_netmask}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-1/network/mgmt/netmask PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-1/network/mgmt/netmask \"${srv1_public_netmask}\"
    fi

    # Server Node B
    srv2_public_mgmt_ip=$(salt-call --local pillar.get cluster:srvnode-2:network:mgmt:public_ip --output=newline_values_only)
    if [[ "${srv2_public_mgmt_ip}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-2/network/mgmt/public_ip PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-2/network/mgmt/public_ip \"${srv2_public_mgmt_ip}\"
    fi

    srv2_public_gateway=$(salt-call --local pillar.get cluster:srvnode-2:network:mgmt:gateway --output=newline_values_only)
    if [[ "${srv2_public_gateway}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-2/network/mgmt/gateway PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-2/network/mgmt/gateway \"${srv2_public_gateway}\"
    fi

    srv2_public_netmask=$(salt-call --local pillar.get cluster:srvnode-2:network:mgmt:netmask --output=newline_values_only)
    if [[ "${srv2_public_netmask}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-2/network/mgmt/netmask PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-2/network/mgmt/netmask \"${srv2_public_netmask}\"
    fi

    search_domains=$(salt-call --local pillar.get cluster:search_domains --output=json|jq --compact-output .local)
    if [[ "${search_domains}" == "None" ]]; then
        provisioner pillar_set cluster/search_domains PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/search_domains ${search_domains}
    fi

    public_dns_servers=$(salt-call --local pillar.get cluster:dns_servers --output=json|jq --compact-output .local)
    if [[ "${public_dns_servers}" == "None" ]]; then
        provisioner pillar_set cluster/dns_servers PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/dns_servers ${public_dns_servers}
    fi

    
    # Copy public data network parameters
    # Server Node A
    srv1_public_data_ip=$(salt-call --local pillar.get cluster:srvnode-1:network:data:public_ip --output=newline_values_only)
    if [[ "${srv1_public_data_ip}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-1/network/data/public_ip PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-1/network/data/public_ip \"${srv1_public_data_ip}\"
    fi

    srv1_public_data_gateway=$(salt-call --local pillar.get cluster:srvnode-1:network:data:gateway --output=newline_values_only)
    if [[ "${srv1_public_data_gateway}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-1/network/data/gateway PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-1/network/data/gateway \"${srv1_public_data_gateway}\"
    fi

    srv1_public_data_netmask=$(salt-call --local pillar.get cluster:srvnode-1:network:data:netmask --output=newline_values_only)
    if [[ "${srv1_public_data_netmask}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-1/network/data/netmask PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-1/network/data/netmask \"${srv1_public_data_netmask}\"
    fi

    # Server Node B
    srv2_public_data_ip=$(salt-call --local pillar.get cluster:srvnode-2:network:data:public_ip --output=newline_values_only)
    if [[ "${srv2_public_data_ip}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-2/network/data/public_ip PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-2/network/data/public_ip \"${srv2_public_data_ip}\"
    fi

    srv2_public_data_gateway=$(salt-call --local pillar.get cluster:srvnode-2:network:data:gateway --output=newline_values_only)
    if [[ "${srv2_public_data_gateway}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-2/network/data/gateway PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-2/network/data/gateway \"${srv2_public_data_gateway}\"
    fi

    srv2_public_data_netmask=$(salt-call --local pillar.get cluster:srvnode-2:network:data:netmask --output=newline_values_only)
    if [[ "${srv2_public_data_netmask}" == "None" ]]; then
        provisioner pillar_set cluster/srvnode-2/network/data/netmask PRVSNR_UNDEFINED
    else
        provisioner pillar_set cluster/srvnode-2/network/data/netmask \"${srv2_public_data_netmask}\"
    fi

    [[ -e /var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all/zzz_cluster.sls ]] && \
    rm -f /var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all/zzz_cluster.sls
    
    ssh_over_pvt_data ${pvt_ip_b} \
    "[[ -e /var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all/zzz_cluster.sls ]] && \
    rm -f /var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all/zzz_cluster.sls"
}


function update_cluster_sls {
    local mgmt_vip=${1:-}
    local cluster_vip=${2:-}
    local _static_ip_a=${3:-}
    local _static_ip_b=${4:-}

    # Copy local pillar to shared pillar and Cleanup the local pillar files
    update_pillar_from_local

    if [[ "srvnode-1" == "$(cat /etc/salt/minion_id)" ]]; then
        echo "Updating Management VIP in pillar" 2>&1|tee -a ${LOG_FILE}
        provisioner pillar_set cluster/mgmt_vip \"${mgmt_vip}\"

        echo "Updating Cluster IP in pillar" 2>&1|tee -a ${LOG_FILE}
        provisioner pillar_set cluster/cluster_ip \"${cluster_vip}\"

         #Update public data interface ips
        if [[ ! -z ${_static_ip_a} && ! -z ${_static_ip_b} ]]; then
            echo "Updating static IP of public data network in pillar for both servers" |tee -a ${LOG_FILE}
            echo "Updating static data ip ($_static_ip_a) for server A" >> $LOG_FILE
            provisioner pillar_set cluster/srvnode-1/network/data/public_ip \"${_static_ip_a}\"
            
            echo "Updating static data ip ($_static_ip_b) for server B" >> $LOG_FILE
            provisioner pillar_set cluster/srvnode-2/network/data/public_ip \"${_static_ip_b}\"
            echo "Done" | tee -a ${LOG_FILE}
        fi

        echo "Updating hostname of Server A" 2>&1|tee -a ${LOG_FILE}
        # Replace node-1 entry
        # Hostname
        local primary_host=$(hostname)
        echo "DEBUG:primary_host: ${primary_host}" >> ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-1/hostname \"${primary_host}\"
        # BMC IP-address
        (hostnamectl status | grep Chassis | grep -q server) && {
            echo "Fetching and updating BMC IP for Server A" 2>&1|tee -a ${LOG_FILE}
            #update_bmc_ip "srvnode-1" >> ${LOG_FILE}
            _ip_line=$(ipmitool lan print 1|grep -oP 'IP Address.+:.*\d+')
            _ip=$(echo ${_ip_line}|cut -f2 -d':'|tr -d ' ')
            if [[ -n "$_ip" && "$_ip" != "0.0.0.0" ]]; then
                echo "BMC IP (Server A): ${_ip}" | tee -a ${LOG_FILE}
                provisioner pillar_set cluster/srvnode-1/bmc/ip \"${_ip}\"
            else
                echo "ERROR: BMC_IP is not configured on Server A" | tee -a ${LOG_FILE}
                exit 1
            fi
            echo "Done." >> ${LOG_FILE}

        }

        # Replace node-2 entry
        # Hostname
        echo "Updating hostname of Server B" 2>&1|tee -a ${LOG_FILE}
        local secondary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${pvt_ip_b} "hostname")
        echo "DEBUG: secondary_host: ${secondary_host}" >> ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-2/hostname \"${secondary_host}\"
        # BMC IP-address
        (hostnamectl status | grep Chassis | grep -q server) && {
            echo "Fetching and updating BMC IP for Server B" 2>&1|tee -a ${LOG_FILE}
            #update_bmc_ip "srvnode-2" "srvnode-2" >> ${LOG_FILE}
            _ip_line=$(${ssh_cmd} ${pvt_ip_b} ipmitool lan print 1|grep -oP 'IP Address.+:.*\d+')
            _ip=$(echo ${_ip_line}|cut -f2 -d':'|tr -d ' ')
            if [[ -n "$_ip" && "$_ip" != "0.0.0.0" ]]; then
                echo "BMC IP (Server B): ${_ip}" | tee -a ${LOG_FILE}
                provisioner pillar_set cluster/srvnode-2/bmc/ip \"${_ip}\"
            else
                echo "ERROR: BMC_IP is not configured on Server B" | tee -a ${LOG_FILE}
                exit 1
            fi
            echo "Done." >> ${LOG_FILE}

        }
    fi
}

function recover_rabbitmq_cluster {
    # # Update RabbitMQ cluster
    echo -n "Starting rabbitmq cluster......" 2>&1|tee -a ${LOG_FILE}
    salt "srvnode-1" state.apply misc_pkgs.rabbitmq ${salt_opts}; sleep 5
    salt "srvnode-2" state.apply misc_pkgs.rabbitmq ${salt_opts}; sleep 5
    echo "Done"
}



function lock_unboxing_user {

    # TODO: Locking the password of cortxub is commented for Beta.
    #       Find alternative if unboxing fails and user gets locked out.
    #passwd --lock cortxub >> ${LOG_FILE}

    echo "\

******************* Please Run next steps manually ************************

  1. Check if all IP addresses are assigned as expected

      $ sudo ip a

      NOTE: run this on both servers.

  2. Check if system has been assigned a hostname:

      $ sudo salt '*' cmd.run hostname

      NOTE: run this from Server A (Primary server)

  3. Check if Cortx cluster is up and all services are Started, run:

      $ sudo pcs status

      NOTE: All the resources/services should be listed as started.
" 2>&1 | tee -a ${LOG_FILE}
}

function remove_boxing_flag {
    if [[ -f '/opt/seagate/cortx_configs/provisioner_generated/boxed' ]]
    then
        echo "DEBUG: Boxed file found. Removing boxed file." >> ${LOG_FILE}
        rm -f /opt/seagate/cortx_configs/provisioner_generated/boxed || true
    else
        echo "\
ERROR: Boxing command was not run
       Please ensure that the boxing sequence was done before doing unboxing.
***** FAILED!! *****
The detailed logs are kept at: ${LOG_FILE}" | tee -a ${LOG_FILE}
    fi
}

