[EOS-15766]: Test- Resume update process after reboot

Step-1: Deploy Cortx software stack on VM/HW and verify update-post-boot.service is enabled
        `$systemctl status update-post-boot.service`

Step-2: Create update_states.sls pillar and add provisioner commands stating which states will run before and after reboot
        `$provisioner pillar_set update_states/pre_boot [\"command1\",\ \"command2\"]`
        `$provisioner pillar_set update_states/post_boot [\"command1\",\ \"command2\"]`

Step-3: Reboot servers

Step-4: Check update-post-boot.service status once the system's are back. 
        Service should be inactive but `/opt/seagate/cortx/provisioner/cli/update-post-reboot.sh` should have got executed. 
        Also commands added in update_states/post_boot should be executed.

[EOS-15751]: Test - Setup R2 upgrade release bundle

Pre-requisites:
    1. Need to be installed
        - glusterfs
        - saltstack
        - provisioner api
    2. It is enough to configure provisioner using the following command, for example:
        ```
        run provisioner setup_cluster --source local --srvnode1 <IP1> --srvnode2 <IP2> --ha --logfile --logfile-filename ./setup.log --console-formatter full
        ```
       where
       <IP1> and <IP2> - ip addresses of VM nodes

Step-1: Ensure that there are not enabled or disabled `sw_upgrade_*` yum repos
        `$ yum repolist enabled`
        `$ yum repolist disabled`

Step-2: a)  For testing `set_swupgrade` command is enough to download any valid single ISO.
            For example, this ISO:

            http://cortx-storage.colo.seagate.com/releases/cortx/github/cortx-1.0/iso/cortx-1.0-280.iso

        b)  To create the valid Singe SW Upgrade ISO structure is needed to create

            1. `$ mkdir sw_upgrade`
            2. `$ cd sw_upgrade`
            3. `$ mkdir 3rdparty`
            4. `$ mkdir os`
            5. `$ mkdir cortx`
            6. `$ mkdir python`
            7. `$ sudo cortx-1.0-280.iso /mnt/iso`
            8. `$ cp -r /mnt/iso ./3rdparty`
            9. `$ cp -r /mnt/iso ./cortx`
            10. `$ cp -r /mnt/iso ./os`
            11. `$ cd ..`
            12. `$ mkisofs -graft-points -r -l -iso-level 2 -J -o sw_upgrade.iso ./sw_upgrade/`

        c)  After the steps mentioned above we have the `sw_upgrade.iso` which can be used for the
            `set_swupgrade` command testing

Step-3: Run provisioner command:

        `$ provisioner set_swupgrade_repo 1.0.0 --source="./sw_upgrade.iso" --username=<provisioner_user> --password=<provisioner_user_password>`

        Command should be finished successfully.

Step-4: Check that all yum repositories are listed in `$ yum repolist enabled` output:

        Example of possible output
        ```
         $ yum repolist enabled
         Loaded plugins: fastestmirror, product-id, search-disabled-repos, subscription-manager
         Repository extras is listed more than once in the configuration
         Loading mirror speeds from cached hostfile
         repo id                                                             repo name                                                    status
         base                                                                base                                                         14,689+32
         cortx_commons                                                       cortx_commons                                                  334+129
         epel                                                                3rd_party_epel                                                  16,808
         extras                                                              extras                                                             451
         saltstack/7/x86_64                                                  SaltStack repo for RHEL/CentOS 7                                 93+34
         sw_upgrade_3rdparty_1.0.0-301                                       Cortx Upgrade repo 3rdparty-1.0.0-301                               22
         sw_upgrade_cortx_1.0.0-301                                          Cortx Upgrade repo cortx-1.0.0-301                                  22
         sw_upgrade_os_1.0.0-301                                             Cortx Upgrade repo os-1.0.0-301
         ```

[EOS-16927]: New API for upgrade: adjust rollback logic for R2 (singlenode)

Step-1: Ensure rollback pillar data is present/updated as part of sw_update command
        `$salt-call pillar.get rollback`

Step-2: Run sw_rollback api
        `$provisioner sw_rollback CORTX_VERSION`


[EOS-18337]: Make bootstrap API salt states for cortx repositories available in deployed environment

Step-1: Start the docker container

```bash
pytest test/build_helpers -k test_build_setup_env -s --root-passwd root --nodes-num 1
```

Step-2: Install provisioner API to created docker container

```bash
provisioner setup_provisioner --source local --logfile --logfile-filename ./setup.log --console-formatter full srvnode-1:172.17.0.2
```

NOTE: For **srvnode-1**, **srvnode-2**, etc. parameter values use the **Hostname** output from previous step

Step-3: Copy the `devops` directory of `cortx-prvsnr` repository

```bash
cd /PATH/TO/cortx-prvsnr/
docker cp ./devops/ <CONTAINER ID>:/opt/seagate/cortx/provisioner/devops
```

Step-4: Build core `cortx-prvsnr` RPM package

```bash
docker exec -it <CONTAINER ID> bash
bash -x /opt/seagate/cortx/provisioner/devops/rpms/buildrpm.sh -g 123456
yum install /root/rpmbuild/RPMS/x86_64/cortx-prvsnr-2.0.0-0_git123456_el7.x86_64.rpm
```

Step-4: Validate that `repos` states are available and visible for salt commands

```bash
salt '*' state.sls_exists repos.upgrade
```

Expected output should be
```
srvnode-1:
    True
```

Also you can validate after that command that variable `$?` is set to 0:
```bash
if [ $? -eq  0 ] ; then echo "No errors" ; else echo "Some error occurred"; fi
```

  NOTE: Useful command to list all salt states
```bash
salt-call state.show_top
```

[EOS-17831] - Bootstrap - Storage-Set-ID & Machine-ID assignment
  1. Storage-set-id should be  assigned to machine
  Steps: 
    1. Deploy single node vm
    2. Check in  cluster storage set id is assigned 
    ``` salt-call pillar.get cluster:srvnode-1:storage_set_id ```
    3. List storage_sets
    ``` salt-call pillar.get cluster:storage_sets ```
	
  2. Machine_id is assigned to vm   
  prereq: 
    1. Take new fresh vm and check if machine id is available and make note of it
    ``` cat /etc/machine-id ```
    2. Deploy single node vm
  Steps:
    1. Check if machine id  is reset
    ``` salt-call grains.get machine_id ```
    2. Verify machine id from grains
    ``` cat /etc/machine-id ```
    3. Check if machine_id  is set in cluster
    ``` salt-call pillar.get cluster:srvnode-1:machine_id ```


[EOS-17235][EOS-16869] - Destroy cortx on single node vm
   Steps : Verify if all steps are working fine from doc: https://github.com/Seagate/cortx-prvsnr/wiki/Teardown-Node

[EOS-18270] - 3-node VM deployment implementation and integration - Phase-2 - 3rd Party SW
Step 1: Run setup_provisioner command with --ha option
Step 2: Run deploy_vm command to deploy system and prereq states
        ```
        provisioner deploy_vm --states system --setup-type 3_node
        provisioner deploy_vm --states prereq --setup-type 3_node
        ```
Step 3: Ensure third party services are installed on all three vm'salt

[EOS-13889]: Move generated configs directory in provsioner
Step 1: Deploy single node vm
Step 2: Verify checkpoint flags are created under /opt/seagate/cortx_configs/provisioner_generated/ directory

[EOS-17115]:Add additional keys to cluster ConfStor
Step 1: Run setup provisioner on a single/multinode VM.
Step 2: Run provisioner confstore to get confstore generated
        ```
        provisioner confstore_export
        ```
Step 3: Verify the confstore is generated across all nodes

[EOS-18917]:Creation of custom grains for enclosure_id,netmask, serial number and multipath
Step 1: Deploy single/multinode node vm
Step 2: For fetching netmask from grains.
        ```
        salt "*" grains.get ip4_netmask
        ```
Step 3: Verify netmask from individual interfaces
Step 4: For fetching enclosure-id 
        ```
        salt "*" grains.get enclosure_id
        ```
Step 4: For fetching serial number
        ```
        salt "*" grains.get lr-serial-number
        ```
