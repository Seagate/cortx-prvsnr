#!/bin/bash
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing, 
# please email opensource@seagate.com or cortx-questions@seagate.com."
#



# Script to run end to end deployment of CORTX for 2 node setup.

set -euE

export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/auto-deploy.log}"
mkdir -p $(dirname "${LOG_FILE}")
# /usr/bin/true > $LOG_FILE
truncate -s 0 ${LOG_FILE}

PRVSNR_ROOT="/opt/seagate/cortx/provisioner"

trap_handler ()
{
    echo -e "\n***** FAILED!!*****" 2>&1 | tee -a $LOG_FILE
    echo "Detailed error log is kept at: $LOG_FILE" 2>&1 | tee -a $LOG_FILE
    exit 1
}
trap trap_handler ERR

cat > /etc/profile.d/set_path_env << EOM
#!/bin/bash
echo $PATH | grep -q "/usr/local/bin" || export PATH=$PATH:/usr/local/bin
EOM
. /etc/profile.d/set_path_env

srvnode_2_host=
srvnode_2_passwd=
cluster_ip=
mgmt_vip=
pub_din=
pvt_din=
pub_dip1=
pub_dip2=
ctrl_a_ip=
ctrl_b_ip=
ctrl_user=
ctrl_passwd=
tgt_build=
bmc1_user=
bmc1_passwd=
bmc2_user=
bmc2_passwd=


srvnode_2_host_opt=false
srvnode_2_passwd_opt=false
cluster_ip_opt=false
mgmt_vip_opt=false
pub_din_opt=false
pvt_din_opt=false
pub_dip1_opt=false
pub_dip2_opt=false
ctrl_a_ip_opt=false
ctrl_b_ip_opt=false
ctrl_user_opt=false
ctrl_passwd_opt=false   
tgt_build_opt=false
bmc1_user_opt=false
bmc1_passwd_opt=false
bmc2_user_opt=false
bmc2_passwd_opt=false
cross_connect_opt=false


cortx_repo=
bundled_release=false

usage()
{
    echo "\
Usage:
auto-deploy { -s <secondary node hostname (srvnode-2)> -p <password for sec node>
                -C <cluster-ip> -V <management VIP>
                -t <target build url for CORTX> [--bundle]
                [Optional Arguments] }

Optional Arguments:
-n    <NETWORK IF>     Public n/w interface name (default enp175s0f0)
-N    <NETWORK IF>     Private n/w interface name (default enp175s0f1).
                        Network interface for direct connect.
-i    <IP ADDRESS>     IP address for public n/w interface name on node-1.
                        This IP will be assigned to the n/w interface
                        provided with -n option.
                        Omit this option if ip is already set by DHCP
-I    <IP ADDRESS>     IP address for public n/w interface name on node-2,
                        This IP will be assigned to the n/w interface name
                        provided with -N option.
                        Omit this option if ip is already set by DHCP.
-A    <IP ADDRESS>     IP address of controller A (default 10.0.0.2)
-B    <IP ADDRESS>     IP address of controller B (default 10.0.0.3)
-U    <USER NAME>      User for controller (default 'manage')
-P    <PASSWORD>       Password for controller (default 'passwd')

--bundle               Switch to bundled release installation mode:
                       target build url is treated as a base url of a
                       a bundled distribution structure:
                          <base_url>/
                               rhel7.7 or centos7.7   <- OS ISO is mounted here
                               3rd_party              <- CORTX 3rd party ISO is mounted here
                               cortx_iso              <- CORTX ISO (main) is mounted here
--b1  <BMC USER>       BMC User for Node-1. Default ADMIN
--m1  <BMC PASSWORD>   BMC Password for Node-1. Default adminBMC!
--b2  <BMC USER>       BMC User for Node-2. Default ADMIN
--m2  <BMC PASSWORD>   BMC Password for Node-2. Default adminBMC!
"
}

help()
{
echo "\
----------- Caveats -------------
1. The command must be run from primary node in the cluster.
2. Ensure the setup is clean and no cortx rpms are installed.
3. Ensure the ~/.ssh directory is empty.
4. If optional arguments are skipped the default values in cluster.sls and
storage_enclosure.sls are used.
5. Ensure the BMC user and password are same for both nodes in the cluster.

-------- Sample command ---------
$ auto-deploy -s sm27-r22.pun.seagate.com -p 'seagate'
-C 172.19.222.27 -V 10.230.215.45 -n enp216s0f0 -N enp216s0f1
-i 172.19.22.27 -I 172.19.22.28 -A 10.230.10.2 -B 10.230.10.3 -U manage -P 'S34gate123!'
-b manage -m 'admin!'
-t http://cortx-storage.colo.seagate.com/releases/eos/github/release/rhel-7.7.1908/2703/
"
}

while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help) usage; help; exit 0
        ;;
        -s)
            srvnode_2_host_opt=true
            [ -z "$2" ] &&
                echo "Error: srvnode-2 not provided" && exit 1;
            srvnode_2_host="$2"
            shift 2 ;;
        -p)
            srvnode_2_passwd_opt=true
            [ -z "$2" ] &&
                echo "Error: srvnode-2 password not provided" && exit 1;
            srvnode_2_passwd="$2"
            shift 2 ;;
        -C)
            cluster_ip_opt=true
            [ -z "$2" ] &&
                echo "Error: cluster ip not provided" && exit 1;
            cluster_ip="$2"
            shift 2 ;;
        -V)
            mgmt_vip_opt=true
            [ -z "$2" ] &&
                echo "Error: management vip not provided" && exit 1;
            mgmt_vip="$2"
            shift 2 ;;
        -n)
            pub_din_opt=true
            [ -z "$2" ] &&
                echo "Error: public data interface name not provided" && exit 1;
            pub_din="$2"
            shift 2 ;;
        -N)
            pvt_din_opt=true
            [ -z "$2" ] &&
                echo "Error: private data interface name not provided" && exit 1;
            pvt_din="$2"
            shift 2 ;;
        -i)
            pub_dip1_opt=true
            [ -z "$2" ] &&
                echo "Error: Public data ip for node-1 not provided" && exit 1;
            pub_dip1="$2"
            shift 2 ;;
        -I)
            pub_dip2_opt=true
            [ -z "$2" ] &&
                echo "Error: Public data ip for node-2 not provided" && exit 1;
            pub_dip2="$2"
            shift 2 ;;
        -A)
            ctrl_a_ip_opt=true
            [ -z "$2" ] &&
                echo "Error: IP of Controller A not provided" && exit 1;
            ctrl_a_ip="$2"
            shift 2 ;;
        -B)
            ctrl_b_ip_opt=true
            [ -z "$2" ] &&
                echo "Error: IP of Controller B not provided" && exit 1;
            ctrl_b_ip="$2"
            shift 2 ;;
        -U)
            ctrl_user_opt=true
            [ -z "$2" ] &&
                echo "Error: Controller user name not provided" && exit 1;
            ctrl_user="$2"
            shift 2 ;;
        -P)
            ctrl_passwd_opt=true
            [ -z "$2" ] &&
                echo "Error: Controller password not provided" && exit 1;
            ctrl_passwd="$2"
            shift 2 ;;
        -t)
            tgt_build_opt=true
            [ -z "$2" ] &&
                echo "Error: Target build not provided" && exit 1;
            tgt_build="$2"
            shift 2 ;;
        --bundle)
            bundled_release=true
            shift ;;
        --m1)
            bmc1_passwd_opt=true
            [ -z "$2" ] &&
                echo "Error: BMC password for node-1 not provided" && exit 1;
            bmc1_passwd="$2"
            shift 2 ;;
        --b1)
            bmc1_user_opt=true
            [ -z "$2" ] &&
                echo "Error: BMC user for node-1 not provided" && exit 1;
            bmc1_user="$2"
            shift 2 ;;
        --m2)
            bmc2_passwd_opt=true
            [ -z "$2" ] &&
                echo "Error: BMC password for node-2 not provided" && exit 1;
            bmc2_passwd="$2"
            shift 2 ;;
        --b2)
            bmc2_user_opt=true
            [ -z "$2" ] &&
                echo "Error: BMC user for node-2 not provided" && exit 1;
            bmc2_user="$2"
            shift 2 ;;

        *) echo "Invalid option $1"; usage; exit 1;;
    esac
done

if [[ "$srvnode_2_host_opt" == false ||
    "$srvnode_2_passwd_opt" == false ||
    "$cluster_ip_opt" == false || 
    "$mgmt_vip_opt" == false ||
    "$tgt_build_opt" == false ]]; then

        echo "Insufficient input provided"
        usage
        exit 1
fi

if [[ "$bundled_release" == true ]]; then
    cortx_repo="$tgt_build/cortx_iso"
else
    cortx_repo="$tgt_build"
fi


ssh_tool="/usr/bin/sshpass"
ssh_base_cmd="/bin/ssh"
ssh_opts="-o UserKnownHostsFile=/dev/null\
    -o StrictHostKeyChecking=no -o LogLevel=error"
user=root
ssh_cred="$ssh_tool -p $srvnode_2_passwd"
ssh_cmd="$ssh_base_cmd $ssh_opts $user@$srvnode_2_host"
remote_cmd="$ssh_cred $ssh_cmd"

ssh_tool_pkg=$(basename $ssh_tool)
[ -f "$ssh_tool" ] || {
    echo "Installing $ssh_tool_pkg"
    yum install -y $ssh_tool_pkg
}

# TODO TEST EOS-12508
install_prvsnr_cli()
{
    # Install cortx-prvsnr-cli rpm from cortx-storage and install it on both nodes
    target_node="$1"
    echo "Target Node: $target_node" 2>&1|tee -a $LOG_FILE
    if [[ "$target_node" = "srvnode-1" ]]; then
        rpm -qa | grep -q cortx && {
            echo "ERROR: cortx packages are already installed"
            echo "Cleaning up the packages"
            yum erase -y cortx*
            yum clean all
            # echo "Please clean-up previous installtion from both the nodes and retry"
            # exit 1
        }

        yum install -y yum-utils; yum-config-manager --add-repo $cortx_repo
        yum install -y cortx-prvsnr-cli-1.0.0 --nogpgcheck
        rm -rf /etc/yum.repos.d/$(echo ${cortx_repo#"http://"} | sed -e 's/\/\//\//' -e 's/\//_/g').repo

        systemctl stop firewalld || true

        return 0
    fi
$remote_cmd <<EOF
    set -eu
    rpm -qa | grep -q cortx && {
        echo "ERROR: cortx packages are already installed"
        echo "Cleaning up the packages"
        yum erase -y cortx*
        yum clean all
        # echo "Please clean-up previous installtion from both the nodes and retry"
        # exit 1
    }

    yum install -y yum-utils; yum-config-manager --add-repo $cortx_repo
    yum install -y cortx-prvsnr-cli-1.0.0 --nogpgcheck
    rm -rf /etc/yum.repos.d/$(echo ${cortx_repo#"http://"} | sed -e 's/\/\//\//' -e 's/\//_/g').repo
    
    systemctl stop firewalld || true

EOF
}

install_config_prvsnr()
{
    local _bundle=

    set -eu
    echo -e "\n\t***** INFO: Installing cortx-prvsnr-cli on node-2 *****" 2>&1 | tee -a $LOG_FILE
    sleep 1
    install_prvsnr_cli "srvnode-2"

    echo -e "\n\t***** INFO: Installing cortx-prvsnr-cli on node-1*****" 2>&1 | tee -a $LOG_FILE
    sleep 1
    install_prvsnr_cli "srvnode-1"

    # Run setup provisioner
    echo -e "\n\t***** INFO: Running setup-provisioner *****" 2>&1 | tee -a $LOG_FILE
    sleep 1

    if [[ "$bundled_release" == true ]]; then
        _bundle="--bundle"
    fi

    echo -e "\n\tRunning sh /opt/seagate/cortx/provisioner/cli/setup-provisioner"\
        "--srvnode-2='$srvnode_2_host' '$tgt_build' $_bundle" 2>&1 | tee -a $LOG_FILE
    sh /opt/seagate/cortx/provisioner/cli/setup-provisioner --srvnode-2="$srvnode_2_host" "$tgt_build" $_bundle 2>&1 | tee -a $LOG_FILE

    echo "Done" 2>&1 | tee -a $LOG_FILE
}

update_pillar()
{
    # Update cluster.sls
    echo -e "\n\t***** INFO: Updating cluster.sls *****" 2>&1 | tee -a $LOG_FILE


    #Update cluster_ip
    echo "Updating cluster_ip" 2>&1|tee -a $LOG_FILE
    provisioner pillar_set cluster/cluster_ip \"${cluster_ip}\"

    #Update mgmt_vip
    echo "Updating mgmt_vip" 2>&1|tee -a $LOG_FILE
    provisioner pillar_set cluster/mgmt_vip \"${mgmt_vip}\"

    #Update interface names
    if [[ "$pub_din_opt" == true && "$pvt_din_opt" == true ]]; then
        echo "Updating interface names" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-1/network/data_nw/iface [\"${pub_din}\",\ \"${pvt_din}\"]
        provisioner pillar_set cluster/srvnode-2/network/data_nw/iface [\"${pub_din}\",\ \"${pvt_din}\"]
    fi

    #Update interface ips
    if [[ "$pub_dip1_opt" == true ]]; then
        echo "Updating interface ip ($pub_dip1) for node1" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-1/network/data_nw/public_ip_addr \"${pub_dip1}\"
    fi

    if [[ "$pub_dip2_opt" == true ]]; then
        echo "Updating interface ip ($pub_dip2) for node2" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-2/network/data_nw/public_ip_addr \"${pub_dip2}\"
    fi

    if [[ "$bmc1_user_opt" == true ]]; then
        #Node1
        echo "Updating BMC user ($bmc1_user) for node-1" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-1/bmc/user \"${bmc1_user}\"
    fi
    
    if [[ "$bmc2_user_opt" == true ]]; then
        #Node2
        echo "Updating BMC user ($bmc2_user) for node-2" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-2/bmc/user \"${bmc2_user}\"
    fi

    if [[ "$bmc1_passwd_opt" == true ]]; then
        echo "Updating BMC password ($bmc1_passwd) for node-1" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-1/bmc/secret \"${bmc1_passwd}\"
    fi
    
    if [[ "$bmc2_passwd_opt" == true ]]; then
        echo "Updating BMC password ($bmc2_passwd) for node-2" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set cluster/srvnode-2/bmc/secret \"${bmc2_passwd}\"
    fi

    #Update storage_enclosure pillar
    echo -e "\n\t***** INFO: Updating Enclosure details in pillar *****" 2>&1 | tee -a $LOG_FILE

    #Update controller_ip1
    if [[ "$ctrl_a_ip_opt" == true ]]; then
        echo "Updating controller_ip1" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set storage_enclosure/controller/primary_mc/ip \"${ctrl_a_ip}\"
    fi

    #Update controller_ip2
    if [[ "$ctrl_b_ip_opt" == true ]]; then
        echo "Updating controller_ip2" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set storage_enclosure/controller/secondary_mc/ip \"${ctrl_b_ip}\"
    fi

    #Update controller user
    if [[ "$ctrl_user_opt" == true ]]; then
        echo "Updating controller user" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set storage_enclosure/controller/user \"${ctrl_user}\"
    fi

    #Update controller_passwd
    if [[ "$ctrl_passwd_opt" == true ]]; then
        echo "Updating controller_passwd" 2>&1|tee -a $LOG_FILE
        provisioner pillar_set storage_enclosure/controller/secret \"${ctrl_passwd}\"
    fi

    # Update s3client.sls
    echo "Updating ip in s3clients" 2>&1|tee -a $LOG_FILE
    provisioner pillar_set s3clients/ip \"${cluster_ip}\"

    echo -e "\n\t ***** DEBUG: Updated pillars *****" 2>&1|tee -a $LOG_FILE
    echo -e "\n\t cluster.sls" 2>&1|tee -a $LOG_FILE
    salt "srvnode-1" pillar.get cluster 2>&1|tee -a $LOG_FILE

}

install_config_prvsnr
update_pillar

# Run deploy
echo -e "\n\t***** INFO: Running deploy *****"
sleep 2
sh /opt/seagate/cortx/provisioner/cli/deploy -v

echo -e "\n***** SUCCESS!! *****" 2>&1 | tee -a $LOG_FILE
echo " Check following logs to see the complete logs of auto-deploy: $LOG_FILE" 2>&1 | tee -a $LOG_FILE
echo "Done"
