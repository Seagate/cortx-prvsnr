#!/bin/sh
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing, 
# please email opensource@seagate.com or cortx-questions@seagate.com."
#



# Functions in this file address following:
#   1. Updates /root/.ssh/config file 
#   2. Update cluster.sls with hostnames obtained for node-1 and node-2 
#   3. Update /etc/salt/minion for hostname
#   4. Start rabbitmq cluster   <= Currently handled in init
set -euE

export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/unboxing_config_update.log}"
mkdir -p $(dirname "${LOG_FILE}")
truncate -s 0 ${LOG_FILE}

BASEDIR=$(dirname "${BASH_SOURCE}")

. ${BASEDIR}/../../common_utils/utility_scripts.sh
. ${BASEDIR}/../../common_utils/functions.sh


PRVSNR_ROOT="/opt/seagate/cortx/provisioner"
salt_opts="--no-color --out-file=${LOG_FILE} --out-file-append"

# private_data_ip_node_1=$(grep -m1 -A8 -P "data_nw:" ${PRVSNR_ROOT}/pillar/user/groups/all/cluster.sls|grep "pvt_ip_addr"|tail -n1|cut -d':' -f2|tr -d "[:space:]")
# private_data_ip_node_2=$(grep -m2 -A8 -P "data_nw:" ${PRVSNR_ROOT}/pillar/user/groups/all/cluster.sls|grep "pvt_ip_addr"|tail -n1|cut -d':' -f2|tr -d "[:space:]")
#private_data_ip_node_1=$(get_pillar_data cluster:srvnode-1:network:data_nw:pvt_ip_addr)
#private_data_ip_node_2=$(get_pillar_data cluster:srvnode-2:network:data_nw:pvt_ip_addr)

function trap_handler {
    echo "***** ERROR! *****"
    echo "For detailed error logs, please see: $LOG_FILE"
    echo "******************"
}
trap trap_handler ERR

function update_ssh_config {
    if [[ "srvnode-1" == "$(cat /etc/salt/minion_id)" ]]; then
        echo -n "Updating server A hostname in ssh config file of server A..................." 2>&1|tee -a ${LOG_FILE}
        # Replace node-1 entry
        local primary_host=$(hostname)
        # echo ${primary_host}
        local line_to_replace=$(grep -m1 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-1.*|Host srvnode-1 ${primary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${primary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}

        # Replace node-2 entry
        echo "private_data_ip_node_2: $private_data_ip_node_2"
        echo -n "Updating server B hostname in ssh config file of server A..................." 2>&1|tee -a ${LOG_FILE}
        local secondary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${private_data_ip_node_2} "hostname")
        # echo ${secondary_host}
        local line_to_replace=$(grep -m2 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-2.*|Host srvnode-2 ${secondary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${secondary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}
    else
        echo "private_data_ip_node_1: $private_data_ip_node_1"
        echo -n "Updating server B hostname in ssh config file of server B..................." 2>&1|tee -a ${LOG_FILE}
        # Replace node-1 entry
        local primary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${private_data_ip_node_1} "hostname")
        # echo ${primary_host}
        local line_to_replace=$(grep -m1 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-1.*|Host srvnode-1 ${primary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${primary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}

        # Replace node-2 entry
        echo -n "Updating server A hostname in ssh config file of server B..................." 2>&1|tee -a ${LOG_FILE}
        local secondary_host=$(hostname)
        # echo ${secondary_host}
        local line_to_replace=$(grep -m2 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        sed -i "s|Host srvnode-2.*|Host srvnode-2 ${secondary_host}|" /root/.ssh/config
        sed -i "${line_to_replace}s|HostName.*|HostName ${secondary_host}|" /root/.ssh/config
        echo "Ok." | tee -a ${LOG_FILE}
    fi
}

function update_salt_minion {

    if [[ "srvnode-1" == "$(cat /etc/salt/minion_id)" ]]; then
        local host=$(hostname)
        local line_to_replace=$(grep -m1 -noP "master: " /etc/salt/minion|tail -1|cut -d: -f1)
        # echo ${line_to_replace}
        
        echo -n "Setting salt-master on server A (primary node).............................." 2>&1|tee -a ${LOG_FILE}
        sed -i "${line_to_replace}s|^master:.*|master: ${host}|" /etc/salt/minion
        echo "Ok." | tee -a ${LOG_FILE}

        echo -n "Setting salt-master on server B (secondary node)............................" 2>&1|tee -a ${LOG_FILE}
        ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${private_data_ip_node_2} "sed -i \"${line_to_replace}s|^master:.*|master: ${host}|\" /etc/salt/minion"
        echo "Ok." | tee -a ${LOG_FILE}
        
        # It's safe to restart service on both nodes
        echo -n "Restarting salt-minion on Server A.........................................." 2>&1|tee -a ${LOG_FILE}
        systemctl restart salt-minion
        echo "Ok." | tee -a ${LOG_FILE}

        echo -n "Restarting salt-minion on Server B.........................................." 2>&1|tee -a ${LOG_FILE}
        ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${private_data_ip_node_2} "systemctl restart salt-minion"
        echo "Ok." | tee -a ${LOG_FILE}

        sleep 5

        echo -n "Listing salt keys..........................................................." 2>&1 | tee -a ${LOG_FILE}
        salt-key -L ${salt_opts}
        echo "Ok."
        echo -n "Accepting salt keys........................................................." 2>&1 | tee -a ${LOG_FILE}
        salt-key -A -y >> ${LOG_FILE}
        echo "Ok." | tee -a ${LOG_FILE}

        sleep 5
    fi
}

function update_cluster_sls {
    local mgmt_vip=${1:-}
    local cluster_vip=${2:-}
    local _static_ip_a=${3:-}
    local _static_ip_b=${4:-}

    local _cluster_sls_path=${PRVSNR_ROOT}/pillar/components/cluster.sls
    if [[ -f "${PRVSNR_ROOT}/pillar/user/groups/all/cluster.sls" ]]; then
        _cluster_sls_path=${PRVSNR_ROOT}/pillar/user/groups/all/cluster.sls
    fi

    if [[ "srvnode-1" == "$(cat /etc/salt/minion_id)" ]]; then
        echo -n "Updating Management VIP in pillar..........................................." 2>&1|tee -a ${LOG_FILE}
        #local line_to_replace=$(grep -m1 -n "mgmt_vip:" ${_cluster_sls_path}|tail -1|cut -d: -f1)
        #sed -i "${line_to_replace},/mgmt_vip:*/ s|mgmt_vip:.*|mgmt_vip: ${mgmt_vip}|" ${_cluster_sls_path}
        provisioner pillar_set cluster/mgmt_vip \"${mgmt_vip}\"
        echo "Ok." | tee -a ${LOG_FILE}

        echo -n "Updating Cluster IP in pillar..............................................." 2>&1|tee -a ${LOG_FILE}
        #local line_to_replace=$(grep -m1 -n "cluster_ip:" ${_cluster_sls_path}|tail -1|cut -d: -f1)
        #sed -i "${line_to_replace},/cluster_ip:*/ s|cluster_ip:.*|cluster_ip: ${cluster_vip}|" ${_cluster_sls_path}
        provisioner pillar_set cluster/cluster_ip \"${cluster_vip}\"
        echo "Ok." |tee -a ${LOG_FILE}

         #Update public data interface ips
        if [[ ! -z ${_static_ip_a} && ! -z ${_static_ip_b} ]]; then
            echo "Updating static IP of public data network in pillar for both servers" |tee -a ${LOG_FILE}
            echo "Updating static data ip ($_static_ip_a) for server A" >> $LOG_FILE
            #line_node1_ip=`grep -A8 -n "data_nw:"  $_cluster_sls_path | grep "public_ip_addr:" | cut -d- -f1 | head -1`
            #sed -ie "${line_node1_ip}s/.*/                public_ip_addr: ${_static_ip_a}/" $_cluster_sls_path
            provisioner pillar_set cluster/srvnode-1/network/data_nw/public_ip_addr \"${_static_ip_a}\"
            
            echo "Updating static data ip ($_static_ip_b) for server B" >> $LOG_FILE
            #line_node2_ip=`grep -A8 -n "data_nw:"  $_cluster_sls_path | grep "public_ip_addr:" | cut -d- -f1 | tail -1`
            #sed -ie "${line_node2_ip}s/.*/                public_ip_addr: ${_static_ip_b}/" $_cluster_sls_path
            provisioner pillar_set cluster/srvnode-2/network/data_nw/public_ip_addr \"${_static_ip_b}\"
            echo "Done" | tee -a ${LOG_FILE}
        fi

        echo -n "Updating hostname of Server A..............................................." 2>&1|tee -a ${LOG_FILE}
        # Replace node-1 entry
        # Hostname
        local primary_host=$(hostname)
        echo "DEBUG:primary_host: ${primary_host}" >> ${LOG_FILE}
        local line_to_replace=$(grep -m1 -n "srvnode-1:" ${_cluster_sls_path}|tail -1|cut -d: -f1)
        echo "DEBUG: line_to_replace: ${line_to_replace}" >> ${LOG_FILE}
        #sed -i "${line_to_replace},/hostname:*/ s|hostname:.*|hostname: ${primary_host}|" ${_cluster_sls_path}
        provisioner pillar_set cluster/srvnode-1/hostname \"${primary_host}\"
        echo "Ok." | tee -a ${LOG_FILE}
        # BMC IP-address
        # grep -m1 -A10 -n "srvnode-1:" ${_cluster_sls_path}|grep -m1 -A3 "bmc"|grep -m1 "ip"|cut -d: -f2|tr -d [:space:]
        # sed -i "${line_to_replace},/ip:*/ s|ip:.*|ip: ${primary_host}|" ${_cluster_sls_path}
        # echo "Updated cluster.sls for node-1 hostname on primary node" 2>&1|tee -a ${LOG_FILE}
        (hostnamectl status | grep Chassis | grep -q server) && {
            echo -n "Fetching and updating BMC IP for Server A..................................." 2>&1|tee -a ${LOG_FILE}
            update_bmc_ip "srvnode-1" >> ${LOG_FILE}
            echo "Ok." | tee -a ${LOG_FILE}
        }

        # Replace node-2 entry
        # Hostname
        echo -n "Updating hostname of Server B..............................................." 2>&1|tee -a ${LOG_FILE}
        local secondary_host=$(ssh -i /root/.ssh/id_rsa_prvsnr -o "StrictHostKeyChecking no" ${private_data_ip_node_2} "hostname")
        echo "DEBUG: secondary_host: ${secondary_host}" >> ${LOG_FILE}
        local line_to_replace=$(grep -m1 -n "srvnode-2:" ${_cluster_sls_path}|tail -1|cut -d: -f1)
        echo "DEBUG: line_to_replace: ${line_to_replace}" >> ${LOG_FILE}
        #sed -i "${line_to_replace},/hostname:*/ s|hostname:.*|hostname: ${secondary_host}|" ${_cluster_sls_path}
        provisioner pillar_set cluster/srvnode-2/hostname \"${secondary_host}\"
        echo "Ok." | tee -a ${LOG_FILE}
        # BMC IP-address
        # grep -m1 -A10 -n "srvnode-2:" ${_cluster_sls_path}|grep -m1 -A3 "bmc"|grep -m1 "ip"|cut -d: -f2|tr -d [:space:]
        # sed -i "${line_to_replace},/ip:*/ s|ip:.*|ip: ${primary_host}|" ${_cluster_sls_path}
        # echo "Updated cluster.sls for node-1 hostname on primary node" 2>&1|tee -a ${LOG_FILE}
        (hostnamectl status | grep Chassis | grep -q server) && {
            echo -n "Fetching and updating BMC IP for Server B..................................." 2>&1|tee -a ${LOG_FILE}
            update_bmc_ip "srvnode-2" "srvnode-2" >> ${LOG_FILE}
            echo "Ok." | tee -a ${LOG_FILE}
        }

        echo -n "Refreshing Salt pillar from Server A........................................" 2>&1|tee -a ${LOG_FILE}
        sleep 5
        salt "*" saltutil.refresh_pillar ${salt_opts}
        echo "Ok." 2>&1|tee -a ${LOG_FILE}
    fi
}

function recover_rabbitmq_cluster {
    # # Update RabbitMQ cluster
    echo -n "Starting rabbitmq cluster......" 2>&1|tee -a ${LOG_FILE}
    salt "srvnode-1" state.apply components.misc_pkgs.rabbitmq ${salt_opts}; sleep 5
    salt "srvnode-2" state.apply components.misc_pkgs.rabbitmq ${salt_opts}; sleep 5
    echo "Done"
}


function lock_unboxing_user {
    echo "\
**********************************************************
Please validate the following:
  1. Check if all IP addresses have been assigned [run: ip a]
  2. Check if system has been assigned a hostname [run: hostname -f]
  3. Check HA cluster is up [run: pcs status].
     All resources should show as started. Check for fails.]
**********************************************************" 2>&1 | tee -a ${LOG_FILE}

    while true; do
        read -p "Does Unboxing look good (y/n)? " _ans
        case $_ans in
            [Yy] )
                passwd --lock cortxub >> ${LOG_FILE}
                echo "\
*************************SUCCESS!!********************************
The system is ready for use.
User cortxub has been locked and can no longer be used for login.
******************************************************************" 2>&1 | tee -a ${LOG_FILE}
                break;;
            [Nn] )
              echo "\
**************************FAILED!!********************************
User feels unboxing is incomplete.
Exiting unboxing process...
******************************************************************" 2>&1 | tee -a ${LOG_FILE}
              exit 10;;
            * ) echo "Please answer y or n.";;
        esac
    done
}

function remove_boxing_flag {
    if [[ -f '/opt/seagate/cortx/provisioner/generated_config/boxed' ]]
    then
        echo "DEBUG: Boxed file found. Removing boxed file." >> ${LOG_FILE}
        rm -f /opt/seagate/cortx/provisioner/generated_config/boxed || true
    else
        echo "\
ERROR: Boxing command was not run
       Please ensure that the boxing sequence was done before doing unboxing.
***** FAILED!! *****
The detailed logs are kept at: ${LOG_FILE}" | tee -a ${LOG_FILE}
    fi
}

