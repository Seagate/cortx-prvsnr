#!/bin/sh
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#


# Functions in this file address following:
#   1. Stop rabbitmq cluster, if running
#   2. Ref: Stop and restart a RabbitMQ cluster, RMQ clustering
#   3. Ensure Lnet service is stopped
#   4. Collect system-wide support bundle using CSM CLI interface
#   5. Backup files
#       a. /etc/multipath/bindings
#       b. /etc/multipath.conf
#   6. Configure service port
#   7. Unmount /var/motr and SWAP? (This should be ideally taken care of by OS shutdown)
#   8. Cleanup /tmp
#   9. Create unboxing user.
#   10. Create boxing flag file on primary node:
#       /opt/seagate/cortx/provisioner/generated_config/boxed
#       Creating file on only one node ensures that the unboxing is executed only on primary node.
set -euE

export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/boxing_prov_tasks.log}"
mkdir -p $(dirname "${LOG_FILE}")

PRVSNR_ROOT="/opt/seagate/cortx/provisioner"
export salt_opts="--no-color --out-file=${LOG_FILE} --out-file-append"
subscription_enabled=false

export pvt_ip_a="${pvt_ip_a:-"192.168.0.1"}"
export pvt_ip_b="${pvt_ip_b:-"192.168.0.2"}"
pvt_ips=("${pvt_ip_a}" "${pvt_ip_b}")
server_names=("Server A" "Server B")
export ssh_cmd="ssh -q -o ConnectTimeout=5 -o PreferredAuthentications=publickey -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa_prvsnr"
export scp_cmd="scp -q -o ConnectTimeout=5 -o PreferredAuthentications=publickey -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa_prvsnr"

export interface_mgmt_a=
export interface_data_a=
export interface_mgmt_b=
export interface_data_b=

function trap_handler {
    echo -e "\n***** ERROR! *****"
    echo "For detailed error logs, please see: $LOG_FILE"
    echo "******************"
}
trap trap_handler ERR

export gfs_vol_prvsnr_data="${gfs_vol_prvsnr_data:-"volume_prvsnr_data"}"
export gfs_vol_salt_cache_jobs="${gfs_vol_salt_cache_jobs:-"volume_salt_cache_jobs"}"
export mount_dir_salt_cache="${mount_dir_salt_cache:-"/srv/glusterfs/volume_salt_cache_jobs"}"
export mountpt_salt_cache_vol="${mountpt_salt_cache_vol:-"/var/cache/salt/master/jobs"}"
export mountpt_prvsnr_data_vol="${mountpt_prvsnr_data_vol:-"/var/lib/seagate/cortx/provisioner/shared"}"
export mount_dir_prvsnr_data="${mount_dir_prvsnr_data:-"/srv/glusterfs/volume_prvsnr_data"}"


export hostname_a=$(hostname)
export hostname_b=$($ssh_cmd ${pvt_ip_b} hostname)

function get_nw_interface_names {
    interface_mgmt_a=$(salt-call pillar.get cluster:srvnode-1:network:mgmt_nw:iface:0 --output=newline_values_only)
    interface_data_a=$(salt-call pillar.get cluster:srvnode-1:network:data_nw:iface:0 --output=newline_values_only)
    interface_mgmt_b=$(salt-call pillar.get cluster:srvnode-2:network:mgmt_nw:iface:0 --output=newline_values_only)
    interface_data_b=$(salt-call pillar.get cluster:srvnode-2:network:data_nw:iface:0 --output=newline_values_only)
}

function stop_rabbitmq_cluster {
    echo -n "INFO: Removing RabbitMQ from both nodes....." | tee -a ${LOG_FILE}

    salt "*" state.apply components.misc_pkgs.rabbitmq.teardown ${salt_opts} || (
        echo -e "\nERROR: Remove RabbitMQ from both nodes failed." | tee -a ${LOG_FILE}
    ) && ( echo "Done" | tee -a ${LOG_FILE} )
}


function stop_services {
    echo "INFO: Stop LNET from both nodes if active." | tee -a ${LOG_FILE}

    echo "INFO: Stopping lnet on both nodes" | tee -a ${LOG_FILE}
    salt "*" service.stop lnet ${salt_opts} || (echo "ERROR: Failed to stop LNET from both nodes." | tee -a ${LOG_FILE})

    echo "INFO: Stopped LNET from both nodes if active." | tee -a ${LOG_FILE}
}


function backup_files {
    echo -n "INFO: Backing up files on both nodes....." | tee -a ${LOG_FILE}

    bkp_file_list=(
        "/etc/multipath/bindings"
        "/etc/multipath.conf"
    )
    for file in "${bkp_file_list[@]}"; do
        cp $file $file.bak
    done

    echo "Done." | tee -a ${LOG_FILE}
}


function configure_service_port {
    salt "srvnode-1" state.apply components.system.network.mgmt.service_port ${salt_opts} || (
        echo "ERROR: Failed to configure service port on srvnode-1." | tee -a ${LOG_FILE}
        exit 25
    )
}


function update_ssh_settings {
    echo -n "INFO: Updating ssh settings on both nodes....." | tee -a ${LOG_FILE}

    salt "*" state.apply components.system.config.sshd_boxing ${salt_opts} || (
        echo "ERROR: Failed to update ssh settings from both nodes." | tee -a ${LOG_FILE}
        exit 1
    )

    echo "Done." | tee -a ${LOG_FILE}
}


function dump_unboxing_data {
    set -euE
    _user="$1"
    _secret="$2"

    _create_date=$(date '+%Y-%m-%d')
    _output_file="/root/Lyve_rack_SystemID_${_create_date}.txt"
    _lr_serial=-

    ###### Lyve Rack Serial Number ######
    echo -n "INFO: Getting Lyve Rack serial numbers " | tee -a ${LOG_FILE}
    [[ -f /opt/seagate/lr-serial-number ]] && _lr_serial=$(cat /opt/seagate/lr-serial-number) || echo "WARNING: Lyve Rack serial number has not been provided"

    ###### Server A #####
    echo -n "INFO: Getting serial numbers and mac addresses for server A.........." | tee -a ${LOG_FILE}
    _serial_a=`dmidecode -t system | grep Serial | cut -d: -f 2`
    _mac_bmc_a=`ipmitool lan print | grep "MAC Address" | awk '{ print $4 }'`
    _mac_mgmt_a=`cat /sys/class/net/${interface_mgmt_a}/address`
    _mac_data_a=`cat /sys/class/net/${interface_data_a}/address`
    echo "Done" | tee -a ${LOG_FILE}

    ###### Server B #####
    echo -n "INFO: Getting serial numbers and mac addresses for server B.........." | tee -a ${LOG_FILE}
    _serial_b=`$ssh_cmd ${pvt_ip_b} "dmidecode -t system" | grep Serial | cut -d: -f 2`
    _mac_bmc_b=`$ssh_cmd ${pvt_ip_b} "ipmitool lan print" | grep 'MAC Address' | awk '{ print $4 }'`
    _mac_mgmt_b=`$ssh_cmd ${pvt_ip_b} "cat /sys/class/net/${interface_mgmt_b}/address"`
    _mac_data_b=`$ssh_cmd ${pvt_ip_b} "cat /sys/class/net/${interface_data_b}/address"`
    echo "Done" | tee -a ${LOG_FILE}

    echo "\
**************************************************
NOTE: Store following information for unboxing.
**************************************************" 2>&1 | tee -a ${LOG_FILE}

    cat <<EOL > ${_output_file}
**************************************************
*             Lyve Drive Rack                    *
**************************************************
*         Lyve Drive Rack System ID              *
*------------------------------------------------*
*                                                *
* Lyve Rack Serial number  : $_lr_serial         *
*                                                *
**************************************************


**************************************************
*    CORTX credentials for initial setup         *
-------------------------------------------------*
  user     : $_user
  password : $_secret

  NOTE: Password expires on first login.

**************************************************


**************************************************
*                 Server A                       *
*------------------------------------------------*
  Serial Number            : $_serial_a
  Management Interface MAC : $_mac_mgmt_a
  BMC Interface MAC        : $_mac_bmc_a
  Data Interface MAC       : $_mac_data_a

**************************************************


**************************************************
*                 Server B                       *
*------------------------------------------------*
  Serial Number            : $_serial_b
  Management Interface MAC : $_mac_mgmt_b
  BMC Interface MAC        : $_mac_bmc_b
  Data Interface MAC       : $_mac_data_b

**************************************************
EOL
    cat ${_output_file} 2>&1 | tee -a ${LOG_FILE}

    echo -e "\n\
NOTE: The above system details are required for unboxing and is stored at: $_output_file
      Please replace SystemID with actual System ID in the file name before shipping." | tee -a ${LOG_FILE}

    echo -e "\nThe cluster nodes are going to shutdown now, please copy the above details\n" | tee -a ${LOG_FILE}

    while true; do
        read -p "Have you copied the above details?" _ans
        case $_ans in
            [Yy]* ) break;;
            [Nn]* ) echo "Please copy and press y to proceed..."; continue;;
            * ) echo "Please answer y or n.";;
        esac
    done
}

function reset_vips {
    #Remove VIPs for data and management network from Salt pillars

    echo "Removing the VIPs from Salt configuration" | tee -a ${LOG_FILE}
    echo "Fetching Data VIP from cluster pillar" >> ${LOG_FILE}
    current_cip=$(salt 'srvnode-1' pillar.get cluster:cluster_ip --output=newline_values_only)
    if [[ ! -z $current_cip ]]; then
        echo "Removing Data VIP from cluster pillar" >> ${LOG_FILE}
        provisioner pillar_set cluster/cluster_ip \"\" --logfile --logfile-filename ${LOG_FILE}
    else
        echo "Data VIP already reset from Salt configuration" | tee -a ${LOG_FILE}
    fi
    echo "Fetching Management VIP from cluster pillar" >> ${LOG_FILE}
    current_vip=$(salt 'srvnode-1' pillar.get cluster:mgmt_vip --output=newline_values_only)
    if [[ ! -z $current_vip ]]; then
        echo "Removing Management VIP from cluster pillar" >> ${LOG_FILE}
        provisioner pillar_set cluster/mgmt_vip \"\" --logfile --logfile-filename ${LOG_FILE}
    else
        echo "Management VIP already reset from Salt configuration" | tee -a ${LOG_FILE}
    fi
    echo "Done" | tee -a ${LOG_FILE}

}

function reset_pub_data_ips {
    echo "Removing the IPs of public data network from Salt configuration" | tee -a ${LOG_FILE}

    echo "Fetching public data IP for server A from cluster pillar" >> ${LOG_FILE}
    pub_dip1=$(salt 'srvnode-1' pillar.get cluster:srvnode-1/network/data_nw/public_ip_addr --output=newline_values_only)
    if [[ ! -z $pub_dip1 ]]; then
        echo "Removing public data IP for server A from cluster pillar" >> ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-1/network/data_nw/public_ip_addr \"\" --logfile --logfile-filename ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-1/network/data_nw/gateway \"\" --logfile --logfile-filename ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-1/network/data_nw/netmask \"\" --logfile --logfile-filename ${LOG_FILE}
    else
        echo "Public data IP for server A already reset from Salt configuration" | tee -a ${LOG_FILE}
    fi


    echo "Fetching public data IP for server B from cluster pillar" >> ${LOG_FILE}
    pub_dip2=$(salt 'srvnode-1' pillar.get cluster:srvnode-1/network/data_nw/public_ip_addr --output=newline_values_only)
    if [[ ! -z $pub_dip2 ]]; then
        echo "Removing public data IP for server B from cluster pillar" >> ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-2/network/data_nw/public_ip_addr \"\" --logfile --logfile-filename ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-2/network/data_nw/gateway \"\" --logfile --logfile-filename ${LOG_FILE}
        provisioner pillar_set cluster/srvnode-2/network/data_nw/netmask \"\" --logfile --logfile-filename ${LOG_FILE}
    else
        echo "Public data IP for server B already reset from Salt configuration" | tee -a ${LOG_FILE}
    fi

    echo "INFO: Resetting the IPs of public data network interface on both nodes." | tee -a ${LOG_FILE}
    if [[ ! -z $pub_dip1 && ! -z $pub_dip2 ]]; then
        salt '*' state.apply components.system.network.data.direct ${salt_opts}
    else
        echo "The IPs of public data network interface are already reset on both nodes"
    fi
}


function reset_pub_mgmt_ips {
    echo "Removing the IPs of public management network from Salt Pillar" | tee -a ${LOG_FILE}
    echo "Removing public management IP for Server A from cluster pillar" >> ${LOG_FILE}
    provisioner pillar_set cluster/srvnode-1/network/mgmt_nw/public_ip_addr PRVSNR_UNDEFINED --logfile --logfile-filename ${LOG_FILE}
    provisioner pillar_set cluster/srvnode-1/network/mgmt_nw/gateway PRVSNR_UNDEFINED --logfile --logfile-filename ${LOG_FILE}
    provisioner pillar_set cluster/srvnode-1/network/mgmt_nw/netmask PRVSNR_UNDEFINED --logfile --logfile-filename ${LOG_FILE}

    echo "Removing public management IP for Server B from cluster pillar" >> ${LOG_FILE}
    provisioner pillar_set cluster/srvnode-2/network/mgmt_nw/public_ip_addr PRVSNR_UNDEFINED --logfile --logfile-filename ${LOG_FILE}
    provisioner pillar_set cluster/srvnode-2/network/mgmt_nw/gateway PRVSNR_UNDEFINED --logfile --logfile-filename ${LOG_FILE}
    provisioner pillar_set cluster/srvnode-2/network/mgmt_nw/netmask PRVSNR_UNDEFINED --logfile --logfile-filename ${LOG_FILE}
    echo "Done" | tee -a ${LOG_FILE}
}


function boxing_flag {
    #Flag file is created only on primary node,
    # as this helps to ensure unboxing is executed only on primary node.
    echo -n "INFO: Creating flag file on primary node....." | tee -a ${LOG_FILE}

    local file_name=${1:-/opt/seagate/cortx/provisioner/generated_configs/boxed}

    if [ ! -f $file_name ]
    then
        timestamp=$(date "+%Y.%m.%d-%H.%M.%S")
        mkdir -p $(dirname "$file_name")
        echo $timestamp > $file_name
    fi

    echo "Done." | tee -a ${LOG_FILE}
}

function sub_manager_check {
    _pvt_ip="${1:-$pvt_ip_a}"
    _server="${2:-"Server A"}"

    $ssh_cmd $_pvt_ip "grep -qE \"Red Hat\" /etc/*-release" || {
        echo "${_server} is not a RedHat system" | tee -a ${LOG_FILE}
        subscription_enabled=false
        return
    }

    echo "Checking if RHEL subscription manager is enabled on ${_server}" 2>&1 | tee -a ${LOG_FILE}
    subc_list=`$ssh_cmd $_pvt_ip "subscription-manager list" | grep Status: | awk '{ print $2 }'`
    subc_status=`$ssh_cmd $_pvt_ip "subscription-manager status" | grep "Overall Status:" | awk '{ print $3 }'`
    if echo "$subc_list" | grep -q "Subscribed"; then
        if [[  "$subc_status" == "Current" ]]; then
            echo "RedHat subscription is enabled on ${_server}." 2>&1 | tee -a ${LOG_FILE}
            subscription_enabled=true
        else
            echo "RedHat subscription is disabled on ${_server}." | tee -a ${LOG_FILE}
            subscription_enabled=false
        fi
    fi
}

function sub_manager_cleanup {
    _pvt_ip="${1:-$pvt_ip_a}"
    _server=${2:-"Server A"}

    $ssh_cmd $_pvt_ip "grep -q "Red Hat" /etc/*-release" || {
        echo "${_server} is not a RedHat system" | tee -a ${LOG_FILE}
        return
    }
    echo "Cleaning up the subscription manager on ${_node}" | tee -a ${LOG_FILE}

    echo "Removing the Red Hat subscription from ${_server}" | tee -a ${LOG_FILE}
    echo "DEBUG: Running the subscription-manager auto-attach --disable on ${_server}" >> ${LOG_FILE}
    $ssh_cmd $_pvt_ip "subscription-manager auto-attach --disable || true"  | tee -a ${LOG_FILE}

    echo "Running subscription-manager remove --all on ${_server}" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip "subscription-manager remove --all || true" | tee -a ${LOG_FILE}

    echo "Running subscription-manager unregister on ${_server}" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip "subscription-manager unregister || true" | tee -a ${LOG_FILE}

    echo "Running subscription-manager clean on ${_server}" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip "subscription-manager clean || true" | tee -a ${LOG_FILE}

    echo "Running subscription-manager config --rhsm.manage_repos=0 on ${_server}" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip "subscription-manager config --rhsm.manage_repos=0" | tee -a ${LOG_FILE}
}

function seagate_refs_cleanup {
    # 1. Check if subscription manager is enabled
    # 2. Disable and cleanup the subscription
    # 3. Remove all Seagate internal repos from /etc/yum.repos.d

    for i in "${!pvt_ips[@]}"; do
        echo "seagate_refs_cleanup(): Running for node: ${server_names[$i]}" >> ${LOG_FILE}
        if $ssh_cmd ${pvt_ips[$i]} 'grep -q "Red Hat" /etc/*-release'; then
            echo "RedHat system, checking if the subscription is enabled" >> ${LOG_FILE}
            subscription_enabled=false
            sub_manager_check "${pvt_ips[$i]}" "${server_names[$i]}"
            if [[ "$subscription_enabled" == true ]]; then
                sub_manager_cleanup "${pvt_ips[$i]}" "${server_names[$i]}"
            fi
        else
            echo "Non RedHat system, disabling the subscription is not needed" >> ${LOG_FILE}
        fi
        _n_repos=$($ssh_cmd ${pvt_ips[$i]} "ls -1 /etc/yum.repos.d/*.repo 2>/dev/null | wc -l")
        echo "DEBUG: _n_repos=$_n_repos" >> ${LOG_FILE}
        if [[ $_n_repos -ne 0 ]]; then
            echo "Checking if there is salt or seagate references in repos" >> ${LOG_FILE}
            if $ssh_cmd ${pvt_ips[$i]} 'grep -lEq "cortx|seagate|salt" /etc/yum.repos.d/*.repo'; then
                $ssh_cmd ${pvt_ips[$i]} 'for file in `grep -lE "cortx|seagate|salt" /etc/yum.repos.d/*.repo`;\
                    do if ! grep -q "file://" $file; then\
                            echo "Removing repo file: $file";
                            mkdir -p /opt/seagate/cortx/provisioner/generated_configs/repos_backup_boxing;
                            yes | mv -f "$file" /opt/seagate/cortx/provisioner/generated_configs/repos_backup_boxing/;
                            rm -f "$file";
                        fi;
                    done'
            fi
            $ssh_cmd ${pvt_ips[$i]} 'for file in `grep -lE "baseurl=None|baseurl=/3rd_party|baseurl=/cortx_iso" /etc/yum.repos.d/*.repo`;\
                    do\
                    echo "Removing repo: $file";\
                    yes | mv -f "$file" /opt/seagate/cortx/provisioner/generated_configs/repos_backup_boxing;\
                    done'  >> ${LOG_FILE}
            echo "Cleaning yum cache on ${server_names[$i]}" | tee -a ${LOG_FILE}
            $ssh_cmd ${pvt_ips[$i]} "yum clean all || true"  >> ${LOG_FILE}
        else
            echo "No repos found with seagate references, ignoring" >> ${LOG_FILE}
        fi
        echo "Done" | tee -a ${LOG_FILE}
    done
}

function remove_gfs_mounts {
    #1. Stop salt services
    #2. Stop Gluster volumes
    #3. Remove gluster bricks
    #4. Delete volumes
    #5. Detach the gluster peer
    #6. unmount volumes mountpoints
    #7. Remove entries from fstab

    #echo "Stopping Salt services on Server A" | tee -a ${LOG_FILE}
    #systemctl stop salt-master 2>&1 | tee -a ${LOG_FILE}
    #systemctl stop salt-minion 2>&1 | tee -a ${LOG_FILE}

    #echo "Stopping Salt services on Server B" | tee -a ${LOG_FILE}
    #$ssh_cmd ${pvt_ip_b} "systemctl stop salt-master" 2>&1 | tee -a ${LOG_FILE}
    #$ssh_cmd ${pvt_ip_b} "systemctl stop salt-minion" 2>&1 | tee -a ${LOG_FILE}

    echo -e "\n------ Stopping gluster volume: ${gfs_vol_prvsnr_data} ------" | tee -a ${LOG_FILE}
    echo y | gluster volume stop ${gfs_vol_prvsnr_data} 2>&1 | tee -a ${LOG_FILE}

    echo -e "\n------ Stopping gluster volume: ${gfs_vol_salt_cache_jobs} ------" | tee -a ${LOG_FILE}
    echo y | gluster volume stop ${gfs_vol_salt_cache_jobs} 2>&1 | tee -a ${LOG_FILE}

    if gluster volume info ${gfs_vol_salt_cache_jobs} | grep "${hostname_b}:${mount_dir_salt_cache}"; then
        echo -e "\n------ Removing gluster brick: ${hostname_b}:${mount_dir_salt_cache} ------" | tee -a ${LOG_FILE}
        $ssh_cmd ${pvt_ip_b} "echo y | gluster volume remove-brick ${gfs_vol_salt_cache_jobs} replica 1 ${hostname_b}:${mount_dir_salt_cache} force"
    else
        echo "gluster brick ${hostname_b}:${mount_dir_salt_cache} is not available, ignoring" | tee -a ${LOG_FILE}
    fi

    if gluster volume info ${gfs_vol_prvsnr_data} | grep "${hostname_b}:${mount_dir_prvsnr_data}"; then
        echo -e "\n------ Removing gluster brick: ${hostname_b}:${mount_dir_prvsnr_data} ------" | tee -a ${LOG_FILE}
        $ssh_cmd ${pvt_ip_b} "echo y | gluster volume remove-brick ${gfs_vol_prvsnr_data} replica 1 ${hostname_b}:${mount_dir_prvsnr_data} force"
    else
        echo "gluster brick ${hostname_b}:${mount_dir_prvsnr_data} is not available, ignoring" | tee -a ${LOG_FILE}
    fi

    echo -e "\n------ Deleting gluster volumes ------" | tee -a ${LOG_FILE}
    if gluster volume list | grep ${gfs_vol_prvsnr_data}; then
        echo "deleting gluster volume ${gfs_vol_prvsnr_data}" >> ${LOG_FILE}
        echo y | gluster volume delete ${gfs_vol_prvsnr_data}
    else
        echo "gluster volume ${gfs_vol_prvsnr_data} not available, skipping" >> ${LOG_FILE}
    fi
    if gluster volume list | grep ${gfs_vol_salt_cache_jobs}; then
        echo "deleting gluster volume ${gfs_vol_salt_cache_jobs}" >> ${LOG_FILE}
        echo y | gluster volume delete ${gfs_vol_salt_cache_jobs}
    else
        echo "gluster volume ${gfs_vol_salt_cache_jobs} not available, skipping" >> ${LOG_FILE}
    fi

    echo -e "\n------ Detaching the gluster peer ------" | tee -a ${LOG_FILE}
    if gluster peer status | grep $hostname_b; then
        echo y | gluster peer detach ${hostname_b} force
    fi

    echo -e "\n------ Unmounting the gluster volumes from Server A ------" | tee -a ${LOG_FILE}
    if mount | grep -q ${mountpt_salt_cache_vol}; then umount ${mountpt_salt_cache_vol}; fi
    if mount | grep -q ${mountpt_prvsnr_data_vol}; then umount ${mountpt_prvsnr_data_vol}; fi

    echo -e "\n------ Unmounting the gluster volumes from Server B ------" | tee -a ${LOG_FILE}
    $ssh_cmd ${pvt_ip_b} "if mount | grep -q ${mountpt_salt_cache_vol}; then umount ${mountpt_salt_cache_vol}; fi"
    $ssh_cmd ${pvt_ip_b} "if mount | grep -q ${mountpt_prvsnr_data_vol}; then umount ${mountpt_prvsnr_data_vol}; fi"

    #Remove entries from stab Server A
    echo -e "\n------ Removing the gluster mountpoints from fstab on Server A ------" | tee -a ${LOG_FILE}
    yes | cp -f /etc/fstab /opt/seagate/cortx/provisioner/generated_configs/fstab_server_a.org
    sed -i.bak "\@^$hostname_a:$gfs_vol_prvsnr_data@d" /etc/fstab 2>&1 | tee -a ${LOG_FILE}
    sed -i.bak1 "\@^$hostname_a:$gfs_vol_salt_cache_jobs@d" /etc/fstab 2>&1 | tee -a ${LOG_FILE}

    # Remove entries from stab Server B
    # For simplicity copy fstab of Server B to Server A, remove
    # gluster vol entries and copy it back to Server B.
    echo -e "\nRemoving the gluster mountpoints from fstab on Server B" | tee -a ${LOG_FILE}
    $scp_cmd ${pvt_ip_b}:/etc/fstab /opt/seagate/cortx/provisioner/generated_configs/fstab_server_b.org
    $scp_cmd ${pvt_ip_b}:/etc/fstab /tmp/fstab_b
    sed -i.bak "\@^$hostname_a:$gfs_vol_prvsnr_data@d" /tmp/fstab_b 2>&1 | tee -a ${LOG_FILE}
    sed -i.bak1 "\@^$hostname_a:$gfs_vol_salt_cache_jobs@d" /tmp/fstab_b 2>&1 | tee -a ${LOG_FILE}
    $scp_cmd /tmp/fstab_b ${pvt_ip_b}:/etc/fstab 2>&1 | tee -a ${LOG_FILE}

    echo -e "\nDone" | tee -a ${LOG_FILE}
}

function cleanup_hosts {
    # Remove entries for management IP from /etc/hosts

    remote_mgmt_ip_srvnode_1=$(ssh_over_pvt_data ${pvt_ip_a} "ip -4 address show dev ${interface_mgmt_a} | grep inet | head -1 | awk '{print \$2}' | awk -F '/' '{print \$1}'")
    remote_mgmt_ip_srvnode_2=$(ssh_over_pvt_data ${pvt_ip_b} "ip -4 address show dev ${interface_mgmt_b} | grep inet | head -1 | awk '{print \$2}' | awk -F '/' '{print \$1}'")

    echo "Removing management IPs from /etc/hosts"
    sed -i "s/${remote_mgmt_ip_srvnode_1}.*//g" /etc/hosts
    sed -i "s/${remote_mgmt_ip_srvnode_2}.*//g" /etc/hosts

    echo "Copying updated /etc/hosts to Server node B"
    scp -o "UserKnownHostsFile=/dev/null" -o "StrictHostKeyChecking=no" -i /root/.ssh/id_rsa_prvsnr /etc/hosts "${pvt_ip_b}":/etc/hosts ||
        echo "scp command failed to copy files to ${pvt_ip_b} / 
        Check if ssh command works: /
            ssh -o \"StrictHostKeyChecking=no\" -i /root/.ssh/id_rsa_prvsnr ${pvt_ip_b} 'hostname'
        Check if /etc/hosts file exists on current node: /
            ls -l /etc/hosts
        "
}
