#!/bin/bash
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing, 
# please email opensource@seagate.com or cortx-questions@seagate.com."
#


# Script to run end to end deployment of Cortx for Single and Dual node setup on VM.

set -euE

export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/auto-deploy-cortx.log}"
mkdir -p $(dirname "${LOG_FILE}")
# /usr/bin/true > $LOG_FILE
truncate -s 0 ${LOG_FILE}

PRVSNR_ROOT="/opt/seagate/cortx/provisioner/"

trap_handler ()
{
    echo -e "\n***** FAILED!!*****" 2>&1 | tee -a $LOG_FILE
    echo "Detailed error log is kept at: $LOG_FILE" 2>&1 | tee -a $LOG_FILE
    exit 1
}
trap trap_handler ERR

srvnode_1_host=`hostname`
srvnode_2_host=
srvnode_2_passwd=
tgt_build=
cluster_ip=
mgmt_vip=
pub_din=
pvt_din=
mgmt_din= 
pvt_dip1=
pvt_dip2=
singlenode=false

srvnode_2_host_opt=false
srvnode_2_passwd_opt=false
tgt_build_opt=false
cluster_ip_opt=false
pub_din_opt=false
pvt_din_opt=false
mgmt_din_opt=false
mgmt_vip_opt=false
pvt_dip1_opt=false
pvt_dip2_opt=false

usage()
{
    echo "\
Usage:
For Dual Node:
sh auto-deploy-vm -s <secondary node hostname (srvnode-2)> -p <password for sec node>
                  -t <target build url for CORTX>
For Single Node:
sh auto-deploy-vm -S -t <target build url for CORTX> 

Optional Arguments:
   -C	 <Hostname/IP> 	  Hostname/IP address for Public Data n/w interface.
                          This IP will be assigned to the n/w interface
                          provided with -C option.
                          if you not provide this ip, cluster_ip will consider as null
   -M	 <Hostname/IP> 	  Hostname/IP address for Management n/w interface.
                          This IP will be assigned to the n/w interface
                          provided with -M option.
                          if you not provide this ip, mgmt_vip will consider as null
   -m    <NETWORK IF>     Management n/w interface name (default eth0)
   -n    <NETWORK IF>     Public data n/w interface name (default eth2)
   -N    <NETWORK IF>     Private data n/w interface name (default eth1).
                          Network interface for direct connect.
   -i    <IP Address>     IP address for pvt data n/w interface name on node-1.
                          This IP will be assigned to the eth1 n/w interface.
                          if you not provide this ip, pvt_ip_addr will consider as null
                          (default null)
   -I    <IP Address>     IP address for pvt data n/w interface name on node-2,
                          This IP will be assigned to the eth1 n/w interface name.
                          if you not provide this ip, pvt_ip_addr will consider as null
                          (default null)
"
}

help()
{
  echo "\
----------- Caveats -------------
1. The command must be run from primary node in the cluster.
2. Ensure the setup is clean and no cortx rpms are installed.
3. Ensure the ~/.ssh directory is empty.
-------- Sample command ---------
For Dual Node:
$ sh auto-deploy-vm -s <secondary_node> -p seagate -C <cluster_ip> -M <management_vip> [-i 192.168.100.101] [-I 192.168.100.102] -t http://cortx-storage.colo.seagate.com/releases/eos/github/release/rhel-7.7.1908/http://cortx-storage.colo.seagate.com/releases/eos/github/release/rhel-7.7.1908/last_successful/
For Single Node:
$ sh auto-deploy-vm -S -C <cluster_ip> -M <management_vip> [-i 192.168.100.101] -t  http://cortx-storage.colo.seagate.com/releases/eos/github/release/rhel-7.7.1908/http://cortx-storage.colo.seagate.com/releases/eos/github/release/rhel-7.7.1908/last_successful/

"
}

while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help) usage; help; exit 0
        ;;
        -s)
            srvnode_2_host_opt=true
            [ -z "$2" ] &&
                echo "Error: srvnode-2 not provided" && exit 1;
            srvnode_2_host="$2"
            shift 2 ;;
        -p)
            srvnode_2_passwd_opt=true
            [ -z "$2" ] &&
                echo "Error: srvnode-2 password not provided" && exit 1;
            srvnode_2_passwd="$2"
            shift 2 ;;
        -S)
	    singlenode=true
            shift 1 ;;
        -C)
            cluster_ip_opt=true
            [ -z "$2" ] &&
                echo "Error: Cluster ip is not provided" && exit 1;
            cluster_ip="$2"
            shift 2 ;;
        -n)
            pub_din_opt=true
            [ -z "$2" ] &&
                echo "Error: public data interface name not provided" && exit 1;
            pub_din="$2"
            shift 2 ;;
        -N)
            pvt_din_opt=true
            [ -z "$2" ] &&
                echo "Error: private data interface name not provided" && exit 1;
            pvt_din="$2"
            shift 2 ;;
        -m)
            mgmt_din_opt=true
            [ -z "$2" ] &&
                echo "Error: management interface name not provided" && exit 1;
            mgmt_din="$2"
            shift 2 ;;
        -D)
            meta_disk_opt=true
            [ -z "$2" ] &&
                echo "Error: Meta disk name not provided" && exit 1;
            meta_disk="$2"
            shift 2 ;;
        -d)
            data_disk_opt=true
            [ -z "$2" ] &&
                echo "Error: Data disks name not provided" && exit 1;
            data_disk="$2"
            shift 2 ;;
	-M)
	    mgmt_vip_opt=true
	    [ -z "$2" ] &&
		echo "Error: Mgmt_vip is not provided" && exit 1;
	    mgmt_vip="$2"
	    shift 2 ;;
	-i)
	    pvt_dip1_opt=true
	    [ -z "$2" ] &&
		echo "Error: srvnode-1 pvt_ip_addr for data not provided" && exit 1;
	    pvt_dip1="$2"
	    shift 2 ;;
	-I)
	    pvt_dip2_opt=true
	    [ -z "$2" ] &&
		echo "Error: srvnode-2 pvt_ip_addr for data not provided" && exit 1;
	    pvt_dip2="$2"
	    shift 2 ;;
        -t)
            tgt_build_opt=true
            [ -z "$2" ] &&
                echo "Error: Target build not provided" && exit 1;
            tgt_build="$2"
            shift 2 ;;
        *) echo "Invalid option $1"; usage; exit 1;;
    esac
done

if [[ "$singlenode" == false ]]; then
    if [[ "$srvnode_2_host_opt" == false ||
          "$srvnode_2_passwd_opt" == false ||
          "$tgt_build_opt" == false ]]; then

	echo "Insufficient input provided"
        usage
        exit 1
    fi
elif [[ "$tgt_build_opt" == false ]]; then
    echo "Insufficient input provided"
    usage
    exit 1
fi

if [[ "$singlenode" == false ]]; then
    ssh_tool="/usr/bin/sshpass"
    ssh_base_cmd="/bin/ssh"
    ssh_opts="-o UserKnownHostsFile=/dev/null\
              -o StrictHostKeyChecking=no -o LogLevel=error"
    user=root
    ssh_cred="$ssh_tool -p $srvnode_2_passwd"
    ssh_cmd="$ssh_base_cmd $ssh_opts $user@$srvnode_2_host"
    remote_cmd="$ssh_cred $ssh_cmd"
	
    ssh_tool_pkg=$(basename $ssh_tool)
    [ -f "$ssh_tool" ] || {
	echo "Installing $ssh_tool_pkg"
	yum install -y $ssh_tool_pkg
	}
fi

install_prvsnr_cli()
{
    # Install cortx-prvsnr-cli rpm from cortx-storage and install it on both nodes
    target_node="$1"
    echo "Target Node: $target_node" 2>&1|tee -a $LOG_FILE
    if [[ "$target_node" = "srvnode-1" ]]; then
	rpm -qa | grep -q 'cortx' && {
        echo "ERROR: cortx packages are already installed"
        echo "Please clean-up previous installtion from both the nodes and retry"
        exit 1
        }
        yum install -y $tgt_build/$(curl -s $tgt_build/|grep cortx-prvsnr-cli-1.0.0|sed 's/<\/*[^"]*"//g'|cut -d\" -f1)
        systemctl stop firewalld || true
        return 0
    else
        $remote_cmd <<-EOF
            set -eu
            rpm -qa | grep -q 'cortx' && {
            echo "ERROR: cortx packages are already installed"
            echo "Please clean-up previous installtion from both the nodes and retry"
            exit 1
            }
            yum install -y $tgt_build/$(curl -s $tgt_build/|grep cortx-prvsnr-cli-1.0.0|sed 's/<\/*[^"]*"//g'|cut -d\" -f1)
            systemctl stop firewalld || true
	EOF
    fi
}

install_config_prvsnr()
{   
    if [[ "$singlenode" == false ]]; then
	set -eu
	echo -e "\n\t***** INFO: Installing cortx-prvsnr-cli on node-2 *****" 2>&1 | tee -a $LOG_FILE
	sleep 1
	install_prvsnr_cli "srvnode-2"
    fi
    echo -e "\n\t***** INFO: Installing cortx-prvsnr-cli on node-1*****" 2>&1 | tee -a $LOG_FILE
    sleep 1
    install_prvsnr_cli "srvnode-1"

    #export PATH=/usr/local/bin:$PATH
    #if [[ "$singlenode" == false ]]; then
    #    $remote_cmd export PATH=/usr/local/bin:$PATH
    #fi
    
    # Run setup provisioner
    echo -e "\n\t***** INFO: Running setup-provisioner *****" 2>&1 | tee -a $LOG_FILE
    sleep 1
    if [[ "$singlenode" == false ]]; then
        echo -e "\n\tRunning sh /opt/seagate/cortx/provisioner/cli/src/setup-provisioner"\
	        "--srvnode-2='$srvnode_2_host' $tgt_build" 2>&1 | tee -a $LOG_FILE
        sh /opt/seagate/cortx/provisioner/cli/src/setup-provisioner --srvnode-2="$srvnode_2_host" $tgt_build 2>&1 | tee -a $LOG_FILE
	echo "Done" 2>&1 | tee -a $LOG_FILE
    else
        echo -e "\n\tRunning sh /opt/seagate/cortx/provisioner/cli/src/setup-provisioner"\
	        "-S --salt-master="$srvnode_1_host" $tgt_build" 2>&1 | tee -a $LOG_FILE
        sh /opt/seagate/cortx/provisioner/cli/setup-provisioner -S --salt-master="$srvnode_1_host" $tgt_build 2>&1 | tee -a $LOG_FILE
        echo "Done" 2>&1 | tee -a $LOG_FILE
    fi	
}

update_pillar()
{
    # Update cluster.sls
    echo -e "\n\t***** INFO: Updating cluster.sls *****" 2>&1 | tee -a $LOG_FILE
    /usr/local/bin/provisioner pillar_set cluster/srvnode-1/hostname \"`hostname -f`\"
    /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/data_nw/roaming_ip \"\"

    if [[ "$mgmt_din_opt" == true ]]; then
        /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/mgmt_nw/iface \"$mgmt_din\"
    else
        /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/mgmt_nw/iface \"eth0\"
    fi

    if [[ "$pub_din_opt" == true && "$pvt_din_opt" == true ]]; then
        /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/data_nw/iface [\"$pub_din\",\"$pvt_din\"]
    else
        /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/data_nw/iface [\"eth1\",\"eth2\"]
    fi

    if [[ "$pvt_dip1_opt" == true ]]; then
        /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/data_nw/pvt_ip_addr \"$pvt_dip1\"
    else
        /usr/local/bin/provisioner pillar_set cluster/srvnode-1/network/data_nw/pvt_ip_addr \"\"
    fi

    disk_list=($(lsblk -ndpl --output NAME))    

    /usr/local/bin/provisioner pillar_set cluster/srvnode-1/storage/metadata_device [\"${disk_list[-2]}\"]

    /usr/local/bin/provisioner pillar_set cluster/srvnode-1/storage/data_devices [\"${disk_list[-1]}\"]

    if [[ "$singlenode" == false ]]; then
        /usr/local/bin/provisioner pillar_set cluster/srvnode-2/hostname \"$srvnode_2_host\"
        /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/data_nw/roaming_ip \"\"
        
        if [[ "$mgmt_din_opt" == true ]]; then
            /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/mgmt_nw/iface \"$mgmt_din\"
        else
            /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/mgmt_nw/iface \"eth0\"
        fi

        if [[ "$pub_din_opt" == true && "$pvt_din_opt" == true ]]; then
            /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/data_nw/iface [\"$pub_din\",\"$pvt_din\"]
        else
            /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/data_nw/iface [\"eth1\",\"eth2\"]
        fi

        if [[ "$pvt_dip2_opt" == true ]]; then
            /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/data_nw/pvt_ip_addr \"$pvt_dip2\"
        else
            /usr/local/bin/provisioner pillar_set cluster/srvnode-2/network/data_nw/pvt_ip_addr \"\"
        fi

        disk_list=($($remote_cmd lsblk -ndpl --output NAME))

        /usr/local/bin/provisioner pillar_set cluster/srvnode-2/storage/metadata_device [\"${disk_list[-2]}\"]

        /usr/local/bin/provisioner pillar_set cluster/srvnode-2/storage/data_devices [\"${disk_list[-1]}\"]


    fi

    # Updating Cluster IP
    if [[ "$cluster_ip_opt" == true ]]; then
        echo -e "\n\t Updating Cluster IP " 2>&1|tee -a $LOG_FILE
        /usr/local/bin/provisioner pillar_set cluster/cluster_ip \"$cluster_ip\"
    fi

    # Updating Mgmt VIP
    if [[ "$mgmt_vip_opt" == true ]]; then
        echo -e "\n\t Updating Mgmt VIP " 2>&1|tee -a $LOG_FILE
        /usr/local/bin/provisioner pillar_set cluster/mgmt_vip \"$mgmt_vip\"
    fi	
	
    # Changing Meta and Data disks based on lsblk
    #echo -e "\n\t***** INFO: Changing Meta and Data disks based on lsblk *****" 2>&1|tee -a $LOG_FILE
    #os_name=`cat /etc/redhat-release`
    #if grep -q "Red Hat" <<< $os_name ; then
    # 	 disks=$(lsblk | grep -E ^v.*"disk" | cut -c1-3);
    #else
    #    disks=$(lsblk | grep -E ^v.*"disk" | cut -c1-3 | sed "1 d");
    #fi
    #meta_flag=0
    #data_disk=''
    #for disk in $disks
    #do
    #    if [ $meta_flag == 0 ] ; then
    #        meta_flag=1;
    #        meta_disk=\"\/dev\/$disk\"
    #    else
    #        data_disk+=\"\/dev\/$disk\",
    #    fi
    #done
    #data_disk=${data_disk: :-1}
    #/usr/local/bin/provisioner pillar_set cluster/srvnode-1/storage/metadata_device [$meta_disk]
    #/usr/local/bin/provisioner pillar_set cluster/srvnode-1/storage/data_devices [$data_disk]
    #if [[ "$singlenode" == false ]]; then
    #    /usr/local/bin/provisioner pillar_set cluster/srvnode-2/storage/metadata_device [$meta_disk]
    #    /usr/local/bin/provisioner pillar_set cluster/srvnode-2/storage/data_devices [$data_disk]
    #fi
    	
    # Update s3server.sls
    echo -e "\n\t***** INFO: Updating s3server.sls *****" 2>&1 | tee -a $LOG_FILE
    /usr/local/bin/provisioner pillar_set s3server/no_of_inst '1'
    salt-call pillar.get cluster
}

install_config_prvsnr
update_pillar

# Run deploy_cortx
echo -e "\n\t***** INFO: Running deploy-cortx *****"
sleep 5
if [[ "$singlenode" == false ]]; then
    sh /opt/seagate/cortx/provisioner/cli/src/deploy-vm -v
else
    sh /opt/seagate/cortx/provisioner/cli/src/deploy-vm -S 
fi
echo -e "\n***** SUCCESS!! *****" 2>&1 | tee -a $LOG_FILE
echo " Check following logs to see the complete logs of auto-deploy-cortx: $LOG_FILE" 2>&1 | tee -a $LOG_FILE
echo "Done"
