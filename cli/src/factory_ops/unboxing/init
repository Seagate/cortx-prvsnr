#!/bin/sh
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#


set -euE

export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/unboxing.log}"
mkdir -p $(dirname "${LOG_FILE}")

function trap_handler {
    echo "\
**************************FAILED!!********************************
For detailed error logs, please see: $LOG_FILE
******************************************************************" | tee -a ${LOG_FILE}
}
trap trap_handler ERR

function intrpt_handler {
    echo -e "\n\n\
------------------------------------------------------------------
Received Ctrl-c signal, exiting Gracefully.
For detailed logs, please see: $LOG_FILE
------------------------------------------------------------------" | tee -a ${LOG_FILE}

    exit 1
}
trap intrpt_handler SIGTERM SIGINT

BASEDIR=$(dirname "${BASH_SOURCE}")
static_data_ip_a=
static_data_ip_b=
cluster_vip=
management_vip=

export salt_opts="--no-color --out-file=${LOG_FILE} --out-file-append"

. ${BASEDIR}/network_init
. ${BASEDIR}/pre_unbox
. ${BASEDIR}/system_check
. ${BASEDIR}/config_update


function usage {
    echo "\

    Usage:
        $0 -M <management_vip> -C <data_network_vip>

    Command Args:
        -M    <IP ADDRESS>   Static vip on management network
        -C    <IP ADDRESS>   Static vip on data network
    "
}

function help {
  echo "\
    ----------- Caveats -------------
    1. The command must be run from primary node in the cluster.
    2. Mandetory arguments:
        a. Management VIP:      Static vip on management network
        b. Data Network VIP:    Static vip on data network

    -------- Sample commands ---------

    1. Unbox Cortx setup with following Manangement and Data VIPs:
       1. Management VIP   : 10.20.100.201
       2. Data network VIP : 172.19.100.100

       $ sudo /opt/seagate/cortx/provisioner/cli/factory_ops/unboxing/init -M 10.20.100.201 -C 172.19.100.100

       Note: It is assumed that public data network IPs on both servers were assigned by DHCP.

    2. Unbox Cortx setup with following Management & Data VIPs:
       1. Management VIP   : 10.20.100.201
       2. Data network VIP : 172.19.100.100

       $ sudo /opt/seagate/cortx/provisioner/cli/factory_ops/unboxing/init -M 10.20.100.201 -C 172.19.100.100

         Note: In this case it is assumed that there were no IPs assigned to the public data interfaces
               on both servers.
    "
}

function die {
    echo >&2 "$@"
    usage
    exit 1
}

# Parse the input arguments

while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help) usage; help; exit 0
        ;;

        -C)
            [ -z "$2" ] && die "Error: Data network VIP not provided";
            cluster_vip="$2"
            shift 2
            ;;

        -M)
            [ -z "$2" ] && die "Error: Management VIP not provided";
            management_vip="$2"
            shift 2
            ;;
        # --Ia)
        #     [ -z "$2" ] && die "Error: Static IP for public data network on server A is not provided";
        #     static_data_ip_a="$2"
        #     shift 2
        #     ;;
        # --Ib)
        #     [ -z "$2" ] && die "Error: Static IP for public data network on server B is not provided";
        #     static_data_ip_b="$2"
        #     shift 2
        #     ;;
        *) echo "Invalid option $1"; usage; exit 1;;
    esac
done

time_stamp=$(date)
 echo "*********************************************************" | tee -a ${LOG_FILE}
 echo "          Unboxing the Cortx Lyve Drive Rack          " | tee -a ${LOG_FILE}
 echo "*********************************************************" | tee -a ${LOG_FILE}
 echo "DEBUG: run time: $time_stamp" >> ${LOG_FILE}


# Validate the input parameters
if [[ -z "$cluster_vip" || -z "$management_vip" ]]; then
    # error out
    echo "ERROR: Invalid input provided - Management VIP or Data network VIP is missing. Exiting" | tee -a ${LOG_FILE}
    usage
    echo -e "\tRun '$0 -h' for detailed help"
    exit 1
fi

if [ ! -z "$static_data_ip_a" -a -z "$static_data_ip_b" ]; then
    echo -e "\n\
ERROR: Static IP for server B is not provided.
       Please provide the static IPs for both servers or skip them both if they are already assigned by DHCP" | tee -a ${LOG_FILE}
    usage
    echo -e "\tRun '$0 -h' for detailed help" | tee -a ${LOG_FILE}
    exit 1
elif [ -z "$static_data_ip_a" -a ! -z "$static_data_ip_b" ]; then
    echo -e "\n\
ERROR: Static IP for server A is not provided.
       Please provide the static IPs for both servers or skip them both if they are already assigned by DHCP" | tee -a ${LOG_FILE}
    usage
    echo -e "\tRun '$0 -h' for detailed help" | tee -a ${LOG_FILE}
    exit 1
elif [ -z "$static_data_ip_a" -a -z "$static_data_ip_b" ]; then
    echo -e "\n\
INFO: Static IPs for public data networks are not provided.
      Assuming they are already assigned by DHCP" | tee -a ${LOG_FILE}
elif [ ! -z "$static_data_ip_a" -a ! -z "$static_data_ip_b" ]; then
    echo -e "\n\
INFO: Static IPs for public data networks received:
      server A                 : $static_data_ip_a
      server B                 : $static_data_ip_b" | tee -a ${LOG_FILE}
fi

# Proceed only if boxing flag set
check_boxing_flag

echo -e "\n\
      Management VIP           : $management_vip
      Data VIP                 : $cluster_vip" | tee -a ${LOG_FILE}

# Perform basic system check
check_hostname

check_pvt_data_connectivity
check_mgmt_ip

# Check IP addresses for public data interfaces are set
check_public_data_interfaces

# Check BMC is accessible on both servers
check_bmc_accessibility

# Check Server-A and Server-B can access (ping) Controller-A and Controller-B on the enclosure
check_controller_ip

# Run pre_unbox validations
pre_unbox

#echo "Running provisioner replace node api" >> ${LOG_FILE}
#provisioner replace_node --node-host $pvt_ip_b --logfile --logfile-filename ${LOG_FILE}
#echo "Done with provisioner replace node api" >> ${LOG_FILE}

# Check salt services is required as one of initial steps
# This is requried to fetch pillar data on master node
# check_salt_services

# Update /root/.ssh/config file with hosts
#update_ssh_config taken care of in pre_unbox
echo "Configuring gluster volumes on both nodes" | tee -a ${LOG_FILE}
configure_gluster_vols

echo "Configuring the Salt services on both nodes" | tee -a ${LOG_FILE}
update_salt_minion

update_cluster_sls "${management_vip}" "${cluster_vip}" "${static_data_ip_a}" "${static_data_ip_b}"

echo -n "Clean the Salt cache........................................................" | tee -a ${LOG_FILE}
salt '*' saltutil.clear_cache ${salt_opts}
sleep 2
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Refreshing the Salt modules................................................." | tee -a ${LOG_FILE}
salt '*' saltutil.refresh_modules ${salt_opts}
sleep 2
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Syncing all states for Salt................................................." | tee -a ${LOG_FILE}
salt '*' saltutil.sync_all ${salt_opts}
sleep 2
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Refreshing the Salt pillar.................................................." | tee -a ${LOG_FILE}
salt '*' saltutil.refresh_pillar ${salt_opts}
sleep 2
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Refreshing the grains......................................................." | tee -a ${LOG_FILE}
salt '*' saltutil.refresh_grains ${salt_opts}
sleep 2
echo "Ok." | tee -a ${LOG_FILE}

# Re-configure firewall
echo -n "Re-configuring firewall................................." | tee -a ${LOG_FILE}
salt "*" state.apply system.firewall.teardown ${salt_opts}
salt "*" state.apply system.firewall ${salt_opts}
# sleep 5     # Mindfulness break
echo "Ok." | tee -a ${LOG_FILE}

if ! command -v pcs ; then
    echo "[ERROR  ]: Command 'pcs' not found" 2>&1 | tee -a ${LOG_FILE}
    exit 1
fi

echo "Starting Cortx cluster" 2>&1 | tee -a ${LOG_FILE}
pcs cluster start --all 2>&1 | tee -a ${LOG_FILE}
sleep 2

# if [[ ! -z "$static_data_ip_a" && ! -z "$static_data_ip_b" ]]; then
#     # Assign Static IPs on both the nodes
#     echo -n "Assigning static IPs to public data network................................." | tee -a ${LOG_FILE}
#     salt "*" state.apply system.network.data.direct ${salt_opts}
#     sleep 5     # Mindfulness break
#     echo "Ok." | tee -a ${LOG_FILE}
# fi

# Update ClusterIP
echo -n "Configuring Data Network VIP................................................" | tee -a ${LOG_FILE}
salt "srvnode-1" state.apply ha.corosync-pacemaker.config.cluster_ip ${salt_opts}
sleep 5     # Mindfulness break
echo "Ok." | tee -a ${LOG_FILE}

# Update HAProxy config
echo -n "Updating new VIPs in haproxy configuration.................................." | tee -a ${LOG_FILE}
salt "*" state.apply ha.haproxy.config ${salt_opts}
sleep 5     # Mindfulness break
echo "Ok." | tee -a ${LOG_FILE}

# Update Management_vip
echo -n "Configuring Management Network VIP.........................................." | tee -a ${LOG_FILE}
salt "srvnode-1" state.apply ha.corosync-pacemaker.config.mgmt_vip ${salt_opts}
sleep 5     # Mindfulness break
echo "Ok." | tee -a ${LOG_FILE}

# Re-run Stonith
echo -n "Fencing the cluster with Stonith............................................" | tee -a ${LOG_FILE}
salt "srvnode-1" state.apply ha.corosync-pacemaker.config.stonith ${salt_opts}
sleep 5     # Mindfulness break...Breathe in...Breathe out
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Updating Management VIP in CSM HA configuration file........................" | tee -a ${LOG_FILE}
salt "*" state.apply csm.prepare ${salt_opts}
echo "Ok." | tee -a ${LOG_FILE}

echo "Removing build URL from Salt configuration" | tee -a ${LOG_FILE}
target_build=$(salt-call pillar.get release:target_build --output=newline_values_only)
if [[ $target_build != file:///* ]]; then
    provisioner pillar_set release/target_build \"\" --logfile --logfile-filename ${LOG_FILE}
    sleep 2
fi

echo "Removing stale URL (CentOS) from Salt configuration" | tee -a ${LOG_FILE}
provisioner pillar_set commons/cortx_commons/CentOS \"\" --logfile --logfile-filename ${LOG_FILE}
sleep 2

echo "Removing build URL (RedHat) from Salt configuration" | tee -a ${LOG_FILE}
provisioner pillar_set commons/cortx_commons/RedHat \"\" --logfile --logfile-filename ${LOG_FILE}
sleep 2

echo "Bringing HCTL cluster out of maintenance mode" 2>&1 | tee -a ${LOG_FILE}
hctl node unmaintenance --all 2>&1 | tee -a ${LOG_FILE}

echo "Ensuring the cluster is in healthy state" 2>&1 | tee -a ${LOG_FILE}
ensure_healthy_cluster

# Update SSPL init
echo -n "Configuring Cortx RAS services on server A.................................." | tee -a ${LOG_FILE}
salt "srvnode-2" state.apply sspl.config.commons ${salt_opts}; sleep 5
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Configuring Cortx RAS services on server B.................................." | tee -a ${LOG_FILE}
salt "srvnode-1" state.apply sspl.config.commons ${salt_opts}; sleep 5
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Configuring CSM services on Server B........................................" | tee -a ${LOG_FILE}
salt "srvnode-2" state.apply csm.config ${salt_opts}; sleep 5
echo "Ok." | tee -a ${LOG_FILE}

echo -n "Configuring CSM services on Server A........................................" | tee -a ${LOG_FILE}
salt "srvnode-1" state.apply csm.config ${salt_opts}; sleep 5
echo "Ok." | tee -a ${LOG_FILE}

echo "Removing Lustre URL (RedHat) from Salt configuration" | tee -a ${LOG_FILE}
provisioner pillar_set commons/repo/lustre \"\" --logfile --logfile-filename ${LOG_FILE}; sleep 2

echo "Removing the repos with Seagate URL from Server A" | tee -a ${LOG_FILE}
seagate_refs_cleanup

echo "Restarting the Cortx Services" | tee -a ${LOG_FILE}
echo "DEBUG: Bringing HCTL cluster in maintenance mode" >> ${LOG_FILE}
hctl node maintenance --all --timeout-sec=600 2>&1 | tee -a ${LOG_FILE}
echo "DEBUG: Bringing HCTL cluster out of maintenance mode" >> ${LOG_FILE}
hctl node unmaintenance --all 2>&1 | tee -a ${LOG_FILE}
sleep 5

echo "Cleaning up resource history.............." | tee -a ${LOG_FILE}
echo  -n "Cleaning stonith history.............." | tee -a ${LOG_FILE}
pcs stonith history cleanup >> ${LOG_FILE}
echo "Ok." | tee -a ${LOG_FILE}
echo  -n "Cleaning failed resource history......" | tee -a ${LOG_FILE}
pcs resource cleanup --all >> ${LOG_FILE}
echo "Ok." | tee -a ${LOG_FILE}


# lock unboxing user
lock_unboxing_user

service_user=$(salt srvnode-2 pillar.get system:service-user:name --output=newline_values_only)
if [[ -n "$service_user" ]]; then
    echo -n "Activating service user $service_user on the Server B node................." | tee -a ${LOG_FILE}
    salt "srvnode-2" state.single user.present name="$service_user" expire=-1 ${salt_opts}
    sleep 5     # Mindfulness break
    echo "Ok." | tee -a ${LOG_FILE}
else
    # backward compatibility
    echo -n "WARNING: Skipping service user activation on the Server B node, the user is not set" | tee -a ${LOG_FILE}
fi

# Unboxing SUCCESS
remove_boxing_flag

echo -ne "\nWaiting for Cortx cluster services to become ready.."
try=1; tries=60
until false
do
    if [[ "$try" -gt "$tries" ]]; then
        break
    fi
    echo -n "."
    try=$(( $try + 4 ))
    sleep 2
done
echo -e "Ok.\n"

echo "logging the output of pcs status before the cluster health check validation" >> ${LOG_FILE}
pcs status >> ${LOG_FILE}

echo "Ensuring the cluster is in healthy state" 2>&1 | tee -a ${LOG_FILE}
ensure_healthy_cluster

echo "logging the output of pcs status after the cluster health check validation" >> ${LOG_FILE}
pcs status >> ${LOG_FILE}

echo "\
************************* SUCCESS!!! **************************************

Lyve Rack cluster is successfully initialized!!

IMPORTANT:
   - Before you start using the system please ensure that the Cortx cluster
     is up and running by executing the commands in the steps mentioned above.
   - If everything is good, please proceed to start onboarding process else
     please contact the Seagate Support if anything seems wrong.

The detailed logs can be seen at: $LOG_FILE
***************************************************************************" | tee -a ${LOG_FILE}

