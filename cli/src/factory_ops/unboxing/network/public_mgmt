#!/usr/bin/bash
#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#


set -euE
export NETWORK_MANAGEMENT=1

LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/unboxing_nw_update.log}"
mkdir -p $(dirname "${LOG_FILE}")
#truncate -s 0 ${LOG_FILE}

PRVSNR_ROOT="/opt/seagate/cortx/provisioner"
SALT_OPTS="--no-color --out-file=${LOG_FILE} --out-file-append"

pvt_ip_a=$(get_pillar_data cluster:srvnode-1:network:data_nw:pvt_ip_addr)
pvt_ip_b=$(get_pillar_data cluster:srvnode-2:network:data_nw:pvt_ip_addr)

remotes=(
    ${pvt_ip_a}
    ${pvt_ip_b}
)


function backup_ifcfg {
    node=${1:-${pvt_ip_a}}
    if_name=${2:-}

    if [[ -n if_name ]]; then
        if [[ ! $(ssh_over_pvt_data ${node} "test -e /etc/sysconfig/network-scripts/ifcfg-${if_name}.boxing" >/dev/null 2>&1) ]]; then
            _linfo "Backing up ifcfg-${if_name} to ifcfg-${if_name}.boxing in dir /etc/sysconfig/network-scripts"
            ssh_over_pvt_data ${node} "cp /etc/sysconfig/network-scripts/ifcfg-${if_name} /etc/sysconfig/network-scripts/ifcfg-${if_name}.boxing"
        else
            _lerror "ifcfg-${if_name}.boxing file from previous boxing already exists. Skipping backup..."
        fi
    fi
}


function test_mgmt_ips {
    _linfo "Fetching IP of the nodes"
    mgmt_if_1=$(get_pillar_data cluster:srvnode-1:network:mgmt_nw:iface:0)
    mgmt_if_2=$(get_pillar_data cluster:srvnode-2:network:mgmt_nw:iface:0)

    remote_mgmt_ip_srvnode_1=$(ssh_over_pvt_data ${pvt_ip_a} "ip -4 address show dev ${mgmt_if_1} | grep inet | head -1 | awk '{print \$2}' | awk -F '/' '{print \$1}'")
    remote_mgmt_ip_srvnode_2=$(ssh_over_pvt_data ${pvt_ip_b} "ip -4 address show dev ${mgmt_if_1} | grep inet | head -1 | awk '{print \$2}' | awk -F '/' '{print \$1}'")
    
    # Test the IP that is set
    _linfo "Ping management IP ${remote_mgmt_ip_srvnode_2} of Server-B from Server-A to test sanity."
    ping -c1 -W2 -I"${remote_mgmt_ip_srvnode_1}" "${remote_mgmt_ip_srvnode_2}" && (
        _linfo "IP ${remote_mgmt_ip_srvnode_2} set on Server-B is reachable over management network." || (
            _lerror "IP ${remote_mgmt_ip_srvnode_2} set on Server-B is not reachable over management network."
            exit 21
        )
    )
    
    _linfo "Ping management IP ${remote_mgmt_ip_srvnode_1} of Server-A from Server-B to test sanity."
    ssh_over_pvt_data ${pvt_ip_b} "ping -c1 -W2 -I${remote_mgmt_ip_srvnode_2} ${remote_mgmt_ip_srvnode_1}" && ( 
        _linfo "IP ${remote_mgmt_ip_srvnode_1} set on Server-A is reachable over management network." || (
            _lerror "IP ${remote_mgmt_ip_srvnode_1} set on Server-A is not reachable over management network."
            exit 21
        )
    )
}


function update_cluster_pillar_for_mgmt_ips {
    ip_1=${1:-}
    ip_2=${2:-}
    gw_ip=${3:-}
    netmask=${4:-}
    search_domain=${5:-}
    dns_server=${6:-}

    # Configuration is static, update pillar with provided IPs
    # If Configuration is DHCP. Set blank

    if [[ -e "/srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls" ]]; then
        # Get line number for management network section start for srvnode-1
        # occurance_1=$(grep -m2 -n mgmt_nw /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls|head -1|cut -d: -f1)
        # # Update pillar file
        # sed -in "${occurance_1},/public_ip_addr/ s/public_ip_addr:.*/public_ip_addr: \"${ip_1}\"/p" /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls
        # sed -in "${occurance_1},/gateway/ s/gateway:.*/gateway: \"${gw_ip}\"/p" /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls
        # sed -in "${occurance_1},/netmask/ s/netmask:.*/netmask: \"${netmask}\"/p" /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls

        provisioner pillar_set --local cluster/srvnode-1/network/mgmt_nw/public_ip_addr \"${ip_1}\"
        provisioner pillar_set --local cluster/srvnode-1/network/mgmt_nw/gateway \"${gw_ip}\"
        provisioner pillar_set --local cluster/srvnode-1/network/mgmt_nw/netmask \"${netmask}\"
        provisioner pillar_set --local cluster/srvnode-1/network/mgmt_nw/netmask \"${search_domain}\"
        provisioner pillar_set --local cluster/srvnode-1/network/mgmt_nw/netmask \"${dns_server}\"

        # Get line number for management network section start for srvnode-2
        # occurance_2=$(grep -m2 -n mgmt_nw /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls|tail -1|cut -d: -f1)
        # # Update pillar file
        # sed -in "${occurance_2},/public_ip_addr/ s/public_ip_addr:.*/public_ip_addr: \"${ip_2}\"/p" /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls
        # sed -in "${occurance_2},/gateway/ s/gateway:.*/gateway: \"${gw_ip}\"/p" /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls
        # sed -in "${occurance_2},/netmask/ s/netmask:.*/netmask: \"${netmask}\"/p" /srv/glusterfs/volume_prvsnr_data/srv/pillar/groups/all/uu_cluster.sls

        provisioner pillar_set --local cluster/srvnode-2/network/mgmt_nw/public_ip_addr \"${ip_2}\"
        provisioner pillar_set --local cluster/srvnode-2/network/mgmt_nw/gateway \"${gw_ip}\"
        provisioner pillar_set --local cluster/srvnode-2/network/mgmt_nw/netmask \"${netmask}\"
        provisioner pillar_set --local cluster/srvnode-2/network/mgmt_nw/netmask \"${search_domain}\"
        provisioner pillar_set --local cluster/srvnode-2/network/mgmt_nw/netmask \"${dns_server}\"

        # Gluster would take care of this after unboxing
        _linfo "pvt_ip_b: ${pvt_ip_b}"
        ssh -o "StrictHostKeyChecking=no" -i /root/.ssh/id_rsa_prvsnr "${pvt_ip_b}" "mkdir -p /var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all"
        scp -o "StrictHostKeyChecking=no" -i /root/.ssh/id_rsa_prvsnr /var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all/* "${pvt_ip_b}":/var/lib/seagate/cortx/provisioner/local/srv/pillar/groups/all/
    else
        _lerror "Cluster pillar file for user data doesn't exist. Exiting..."
        exit 41
    fi
}


function set_mgmt_nw_config {
    local srvnode=
    local node_name=

    for remote in ${remotes[@]}; do
        if [[ ${remote} == ${pvt_ip_a} ]]; then
            srvnode=srvnode-1
            node_name=Server-A
        elif [[ ${remote} == ${pvt_ip_b} ]]; then
            srvnode=srvnode-2
            node_name=Server-B
        else
            # This should never be hit
            _lerror "This should never be seen. If you see this, there is something unexpected heppening."
            exit 23
        fi

        mgmt_if=$(get_pillar_data cluster:${srvnode}:network:mgmt_nw:iface:0)
        _linfo "Setting up network on ${srvnode} for interface: ${mgmt_if}"
        backup_ifcfg ${remote} ${mgmt_if}
        
        _linfo "Setting /etc/resolv.conf on ${node_name} for interface: ${mgmt_if}"
        ssh_over_pvt_data ${remote} "salt-call --local state.apply components.system.network ${SALT_OPTS}"
        _linfo "Setting ifcfg-${mgmt_if} on ${node_name}"
        ssh_over_pvt_data ${remote} "salt-call --local state.apply components.system.network.mgmt.public ${SALT_OPTS}"

    done
}


# 2.  If DHCP
#     2.1.  Set cluster entries for public management IP to None
#     2.2.  Update /etc/resolv.conf using dhclient
function set_mgmt_dhcp_config {
    _linfo "================================================================================"
    _linfo "Setting Management Network to DHCP"
    _linfo "================================================================================"

    # Reset cluster value for management network
    _linfo "    "
    _linfo "You have selected to proceed with DHCP based configuration for management network interface."
    _linfo "    "
    proceed_check
    _linfo "    "

    # srvnode-1
    _linfo "Preparing cluster pillar for setting management network configuration to DHCP."    
    # Update pillar
    update_cluster_pillar_for_mgmt_ips

    # Set network config to DHCP for pillar value cluster:srvnode-1:network:mgmt_nw:iface:0
    set_mgmt_nw_config
    
    # Sanity check
    test_mgmt_ips

    _linfo "================================================================================"
    _linfo "        "
}


# 3.  If Static
#     3.1.  IP address for public management interface (E.g. for eno1) <Mandatory>:
#           for srvnode-1 & srvnode-2
#     3.2.  Gateway for management network <Mandatory>
#     3.3.  BMC IP <Mandatory>
#           for srvnode-1 & srvnode-2
function set_mgmt_static_config {
    _linfo "================================================================================"
    _linfo "Setting Management Network to Static"
    _linfo "================================================================================"
    _linfo "    "

    _linfo "You have selected to proceed with static IP based configuration for management network interface."
    _linfo "    "

    mgmt_if_1=$(get_pillar_data cluster:srvnode-1:network:mgmt_nw:iface:0)
    mgmt_if_2=$(get_pillar_data cluster:srvnode-2:network:mgmt_nw:iface:0)
    
    # User inputs
    read -p "Management IP for ${mgmt_if_1} on Server-A: " mgmt_ip_1
    read -p "Management IP for ${mgmt_if_2} on Server-B: " mgmt_ip_2
    read -p "Gateway IP for Management interfaces on both nodes: " gw_ip
    read -p "DNS search domain for both nodes: " search_domain
    read -p "DNS server IP for both nodes: " dns_server
    read -p "Netmask for Management interfaces on both nodes [255.255.252.0]: " netmask
    [[ -z ${netmask} ]] && netmask=255.255.252.0

    _linfo "********************************************************************************"
    _linfo "You have provided the following information:"
    _linfo "    Management IP for interface ${mgmt_if_1} on Server-A:   ${mgmt_ip_1}"
    _linfo "    Management IP for interface ${mgmt_if_2} on Server-B:   ${mgmt_ip_2}"
    _linfo "    Gateway IP for both servers:                            ${gw_ip}"
    _linfo "    Netamsk for both servers:                               ${netmask}"
    _linfo "    Search domain for both servers:                         ${search_domain}"
    _linfo "    DNS server IP for both servers:                         ${dns_server}"
    _linfo "********************************************************************************"
    _linfo "    "
    _linfo "Given the above information, 
        we shall now proceed to configure Management Network interface with static IP 
        and related configuration."
    _linfo "    "

    proceed_check

    if [[ -z ${mgmt_ip_1} ]]; then
        _lerror "No value provided for Management IP for interface ${mgmt_if_1}. Exiting..."
        exit 31
    elif [[ -z ${mgmt_ip_2} ]]; then
        _lerror "No value provided for Management IP for interface ${mgmt_if_2}. Exiting..."
        exit 32
    elif [[ -z ${gw_ip} ]]; then
        _lerror "No value provided for Management Gateway IP. Exiting..."
        exit 33
    else
        # Do nothing
        :
    fi

    # Reset cluster value for management network
    # srvnode-1
    _linfo "Preparing cluster pillar for setting management network configuration to static on srvnode-1"
    # provsioner pillar_set provisioner pillar_set cluster/srvnode-1/network/mgmt_nw/public_ip_addr \"${mgmt_ip_1}\"
    update_cluster_pillar_for_mgmt_ips ${mgmt_ip_1} ${mgmt_ip_2} ${gw_ip} ${netmask} ${search_domain} ${dns_server}

    set_mgmt_nw_config
    
    # Sanity check
    test_mgmt_ips

    _linfo "================================================================================"
    _linfo "        "
}
