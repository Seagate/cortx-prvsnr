# Provisioner Test Scenarios

**Table of Contents**

Table column description
- **Status**: proposed/validated/needs attention
- **Jira Story**: link to the original Jira Story (feature)
- **Jira Test**: link to the Jira test ticket
- **Comments**: If needs attention, why? Any special note for the particular case, like specific environment treatment

| Status      | Jira Story  | Jira Test   | Test Scenario | Comment |
| ----------- | ----------- | ----------- | ------------- | ------- |
| Proposed    | [EOS-16883](https://jts.seagate.com/browse/EOS-16883) | | [EOS-15766 Test Resume update process after reboot](#eos-15766-test-resume-update-process-after-reboot)                                                                                                  |             |
| Proposed    | [EOS-15751](https://jts.seagate.com/browse/EOS-15751) | | [EOS-15751 Test - Setup R2 upgrade release bundle](#eos-15751-setup-r2-upgrade-release-bundle)                                                                                                           |             |
| Proposed    | [EOS-16927](https://jts.seagate.com/browse/EOS-16927) | | [EOS-16927 New API for upgrade: adjust rollback logic for R2 (singlenode)](#eos-16927-new-api-for-upgrade-adjust-rollback-logic-for-r2-singlenode)                                                       |             |
| Completed   | [EOS-18337](https://jts.seagate.com/browse/EOS-18337) |[EOS-19546](https://jts.seagate.com/browse/EOS-19546) | [EOS-18337: Make bootstrap API salt states for cortx repositories available in deployed environment](#eos-18337-make-bootstrap-api-salt-states-for-cortx-repositories-available-in-deployed-environment) |    No issue reported        |
| Completed    | [EOS-17831](https://jts.seagate.com/browse/EOS-17831) | [EOS-18993](https://jts.seagate.com/browse/EOS-18993) | [EOS-17831: Bootstrap Storage-Set-ID & Machine-ID assignment](#eos-17831-bootstrap-storage-set-id-and-machine-id-assignment)                                                                             |     No issue reported        |
| Completed    | [EOS-17235](https://jts.seagate.com/browse/EOS-17235) | [EOS-17765](https://jts.seagate.com/browse/EOS-17765)| [EOS-17235, EOS-16869: Destroy cortx on single node vm](#eos-17235-eos-16869-destroy-cortx-on-single-node-vm)                                                                                            |   Found issue during testing [EOS-19218](https://jts.seagate.com/browse/EOS-19218)          |
| Completed    | [EOS-18270](https://jts.seagate.com/browse/EOS-18270) | [EOS-18868](https://jts.seagate.com/browse/EOS-18868)| [EOS-18270: 3-node VM deployment implementation and integration Phase-2 3rd Party SW](#eos-18270-3-node-vm-deployment-implementation-and-integration-phase-2-3rd-party-sw)                               |       Found issue during testing [EOS-19127](https://jts.seagate.com/browse/EOS-19127)      |
| Proposed    | [EOS-13889](https://jts.seagate.com/browse/EOS-13889) | | [EOS-13889: Move generated configs directory in provsioner](#eos-13889-move-generated-configs-directory-in-provsioner)                                                                                   |             |
| Proposed    | [EOS-16883](https://jts.seagate.com/browse/EOS-16883) | | [EOS-16883: SW Upgrade structure validation](#eos-16883-sw-upgrade-structure-validation)                                                                                                                 |             |
| Proposed    | [EOS-18732](https://jts.seagate.com/browse/EOS-18732) | | [EOS-18732: SW upgrade bundle check-sum verification](#eos-18732-sw-upgrade-bundle-check-sum-verification)                                                                                               |             |
| Proposed    | [EOS-18743](https://jts.seagate.com/browse/EOS-18743) | | [EOS-18743: RELEASE.INFO file exists and has the necessary scheme](#eos-18743-releaseinfo-file-exists-and-has-the-necessary-scheme)                                                                      |             |
| Completed    | [EOS-17115](https://jts.seagate.com/browse/EOS-17115) | [EOS-18988](https://jts.seagate.com/browse/EOS-18988)| [EOS-17115: Add additional keys to cluster ConfStor](#eos-17115-add-additional-keys-to-cluster-confstore)                                                                                                |      Found issue during testing [EOS-19090](https://jts.seagate.com/browse/EOS-19090)       |
| Completed    | [EOS-18917](https://jts.seagate.com/browse/EOS-18917) | [EOS-19525](https://jts.seagate.com/browse/EOS-19525)| [EOS-18917: Creation of custom grains for enclosure_id,netmask, serial number and multipath](#eos-18917-creation-of-custom-grains-for-enclosure_id,-netmask,-serial-number-and-multipath)                |      No issue found       |
| Completed   | [EOS-17828](https://jts.seagate.com/browse/EOS-17828) | [EOS-18990](https://jts.seagate.com/browse/EOS-18990)| [EOS-17828: Port adjustments for 3 node](#eos-17828-port-adjustments-post-discussions-for-3-node)                | Found issue during testing [EOS-19589](https://jts.seagate.com/browse/EOS-19589)            |
| Completed   | [EOS-18920](https://jts.seagate.com/browse/EOS-18920) | [EOS-18696](https://jts.seagate.com/browse/EOS-18696)| [EOS-18920: Validations for provisioner CLI commands](#eos-18920-validations-for-provisioner-cli-commands)                |      no issues reported      |
| Completed | [EOS-19230](https://jts.seagate.com/browse/EOS-19230) | [EOS-19548](https://jts.seagate.com/browse/EOS-19548)| [EOS-19230 North Bound Command API Implementation - MGMT Network](#eos-19230-north-bound-command-api-implementation---management-network) | Found issue during testing [EOS-20284](https://jts.seagate.com/browse/EOS-20284)|
| Completed   | [EOS-18918](https://jts.seagate.com/browse/EOS-18918) | [EOS-19337](https://jts.seagate.com/browse/EOS-19337) | [EOS-18918: Create North-Bound API for node - Stage 1](#eos-18918-create-north-bound-api-for-node---stage-1)  | Found issue during testing [EOS-19490](https://jts.seagate.com/browse/EOS-19490)|
| Completed    | [EOS-19231](https://jts.seagate.com/browse/EOS-19231) | [EOS-19540](https://jts.seagate.com/browse/EOS-19540) | [EOS-19231: North Bound Command API Implementation - Data Network](#eos-19231-north-bound-command-api-implementation---data-network) |Found issue during testing [EOS-19815](https://jts.seagate.com/browse/EOS-19815) [EOS-19816](https://jts.seagate.com/browse/EOS-19816) |
| Completed    | [EOS-18494](https://jts.seagate.com/browse/EOS-18494) | [EOS-18990](https://jts.seagate.com/browse/EOS-18990)| [EOS-18494: Validation for Open Ports in Firewall - Salt Execution Module](#eos-18494-validation-for-open-ports-in-firewall---salt-execution-module)                |       No issue reported      |
| Completed    | [EOS-18921](https://jts.seagate.com/browse/EOS-18921) | [EOS-20543](https://jts.seagate.com/browse/EOS-20543)| [EOS-18921: Explode config state files to granular states](#eos-18921-explode-config-state-files-to-granular-states)                |      No issue reported       |
| Completed    | [EOS-19261](https://jts.seagate.com/browse/EOS-19261) | [EOS-20541](https://jts.seagate.com/browse/EOS-20541)| [EOS-19261: Update cluster_id API design](#eos-19261-update-cluster_id-api-design)                |   Found issue during testing [EOS-20506](https://jts.seagate.com/browse/EOS-20506)          |
| Completed    | [EOS-19898](https://jts.seagate.com/browse/EOS-19898) | [EOS-20645](https://jts.seagate.com/browse/EOS-20645) | [EOS-19898: Generate/Read LR Signature for Node Stamping](#eos-19261-generate-/-read-lr-signature-for-node-stamping)     |  No issues reported |
| Completed    | [EOS-20437](https://jts.seagate.com/browse/EOS-20437) | [EOS-20647](https://jts.seagate.com/browse/EOS-20647) | [EOS-20437: Configure Network](#eos-20437-configure-network)    |  Found issue during testing [EOS-20791](https://jts.seagate.com/browse/EOS-20791)   |
| Completed    | [EOS-20438](https://jts.seagate.com/browse/EOS-20438) | [EOS-20647](https://jts.seagate.com/browse/EOS-20647) | [EOS-20438: Factory: Manufacture LR Node: Node Initialize](#eos-20438-factory-manufacture-lr-node-node-initialize)    |    Completed as part of [EOS-20437: Configure Network](#eos-20437-configure-network) testing |
| Completed    | [EOS-20664](https://jts.seagate.com/browse/EOS-20664) | [EOS-20935](https://jts.seagate.com/browse/EOS-20935) | [EOS-20664: Field Prepare: Network-Configuration](#eos-20664-field-prepare-network-configuration)    |  Found issues during testing [EOS-21082](https://jts.seagate.com/browse/EOS-21082), [EOS-21124](https://jts.seagate.com/browse/EOS-21124)   |
| Completed    | [EOS-20668](https://jts.seagate.com/browse/EOS-20668) | [EOS-21229](https://jts.seagate.com/browse/EOS-21229) | [EOS-20668: Field Prepare: Cluster-Definition](#eos-20668-field-prepare-cluster-definition)    |  Found issue during testing [EOS-21368](https://jts.seagate.com/browse/EOS-21368)   |
| Proposed    | [EOS-20667](https://jts.seagate.com/browse/EOS-20667) | | [EOS-20667: Field Prepare: Storage Set Definition](#eos-20667-field-prepare---storage-set-definition)     |   |
| Proposed    | [EOS-20659](https://jts.seagate.com/browse/EOS-20659) | | [EOS-20659: Field Prepare: Time Configuration](#eos-20659-field-prepare---time-configuration)     |   |
| In-progress    | [EOS-20969](https://jts.seagate.com/browse/EOS-20969) | | [EOS-20969: Cluster configuration - Cortx deployment - Bootstrap](#eos-20969-cluster-configuration---cortx-deployment---bootstrap)     |   |
| Proposed    | [EOS-19686](https://jts.seagate.com/browse/EOS-19686) | | [EOS-19686: FW update not supported on vm and should displayed appropriate message](#eos-19686-FW-update-not-supported-on-vm-and-should-displayed-appropriate-message)     |   |

## EOS-15766: Test Resume update process after reboot

Step-1: Deploy Cortx software stack on VM/HW and verify update-post-boot.service is enabled
        `$systemctl status update-post-boot.service`

Step-2: Create update_states.sls pillar and add provisioner commands stating which states will run before and after reboot
        `$provisioner pillar_set update_states/pre_boot [\"command1\",\ \"command2\"]`
        `$provisioner pillar_set update_states/post_boot [\"command1\",\ \"command2\"]`

Step-3: Reboot servers

Step-4: Check update-post-boot.service status once the system's are back.
        Service should be inactive but `/opt/seagate/cortx/provisioner/cli/update-post-reboot.sh` should have got executed.
        Also commands added in update_states/post_boot should be executed.

## EOS-15751: Setup R2 upgrade release bundle

Pre-requisites:
    1. Need to be installed
        - glusterfs
        - saltstack
        - provisioner api
    2. It is enough to configure provisioner using the following command, for example:

```bash
run provisioner setup_cluster --source local --srvnode1 <IP1> --srvnode2 <IP2> --ha --logfile --logfile-filename ./setup.log --console-formatter full
```
   where \<IP1\> and \<IP2\> - ip addresses of VM nodes

Step-1: Ensure that there are not enabled or disabled `sw_upgrade_*` yum repos
```bash
yum repolist enabled
yum repolist disabled
```

Step-2:

a) For testing `set_swupgrade` command is enough to download any valid single ISO.
For example, this ISO:

http://<cortx_release_server>/releases/cortx/github/cortx-1.0/iso/cortx-1.0-280.iso

b)  To create the valid Singe SW Upgrade ISO structure is needed to create

1. `$ mkdir sw_upgrade`
2. `$ cd sw_upgrade`
3. `$ mkdir 3rdparty`
4. `$ mkdir os`
5. `$ mkdir cortx`
6. `$ mkdir python`
7. `$ sudo cortx-1.0-280.iso /mnt/iso`
8. `$ cp -r /mnt/iso ./3rdparty`
9. `$ cp -r /mnt/iso ./cortx`
10. `$ cp -r /mnt/iso ./os`
11. `$ cd ..`
12. `$ mkisofs -graft-points -r -l -iso-level 2 -J -o sw_upgrade.iso ./sw_upgrade/`

c) After the steps mentioned above we have the `sw_upgrade.iso` which can be used for the
`set_swupgrade` command testing

Step-3: Run provisioner command:

```bash
provisioner set_swupgrade_repo "./sw_upgrade.iso" --username=<provisioner_user> --password=<provisioner_user_password>`
```

Command should be finished successfully.

Step-4: Check that all yum repositories are listed in `$ yum repolist enabled` output:

Example of possible output
```bash
 yum repolist enabled

 Loaded plugins: fastestmirror, product-id, search-disabled-repos, subscription-manager
 Repository extras is listed more than once in the configuration
 Loading mirror speeds from cached hostfile
 repo id                                                             repo name                                                    status
 base                                                                base                                                         14,689+32
 cortx_commons                                                       cortx_commons                                                  334+129
 epel                                                                3rd_party_epel                                                  16,808
 extras                                                              extras                                                             451
 saltstack/7/x86_64                                                  SaltStack repo for RHEL/CentOS 7                                 93+34
 sw_upgrade_3rdparty_1.0.0-301                                       Cortx Upgrade repo 3rdparty-1.0.0-301                               22
 sw_upgrade_cortx_1.0.0-301                                          Cortx Upgrade repo cortx-1.0.0-301                                  22
 sw_upgrade_os_1.0.0-301                                             Cortx Upgrade repo os-1.0.0-301
 ```

## EOS-16927: New API for upgrade: adjust rollback logic for R2 (singlenode)

Step-1: Ensure rollback pillar data is present/updated as part of sw_update command
```bash
salt-call pillar.get rollback
```

Step-2: Run sw_rollback api
```bash
provisioner sw_rollback CORTX_VERSION
```


## EOS-18337: Make bootstrap API salt states for cortx repositories available in deployed environment

Step-1: Start the docker container

```bash
pytest test/build_helpers -k test_build_setup_env -s --root-passwd root --nodes-num 1
```

Step-2: Install provisioner API to created docker container

```bash
provisioner setup_provisioner --source local --logfile --logfile-filename ./setup.log --console-formatter full srvnode-1:172.17.0.2
```

NOTE: For **srvnode-1**, **srvnode-2**, etc. parameter values use the **Hostname** output from previous step

Step-3: Copy the `devops` directory of `cortx-prvsnr` repository

```bash
cd /PATH/TO/cortx-prvsnr/
docker cp ./devops/ <CONTAINER ID>:/opt/seagate/cortx/provisioner/devops
```

Step-4: Build core `cortx-prvsnr` RPM package

```bash
docker exec -it <CONTAINER ID> bash
bash -x /opt/seagate/cortx/provisioner/devops/rpms/buildrpm.sh -g 123456
yum install /root/rpmbuild/RPMS/x86_64/cortx-prvsnr-2.0.0-0_git123456_el7.x86_64.rpm
```

Step-4: Validate that `repos` states are available and visible for salt commands

```bash
salt '*' state.sls_exists repos.upgrade
```

Expected output should be
```text
srvnode-1:
    True
```

Also you can validate after that command that variable `$?` is set to 0:
```bash
if [ $? -eq  0 ] ; then echo "No errors" ; else echo "Some error occurred"; fi
```

NOTE: Useful command to list all salt states
```bash
salt-call state.show_top
```

## EOS-17831: Bootstrap Storage-Set-ID and Machine-ID assignment

1. Storage-set-id should be  assigned to machine

   1. Deploy single node vm  

   2. Check in  cluster storage set id is assigned  
      ```
      salt-call pillar.get cluster:srvnode-1:storage_set_id
      ```  

   3. List storage_sets  
      ```
      salt-call pillar.get cluster:storage_sets
      ```  

2. Machine_id is assigned to vm  

   1. prereq:  

      1. Take new fresh vm and check if machine id is available and make note of it  
         ```
         cat /etc/machine-id
         ```

      2. Deploy single node vm  

   2. Steps:

      1. Check if machine id  is reset
         ```
         salt-call grains.get machine_id
         ```

      2. Verify machine id from grains
         ```
         cat /etc/machine-id
         ```

      3. Check if machine_id  is set in cluster
         ```
         salt-call pillar.get cluster:srvnode-1:machine_id
         ```

Test Execution : [EOS-18994](https://jts.seagate.com/browse/EOS-18994)

## EOS-17235, EOS-16869: Destroy cortx on single node vm
   Steps : Verify if all steps are working fine from doc: https://github.com/Seagate/cortx-prvsnr/wiki/Teardown-Node .

   Test Execution : [EOS-18871](https://jts.seagate.com/browse/EOS-18871)

## EOS-18270: 3-node VM deployment implementation and integration Phase-2 3rd Party SW
Step 1: Run setup_provisioner command with --ha option
Step 2: Run deploy_vm command to deploy system and prereq states
        ```
        provisioner deploy_vm --states system --setup-type 3_node
        provisioner deploy_vm --states prereq --setup-type 3_node
        ```
Step 3: Ensure third party services are installed on all three vm'salt.

Test Execution : [EOS-18870](https://jts.seagate.com/browse/EOS-18870)

## EOS-13889: Move generated configs directory in provsioner
Step 1: Deploy single node vm
Step 2: Verify checkpoint flags are created under /opt/seagate/cortx_configs/provisioner_generated/ directory

## EOS-16883: SW Upgrade structure validation

Step-1: Create a mocked ISO bundles with RPMs:

```bash
pytest test/build_helpers/build_bundles
```

Step-2: Start the docker container

```bash
pytest test/build_helpers -k test_build_setup_env -s --root-passwd root --nodes-num 1
```

Step-3: Install provisioner API to created docker container

```bash
provisioner setup_provisioner --source local --logfile --logfile-filename ./setup.log --console-formatter full srvnode-1:172.17.0.2
```

:warning: **NOTE**: For **srvnode-1**, **srvnode-2**, etc. parameter values use the **Hostname** output from previous step

Step-4: Copy the `devops` directory of `cortx-prvsnr` repository

```bash
cd /PATH/TO/cortx-prvsnr/
docker cp ./devops/ <CONTAINER ID>:/opt/seagate/cortx/provisioner/devops
```

Step-5: Build core `cortx-prvsnr` RPM package

```bash
docker exec -it <CONTAINER ID> bash
bash -x /opt/seagate/cortx/provisioner/devops/rpms/buildrpm.sh -g 123456
yum install /root/rpmbuild/RPMS/x86_64/cortx-prvsnr-2.0.0-0_git123456_el7.x86_64.rpm
```

Step-6: Using the built single ISO SW upgrade bundle, run the command

```bash
provisioner set_swupgrade_repo /<path>/<to>/sw_upgrade_bundle.iso
```

Command should not failed. Expected output should be like the following one
```python
{'VERSION': '2.0.0', 'BUILD': 277, 'DATETIME': 'Wed Mar  3 10:02:21 UTC 2021', 'KERNEL': '5.10.19-200.fc33.x86_64', 'NAME': 'CORTX', 'OS': 'Red Hat Enterprise Linux Server release 7.7 (Maipo)', 'COMPONENTS': ['cortx-csm_agent-2.0.0-1.noarch.rpm', 'cortx-ha-2.0.0-1.noarch.rpm', 'cortx-hare-2.0.0-1.noarch.rpm', 'cortx-sspl-2.0.0-1.noarch.rpm', 'uds-pyi-2.0.0-1.noarch.rpm', 'cortx-cli-2.0.0-1.noarch.rpm', 'cortx-sspl-test-2.0.0-1.noarch.rpm', 'cortx-csm_web-2.0.0-1.noarch.rpm', 'cortx-motr-2.0.0-1.noarch.rpm', 'cortx-s3iamcli-2.0.0-1.noarch.rpm', 'cortx-s3server-2.0.0-1.noarch.rpm']}

```

Note:
To pass this test need to be sure that `sw_upgrade_bundle.iso` for SW upgrade has the following structure:

```text
sw_upgrade_bundle.iso
    - 3rd_party/
        - THIRD_PARTY_RELEASE.INFO
    - cortx_iso/
        - RELEASE.INFO
    - python_deps/
    - os/
```

**TODO**: Create a negative cases

## EOS-18732: SW upgrade bundle check-sum verification

Steps 1-5 from [EOS-16883: SW Upgrade structure validation](#eos-16883-sw-upgrade-structure-validation) are the same

Consider that SW upgrade ISO bundle is placed by the path
`/opt/iso/single_cortx.iso`

Step 6: Check MD5 hash sum:
a) Calculate MD5 hash sum for `single_cortx.iso`

```bash
md5sum /opt/iso/single_cortx.iso
fefa1db67588d2783b83f07f4f5beb5c  /opt/iso/single_cortx.iso
```

NOTE: Please, note that output of the command above will be different from presented one!

b) Using the output from the command above run the following provisioner commands one by one

```bash
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="fefa1db67588d2783b83f07f4f5beb5c"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="fefa1db67588d2783b83f07f4f5beb5c"  --hash-type="md5"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="md5:fefa1db67588d2783b83f07f4f5beb5c"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="md5:fefa1db67588d2783b83f07f4f5beb5c /opt/iso/single_cortx.iso"
```

All commands should be succeeded. And output always should be the same as like output in [EOS-16883: SW Upgrade structure validation](#eos-16883-sw-upgrade-structure-validation): Step-6

Step 7: Check sha256 hash sum:
a) Calculate sha256 hash sum:

```bash
sha256sum /opt/iso/single_cortx.iso
ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a  /opt/iso/single_cortx.iso
```

NOTE: Please, note that output of the command above will be different from presented one!

b) Using the output from the command above run the following provisioner commands one by one

```bash
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a"  --hash-type="sha256"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="sha256:ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="sha256:ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a /opt/iso/single_cortx.iso"
```

All commands should be succeeded. And output always should be the same as like output in [EOS-16883: SW Upgrade structure validation](#eos-16883-sw-upgrade-structure-validation): Step-6

Step 8: Check sha512 hash sum:
a) Calculate sha256 hash sum:

```bash
sha512sum /opt/iso/single_cortx.iso
ca6cb2dd717bbde5de59ba69b3d5178184e357b706dba87080f0c0801fe34c99efb5ba0e23b7f98b99ce3df209cff287d18214d21fa16288ab6b9ac4194ba14b  /opt/iso/single_cortx.iso
```

NOTE: Please, note that output of the command above will be different from presented one!

b) Using the output from the command above run the following provisioner commands one by one

```bash
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="ca6cb2dd717bbde5de59ba69b3d5178184e357b706dba87080f0c0801fe34c99efb5ba0e23b7f98b99ce3df209cff287d18214d21fa16288ab6b9ac4194ba14b"  --hash-type="sha512"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="sha512:ca6cb2dd717bbde5de59ba69b3d5178184e357b706dba87080f0c0801fe34c99efb5ba0e23b7f98b99ce3df209cff287d18214d21fa16288ab6b9ac4194ba14b"
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="sha512:ca6cb2dd717bbde5de59ba69b3d5178184e357b706dba87080f0c0801fe34c99efb5ba0e23b7f98b99ce3df209cff287d18214d21fa16288ab6b9ac4194ba14b /opt/iso/single_cortx.iso"
```

All commands should be succeeded. And output always should be the same as like output in [EOS-16883: SW Upgrade structure validation](#eos-16883-sw-upgrade-structure-validation): Step-6


Step 9: Test using a file with hash sum info:

a) Calculate sha256 hash sum and write the hash data to the file

```bash
sha256sum /opt/iso/single_cortx.iso
ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a  /opt/iso/single_cortx.iso

echo "sha256:ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a" >/root/hash.data
```

NOTE: Please, note that output of the command above will be different from presented one!

b) Using the file created in previous step, run the `set_swupgrade_repo` command

```bash
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash=/root/hash.data
```

Command should be succeeded. Output should be the same as like output in [EOS-16883: SW Upgrade structure validation](#eos-16883-sw-upgrade-structure-validation): Step-6

Step 10: Negative cases

a) Calculate for example sha256 hash sum
```bash
sha256sum /opt/iso/single_cortx.iso

ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a  /opt/iso/single_cortx.iso
```

NOTE: Please, note that output of the command above will be different than presented one!

b) In hash sum `ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a` need to change any hex-number.
For example, it is enough to change the last hex-digit from "a" to "b", e.g.
hash string should be like that
`ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12b`

c) Run the `set_swupgrade_repo` command with wrong hash string:

```bash
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12b"  --hash-type="sha256"
```

The exception output should be something like that and provisioner should stop command execution:
```text
'repo source "/opt/iso/single_cortx.iso" is not acceptable, reason: Catalog structure validation error occurred:
"Validation Failed. Reason: Hash sum of file \'/opt/iso/single_cortx.iso\': \'ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a\' mismatches the provided one \'ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12b\'"'
```

d) With correct hash sum `ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a` run the `set_swupgrade_repo` command with wrong `--hash-type` parameter

```bash

provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a"  --hash-type="md5"

```

Output should be the same as in c) sub-step.

e) With correct hash sum `ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a` run the `set_swupgrade_repo` command with missed hash-prefix and without `--hash-type` parameter.
Be default we use MD5 hash sum

```bash

provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a"

```

Output should be the same as in c) sub-step.

f) With correct hash sum `ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a` run the `set_swupgrade_repo` command with incorrect hash-prefix:

```bash

provisioner set_swupgrade_repo /opt/iso/single_cortx.iso --hash="sha512:ff01da01d4304729bfbad3aeca53b705c1d1d2132e94e4303c1ea210288de12a"

```

Output should be the same as in c) sub-step.


## EOS-18743: RELEASE.INFO file exists and has the necessary scheme

The positive test scenario is the same as in [EOS-16883](#eos-16883-sw-upgrade-structure-validation)

For negative test scenario need to create the wrong SW upgrade ISO bundle.
Commands for manual ISO creation are listed in - [EOS-15766](#eos-15766-test-resume-update-process-after-reboot): Step-2: b)

The better way to re-use ISO bundle created by the following command
```bash
pytest test/build_helpers/build_bundles
```

For example, it is enough to delete line which starts with `VERSION:` in file `cortx_iso/RELEASE.INFO`.
And create the new ISO file with this change.

The following command with broken `single_cortx.iso` should be failed
```bash
provisioner set_swupgrade_repo /opt/iso/single_cortx.iso
```

## EOS-18917: Creation of custom grains for enclosure_id, netmask, serial number and multipath
Step 1: Deploy single/multinode node vm
Step 2: For fetching netmask from grains.
        ```
        salt "*" grains.get ip4_netmask
        ```
Step 3: Verify netmask from individual interfaces
Step 4: For fetching enclosure-id
        ```
        salt "*" grains.get enclosure_id
        ```
Step 4: For fetching serial number
        ```
        salt "*" grains.get lr-serial-number
        ```

Test Execution : [EOS-19526](https://jts.seagate.com/browse/EOS-19526)

## EOS-17115: Add additional keys to cluster ConfStor
Step 1: Run setup provisioner on a single/multinode VM.
Step 2: Run provisioner confstore to get confstore generated
        ```
        provisioner confstore_export
        ```
Step 3: Verify the confstore is generated across all nodes.

Test Execution : [EOS-18989](https://jts.seagate.com/browse/EOS-18989)

## EOS-17828: Port adjustments post discussions for 3 node

Test scenarios for [EOS-18494](https://jts.seagate.com/browse/EOS-18494) covers all validations related to Firewall ports.
This is a simple verification to ensure only limited ports are open and mapped accordingly.

Step 1: Ensure firewall component is present in all the nodes of a 3 node VM setup. If not, install with the below command.
```bash
salt '*' state.apply components.system.firewall
```

Step 2: Verify the services with corresponding ports, are mapped to the zones, to all the nodes in the cluster, with this command.
```bash
for node in srvnode-1 srvnode-2 srvnode-3
do
    for zone in "management-zone" "public-data-zone"
    do
        ssh ${node} firewall-cmd --list-all --zone ${zone}
    done
done
```

Step 3: The same can be verified on a single node setup with the command,
```bash
for zone in "management-zone" "public-data-zone"
do
    firewall-cmd --list-all --zone ${zone}
done
```

Test Execution : [EOS-18991](https://jts.seagate.com/browse/EOS-18991)

## EOS-18494: Validation for Open Ports in Firewall - Salt Execution Module

Pre-req: This will remain a single point of verification for all open ports available in pillar data and is expected to be run as a separate step post-deployment. So, it is advised to validate this on a fully-deployed setup, for better results.
NOTE:  Custom module 'lyveutil' is renamed to 'lyveutils'.

Step 1: Ensure firewall component is present in (all) the setup node(s). If not, install with the below command.
```bash
salt '*' state.apply components.system.firewall
```

Step 2: Validate all open firewall ports and services with the below command,
```bash
salt-call lyveutils.validate_firewall
```

Negative scenario: Even if one port/service if not open from the list of all firewall ports in pillar, an error message saying `Validation of open ports failed` will pop-up.

Test Execution : [EOS-19543](https://jts.seagate.com/browse/EOS-19543)

## EOS-18920: Validations for provisioner CLI commands
   Steps : Please verify if all these scenarios are working fine from this test case doc: [Config-Validations-Scenarios](https://seagatetechnology.sharepoint.com/:x:/r/sites/eos.devops/Shared%20Documents/Provisioning/QA/Config%20Validation%20Scenarios.xlsx?d=w73fbedf43ece422ab7ce6b806d6f8f84&csf=1&web=1&e=otLPMZ) .

Test Execution : [EOS-18992](https://jts.seagate.com/browse/EOS-18992)

## EOS-19230: North Bound Command API Implementation - Management Network
Prereq: Use system with minimal configuration i.e. salt-master running and minion-id as srvnode-0

Step 1:  Set dhcp for mgmt network(eth0)
    ```
    provisioner set_mgmt_network --mgmt-interfaces [\"eth0\"] --mgmt-gateway "" --mgmt-netmask ""  --mgmt-public-ip "" --local
    ```

Step 2: Verify local pillar is set properly for above parameters
    ```
    salt-call pillar.get cluster:srvnode-0:network:mgmt --local
    ```
    Only interfaces should be updated in pillar for dhcp.

Step 3: Verify if network file is updated with BOOTPROTO="dhcp".
    network file location: `/etc/sysconfig/network-scripts/ifcfg-eth0`
    Verify time when network file is updated using below command
    ```
    ls -l /etc/sysconfig/network-scripts/ifcfg-eth0
    ```

NOTE: On vm we can apply only dhcp , if we try to set static network it will harm vm and connectivity to vm will be lost.

Test Execution : [EOS-19550](https://jts.seagate.com/browse/EOS-19550)

## EOS-19396: Ensure dependent services are started for s3 server
   Steps: Please verify if below services get started for s3 server:
      1. sshd
      2. haproxy
      3. slapd
      4. rsyslog

Test Execution : [EOS-19396](https://jts.seagate.com/browse/EOS-19396)

## EOS- 19408 : Split teardown states into granular reset and cleanup
   Ensure components teardown states are getting called. Check with below command if teardown is executing properly:
   ```bash
   salt '*' state.apply components.<component>.teardown
   ```

Test Execution : [EOS-19408](https://jts.seagate.com/browse/EOS-19408)

## EOS-18918: Create North-Bound API for node - Stage 1
Prereq: Use system with minimal configuration i.e. salt-master running and minion-id as srvnode-0

Set hostname north-bound api:

Step 1: Run set_hostname api
```bash
provisioner set_hostname --hostname "HOSTNAME" --local
```
Step 2: Verify hostname is updated in local pillar
```bash
salt-call pillar.get cluster:srvnode-0:hostname --local
```
Step 3: Verify system hostname is configured/updated
```bash
hostname
```

Set ntp north-bound api:

Step 1: Run set_ntp api
```bash
provisioner set_ntp --time-server "TIME_SERVER" --time-zone "TIME_ZONE" --local
```
Step 2: Verify time server and zone is updated in local pillar
```bash
salt-call pillar.get system:ntp --local
```
Step 3: Verify ntp is configured and system time is synchronized with time server
```bash
systemctl status chronyd
date
```

Setup firewall north-bound api:
Step 1: Run setup_firewall api
```bash
provisioner setup_firewall
```
Step 2: Verify firewall is configured as expected
```bash
systemctl status firewalld
salt-call state.apply components.system.firewall.sanity_check
```
Test Execution : [EOS-19338](https://jts.seagate.com/browse/EOS-19338)

## EOS-19231: North Bound Command API Implementation - Data Network
Prereq: Use system with minimal configuration i.e. salt-master running and minion-id as srvnode-0

Step 1:  Set dhcp for data network
```bash
provisioner set_data_network --data-public-ip "" --data-gateway "" --data-netmask "" --data-private-ip "" --data-public-interfaces [\"eth1\",\"eth2\"] --data-private-interfaces [\"eth3\",\"eth4\"] --local
```
Step 2: Verify local pillar is set properly for above parameters
```bash
salt-call pillar.get cluster:srvnode-0:network:data --local
```
Only interfaces should be updated in pillar for dhcp.

Step 3: Verify if network file is updated with BOOTPROTO="dhcp".
    network file location: `/etc/sysconfig/network-scripts/ifcfg-eth1`
    Verify time when network file is updated using below command
```bash
ls -l /etc/sysconfig/network-scripts/ifcfg-eth1
```
NOTE: On vm we can apply only dhcp , if we try to set static network it will harm vm and connectivity to vm will be lost.
Test Execution : [EOS-19541](https://jts.seagate.com/browse/EOS-19541)

## EOS-18921: Explode config state files to granular states
In case of any changes to the modules, refresh and sync the modules like here, and then execute the above command again.
```bash
salt-call saltutil.clear_cache && salt-call saltutil.sync_modules && salt-call saltutil.sync_all
```

```
Step 1: Run setup_provisioner command
Step 2: Run deploy_vm command to deploy system and prereq states
      for EOS-18921  following components will get affected with changes:
      ```bash
		provisioner deploy_vm --setup-type single --states iopath
		provisioner deploy_vm --setup-type single --states controlpath
		provisioner deploy_vm --setup-type single --states ha
      ```
step 3: check if all the servies are running
step 4: validate if config,post_install, init calls are exceuted successfully from setup.yaml

OR

use below command to install each component and make sure prepare,install,post_install,config,init_mod,start,test are getting called for each component:
```bash
salt '*' state.apply components.[comp_name]
```

follow the components sequentially:
1.motr
2.s3server
3.hare
4.sspl
5.csm
6.ha

Test Execution : [EOS-18921](https://jts.seagate.com/browse/EOS-18921) [EOS-20543](https://jts.seagate.com/browse/EOS-20543)

## EOS-19261: Update cluster_id API design
   Steps : Verify if all these scenarios are working fine from this test case doc: [ClusterID-Validations-Scenarios](https://seagatetechnology.sharepoint.com/:x:/r/sites/eos.devops/Shared%20Documents/Provisioning/QA/ClusterID%20Validation%20Scenarios.xlsx?d=wb956f3842b634a66b2bed2341786b450&csf=1&web=1&e=kScWES)

   Test Execution [EOS-20443](https://jts.seagate.com/browse/EOS-20443)

## EOS-19568: Use kafka rpm to install and service files to manage kafka
   Verify if Kafka service is running and working as part of deployment of third party softwares. If using old vm verify is everything related to kafka is getting cleaned as part of teardown. New changes are made to teardown as part of this ticket.

   Step 1: Deploy till third party software.
   step 2: verify if kafka service is active and running.

   ```bash
   systemctl status kafka
   ```
   step 3: Verify is kafka is working by creating a topic:

   ---bash
   /opt/kafka/bin/kafka-topics.sh --create --bootstrap-server srvnode-1.data.private:9092 --replication-factor 1 --partitions 1 --topic Testkafka
   ```
   step 4: deploy utils component and make sure it is successfully deployed.
   step 5: if Kafka service fails, Perform teardown to cleanup old directories and logs.

   ```bash
   salt '*' state-apply components.misc_pkgs.kafka.teardown
   ```
Test Execution : [EOS-19568](https://jts.seagate.com/browse/EOS-19568)

## EOS-19740: Provisioner: Elasticsearch deployment

   step 1: Deploy till third party software.
   step 2: Verify if elasticsearch and kibana services are active and running.

   ```bash
   systemctl status <service-name>
   ```
   step 3: Verify if elastisearch nodes are reachable.
   ```bash
   curl -XGET http://srvnode-1:9200/_cat/nodes?v
   ```
Test Execution : [EOS-19740](https://jts.seagate.com/browse/EOS-19470)

## EOS-19898: Generate/Read LR Signature for Node Stamping
   Steps : Verify if all these scenarios are working fine from this test case doc: [Factory-Node-Stamping-Scenarios](https://seagatetechnology.sharepoint.com/:x:/r/sites/eos.devops/Shared%20Documents/Provisioning/QA/Factory%20and%20Field%20API%20test%20cases/Factory-%20Node%20Stamping%20scenarios.xlsx?d=w7f7796bc488d43e1adfed4312ee86311&csf=1&web=1&e=ey4nrz)

Test Execution : [EOS-20645](https://jts.seagate.com/browse/EOS-20645)

## EOS-20437: Configure Network

step 1: Install cortx_setup
```bash
pip3 install -U git+https://github.com/Seagate/cortx-prvsnr@main#subdirectory=lr-cli/
```
step 2: Execute below commands
```bash
cortx_setup network config --mode lnet
cortx_setup network config --mode tcp
cortx_setup network config --interfaces eth1 eth2 --type data
cortx_setup network config --interfaces eth3 eth4 --type private
cortx_setup network config --interfaces eth0 --type management
```
step 3: Verify above network config is saved in confstore and pillar's
```bash
jq . /opt/seagate/cortx_configs/provisioner_cluster.json
salt-call pillar.get cluster:srvnode-0 --local
```
Test execution : [EOS-20648](https://jts.seagate.com/browse/EOS-20648)

## EOS-20438: Factory: Manufacture LR Node: Node Initialize

step 1: At factory prepare a system with all cortx-component's rpm installed

step 2: Install cortx_setup

step 3: Verify below commands execute post_install command for each component
```bash
cortx_setup node initialize
```
Test execution : [EOS-20648](https://jts.seagate.com/browse/EOS-20648)

## EOS-20664: Field Prepare: Network-Configuration

step 1: Install cortx_setup

step 2: Verify hostname is set with below command and same is updated in provisioner_cluster.json
```bash
cortx_setup node prepare network --hostname <hostname>
```
step 3: Verify below command works for each {data | private | management} network and configures the same
```bash
cortx_setup node prepare network --type <type> --ip_address <ip_address> --netmask <netmask> --gateway <gateway>
```
step 4: Verify above network details are saved in confstore and pillar's
```bash
jq . /opt/seagate/cortx_configs/provisioner_cluster.json
salt-call pillar.get cluster:srvnode-0 --local
```
Test Execution : [EOS-20936](https://jts.seagate.com/browse/EOS-20936)

## EOS-20668: Field Prepare: Cluster-Definition

Prerequisites: Ensure cortx_setup CLI is installed.

step 1: Run cluster create and network config  command
```bash
cortx_setup cluster create --name cortx_cluster --site_count 1 --storageset_count 1
cortx_setup cluster config network --dns_servers 10.230.240.51 10.230.240.52 --virtual_host 10.230.240.25 --search_domains colo.seagate.com cortx.colo.seagate.com
```
step 2: Verify if name, site_count,dns_servers,virtual_host,virtual_host and storageset_count updated in confstore and pillar.

```bash
jq . /opt/seagate/cortx_configs/provisioner_cluster.json
salt-call pillar.get cluster
```
step 3: Run cluster show command.
```bash
cortx_setup cluster show
```
step 4: Verify for single node it will show srvnode-1 in cluster nodes

Test Execution : [EOS-21229](https://jts.seagate.com/browse/EOS-21229)

## EOS-20667: Field Prepare - Storage Set Definition

   Prerequisites: Ensure cortx_setup CLI is installed.

   Steps : Verify if all these scenarios are working fine from this test case doc: [Field Prepare - StorageSet Scenarios](https://seagatetechnology.sharepoint.com/:x:/r/sites/eos.devops/Shared%20Documents/Provisioning/QA/Factory%20and%20Field%20API%20test%20cases/Field%20-%20Storageset%20scenarios.xlsx?d=w0fe585f9c9a641c8ae3835f13e573bd7&csf=1&web=1&e=9YzMWo)

## EOS-20659: Field Prepare - Time Configuration
   Prerequisites: Ensure cortx_setup CLI is installed.
   Steps:
   Step 1. Run cortx_setup node prepare time command
   ```
   cortx_setup node prepare time --server 0.in.pool.ntp.org --timezone Asia/Kolkata
   # OR
   cortx_setup node prepare time --server time.seagate.com --timezone UTC
   ```
   Step 2. Verify if the timezone is configured using timedatectl command
   ```
   [root@node1 lr-cli]# timedatectl
   Local time: Mon 2021-05-24 17:18:20 IST
   Universal time: Mon 2021-05-24 11:48:20 UTC
   RTC time: Mon 2021-05-24 11:48:19
   Time zone: Asia/Kolkata (IST, +0530)
   NTP enabled: yes
   NTP synchronized: no
   RTC in local TZ: no
   DST active: n/a
   ```

## EOS-20969: Cluster configuration - Cortx deployment - Bootstrap
   Prerequisites: Ensure cortx_setup CLI is installed.

   Step 1. Run cortx_setup cluster config command
   ```
   cortx_setup cluster config cortx <nodes_fqdn_list_to_bootstrap>
   ```
   For instance,
   ```
   cortx_setup cluster config cortx node1-hostname node2-hostname node3-hostname
   ```

   Step 2. Check if any default parameters are not present and provide them in command
   ```
   cortx_setup cluster config cortx node1-hostname node2-hostname node3-hostname --target_build <build_url>  --config_path <config_file>
   ```

   Step 3. Verify if bootstrap and environment preparation is done as expected
   ```
   [root@xxx cortx-prvsnr]# cortx_setup cluster config cortx host1.colo.seagate.com host2.colo.seagate.com host3.colo.seagate.com
   Bootstrap done for node(s): ['srvnode-1:host1.colo.seagate.com', 'srvnode-2:host2.colo.seagate.com', 'srvnode-3:host3.colo.seagate.com']
   ```

## EOS-19686: FW update not supported on vm and should displayed appropriate message
### How to test

 1. **Prerequisites**: Ensure Cortx stack has been successfully deployed and all required CSM services are running.  

 2. Complete CSM pre-boarding (https://\<system management IP\>:28100/#/preboarding/welcome) and have the CSM admin user credentials handy.  
   (**NOTE**: Continue and complete onboarding, if prompted.)  

 3. Login to CSM as admin user  

      1. Go to **Maintenance** tab  

      2. Click **Manage** link against **Firmware update**  
         The link should open a new page without any error popups.

 4. Ensure requried Salt module is loaded  
      ```bash
      salt "*" saltutil.clear_cache

         srvnode-1:
            True

      salt "*" saltutil.sync_modules

         srvnode-1:
            - modules.cluster
            - modules.commons
            - modules.component_conf_updater
            - modules.controller_cli
            - modules.cortx_unsupported_features
            - modules.gluster
            - modules.lyveutils
            - modules.pillar_ops
            - modules.prvsnr
            - modules.s3server
            - modules.setup_conf
            - modules.sspl
            - modules.sync
      ```

 5. Execute custom module to update the unsupported features registry  
      ```bash
      salt "*" cortx_unsupported_features.register component='provisioner' unsupported_feature_list='["fw_update"]' setup_types='["VM", "virtual", "other", "JBOD"]'

         srvnode-1:
            True
      ```

 6. Restart CSM services on all affected nodes  
      ```bash
      systemctl restart csm_agent csm_web
      ```

 7. Login to CSM as admin user  
      1. Go to **Maintenance** tab  
      2. Click **Manage** link against **Firmware update**  
         The link should either be missing  
         or  
         Should pop up an error **This feature is not supported on this environment**.  
