# This is CORTX GConf (Config Store) which serves as input 
# to all the CORTX Services
# Location: /etc/cortx/cluster.conf

cortx:
  external:
    consul:
      admin: admin
      endpoints:
      - tcp://consul-server.default.svc.cluster.local:8301
      - http://consul-server.default.svc.cluster.local:8500
      num_endpoints: 2
      secret: !!binary |
        MG12NU5BVjBVeWpBSWJuSV9uVjV5UzUxN3lTSVNkNUk1YkRzbjd2QkE9PQ==
    kafka:
      admin: admin
      endpoints:
      - tcp://kafka.default.svc.cluster.local:9092
      num_endpoints: 1
      secret: !!binary |
        TmRPNThMSm5GOVl5RHgwTVFfaTRJZ3k3YUUwMldmNkdxRE1KYVIxc0E9PQ==
  common:
    release:
      name: CORTX
      version: 2.0.0-5072
    security:
      device_certificate: /etc/cortx/solution/ssl/stx.pem
      domain_certificate: /etc/cortx/solution/ssl/stx.pem
      ssl_certificate: /opt/seagate/cortx/s3/install/haproxy/ssl/s3.seagate.com.pem
    service:
      admin: admin
      secret: !!binary |
        Z0FBQUFBQmhVY1dCdzJ6RkRXT3FSVjJXTWtUM0paZlB1LXFRRW0xNy1rRGhJLVpHUjcxMkVOeDZ6
        MG9KWmpsM21yTmprZTJkYTBHUU1OWVl4dnZSaVRTeEs4UEdReHhkR1E9PQ==
    storage:
      config: /etc/cortx
      local: /etc/cortx
      log: /var/log/cortx/
  utils:
    message_bus_backend: kafka
  csm:
    agent:
      num_endpoints: 1
      endpoints:
      - https://:23256
    public:                                                    # Public Control Endpoint
      num_endpoints: 1
      endpoints:
      - https://control-svc:8081
    email_address: cortx@seagate.com # Optional 
    mgmt_admin: cortxadmin
    mgmt_secret: !!binary |
      UlU1U0NrYklraS0ta3BOamN5X2J1VmUwRnJXTUFlb2JDdkItY1B3ZEE9PQ==
    limits:  
      num_services: 1  
      services:              
      - name: agent
        memory:
          min: 128Mi
          max: 256Mi
        cpu:
          min: 250m
          max: 500m
  ha:
    service:
      endpoints:
      - http://cortx-ha-svc:23501                            # HA service  endpoint
    limits:         
      num_services: 3
      services:
      - name: fault_tolerance
        memory:
          min: 128Mi
          max: 1Gi
        cpu:
          min: 250m
          max: 500m
      - name: health_monitor
        memory:
          min: 128Mi
          max: 1Gi
        cpu:
          min: 250m
          max: 500m
      - name: k8s_monitor
        memory:
          min: 128Mi
          max: 1Gi
        cpu:
          min: 250m
          max: 500m
    health:                                                    # TBD: Future. Do not use for now
      num_endpoints: 1
      endpoints:
      - http://server1-node1.com:22000
  rgw:
    auth_user: user_name
    auth_admin: sgiamadmin
    auth_secret: s3_auth_admin_secret

    thread_pool_size: 10                                       # To be tuned as we go
    data_path: /var/cortx/radosgw/$clusterid                   # Needs to be under Local PVC
    init_timeout: 300                                          # In seconds
    gc_max_objs: 32                                            # In seconds
    gc_obj_min_wait: 1800                                      # In seconds
    gc_processor_max_time: 3600                                # In seconds
    gc_processor_period: 3600                                  # In seconds
    
    motr_layout_id: 9                          
    motr_unit_size: 1048576 
    motr_max_units_per_request: 8
    motr_max_idx_fetch_count: 30
    motr_max_rpc_msg_size: 524288                              # In bytes
    motr_reconnect_interval: 10                                # Wait for Seconds before reconnect 
    motr_reconnect_retry_count: 10                             # Give up after these many retries

    public:                                                    # NOTE: was s3 earlier 
      num_endpoints: 2
      endpoints:                                               # S3 IO K8s service endpoint
      - http://s3-rgw-svc:80                                   # Optional (if http needs to be enabled)
      - https://s3-rgw-svc:443                                 # There will be one endpoint (http/https)
    service:
      num_endpoints: 2
      endpoints:                                               # RGW Service Endpoint
      - http://:22751                                          # S ports. S = # of RGW Instances
      - https://:23001
    io_max_units: 8
    max_start_timeout: <<.Values.cortx.max_start_timeout>>
    limits:
      services:
      - name: rgw
        memory:
          min: 128Mi
          max: 1Gi
        cpu:
          min: 250m
          max: 1000m 
  hare:
    hax:
      num_endpoints: 13
      endpoints:
      - https://motr-hax-svc:22003                             # Control endpoint
      - tcp://data1-node1:22001                                # For motr and Hax communication
      - tcp://data2-node1:22001
      - tcp://data1-node2:22001                                # For motr and Hax communication
      - tcp://data2-node2:22001
      - tcp://data1-node3:22001                                # For motr and Hax communication
      - tcp://data2-node3:22001 
      - tcp://server-node1:22001                               # if s3 client is configured
      - tcp://server-node2:22001                               # if s3 client is configured
      - tcp://server-node3:22001                               # if s3 client is configured
      - tcp://client-node1:22001                               # Only if motr_client is configured
      - tcp://client-node2:22001                               # Only if motr_client is configured
      - tcp://client-node3:22001                               # Only if motr_client is configured
    limits:
      num_services: 1
      services:
      - name: hax
        memory:
          min: 128Mi
          max: 1Gi
        cpu:
          min: 250m
          max: 500m
  motr:
    interface_family: inet                                     # Optional: inet (default) | inet6
    transport_type: libfab                                     # libfab | lnet (libfab==inet)
    md_size: 10                                                # % MD size w.r.t. total capacity
    ios:
      group_size: 1                                            # Number of services to be started in a group
                                                               # N = Total CVG / group_size. N must be <=24. 
      num_endpoints: 3
      endpoints:
      - tcp://data1-node1:21002                                # Format: <protocol>://<hostname_ip>:<port>
                                                               # N endpoints one for each Pod, first mentioned here
                                                               # N == Number of CVGs in the Pod
                                                               # Ports: 21002 - (21002 + Number of CVGs)
      - tcp://data2-node1:21002                                # Same thing for the second pod
      - tcp://data1-node2:21002                                # of for Pods on the other nodes
      - tcp://data2-node2:21002
      - tcp://data1-node3:21002
      - tcp://data2-node3:21002  
    confd:
      num_endpoints: 3
      endpoints:
      - tcp://data1-node1:21001                               # Format: <protocol>://<hostname_ip>:21001
      - tcp://data1-node2:21001       
      - tcp://data1-node3:21001
      - tcp://data2-node1:21001
      - tcp://data2-node2:21001
      - tcp://data2-node3:21001  
    limits:
      num_services: 2
      services:
      - name: ios
        memory:
          min: 1Gi                                             # Needs to compute this dynamically 
                                                               # min = Max. of (K * 0.5 /N, ((M/N) * 5% * (K/N)) Gi)
                                                               # where K= Num CVGs, N=Num Containers
          max: 0                                               # max = 1.1 * min. For now set to 0.
        cpu:                         
          min: 1000m                                           # 1000m * N where N = Num of "ios" Containers
          max: 1500m                                           # 1500m * N where N = Num of "ios" Containers
      - name: confd
          memory:
          min: 128Mi
          max: 512Mi
        cpu:
          min: 250m
          max: 500m
    num_clients: 2
    clients:                                                   # Motr Client Settings
      - name: rgw_s3                                           # rgw|s3|motr_client (Motr Clients)
        num_instances: 1                                       # Client instances per node (pod)
        num_endpoints: 2
        endpoints:
        - tcp://server-node1:22501                            # Format: <protocol>://<hostname_ip>:22501 
        - tcp://server-node2:22501                            # This is for client>type == "s3"
                                                               # This is a starting endpoint and num of s3
                                                               # endpoints will be equal to num of s3 instances 
                                                               # in this pod. Port range would be 
                                                               # 22501 to (22501 + "motr>client>{rgw_s3}>num_instances")
        - tcp://server-node3:22501
      - name: motr_client
        num_instances: 0                                       # Optional client. Only for Dev.
        num_subscriptions: 1
        subscriptions:                                         # NEW: Optional: Services subscribed for client
        - fdmi
        num_endpoints: 2
        endpoints:
        - tcp://client1-node1:21501                            # Port range would be 
                                                               # 21501 to (21501 + "motr>client>{motr_client}>num_instances")

cluster:
  id: 0007ec45379e36d9fa089a3d615c32a3
  name: cortx-cluster
  storage_set_count: 1
  storage_set:
  - name: storage-set-1
    durability:
      dix:
        data: '1'
        parity: '0'
        spare: '0'
      sns:
        data: '1'
        parity: '0'
        spare: '0'
    nodes:
    - 9995f539f4f770e2a3fe9e2e615c32a8
    - 1115f539f4f770e2a3fe9e2e615c32a8
    - aaa120a9e051d103c164f605615c32a4
    - bbb340f79047df9bb52fa460615c32a5
    - ccc8700fe6797ed532e311b0615c32a7
    - ddd120a9e051d103c164f605615c32a4
    - eee340f79047df9bb52fa460615c32a5
    - fff8700fe6797ed532e311b0615c32a7
node:
  11dfe4bc7d43c202b28a2a1d8bcaa0f6:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: motr
      version: 2.0.0-5060
      num_services: 1
      services:
      - io
    - name: hare
      version: 2.0.0-5072
    cvg:
    - devices:
        data:
        - /dev/sdc
        - /dev/sdd
        metadata:
        - /dev/sdb
        log:
        - /dev/sdh
        num_log: 1
        num_data: 2
        num_metadata: 1
      name: cvg-01
      type: ios
    hostname: data1-node2
    name: data1-node2
    node_id: bbb340f79047df9bb52fa460615c32a5
    num_components: 3
    num_cvg: 1
    storage_set: storage-set-1
    type: data_node/1
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
  26f752930474c28d64c09b4c198c0fea:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: motr
      version: 2.0.0-5060
      num_services: 1
      services:
      - io
    - name: hare
      version: 2.0.0-5072
    cvg:
    - devices:
        data:
        - /dev/sdf
        - /dev/sdg
        metadata:
        - /dev/sde
        log:
        - /dev/sdi
        num_log: 1
        num_data: 2
        num_metadata: 1
      name: cvg-02
      type: ios
    hostname: data2-node2
    name: data2-node2
    node_id: bba340f79047df9bb52fa460615c32a5
    num_components: 3
    num_cvg: 1
    storage_set: storage-set-1
    type: data_node/2
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
  309ff7f0a7c4dfc22171b75c174bb70c:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: ha
      version: 2.0.0-5070
    hostname: ha-node
    name: ha-node
    node_id: 1115f539f4f770e2a3fe9e2e615c32a8
    num_components: 2
    storage_set: storage-set-1
    type: ha_node
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
  380fb4a039ffc67865bc5b1d51a38811:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: motr
      version: 2.0.0-5060
      num_services: 1
      services:
      - io
    - name: hare
      version: 2.0.0-5072
    cvg:
    - devices:
        data:
        - /dev/sdc
        - /dev/sdd
        metadata:
        - /dev/sdb
        log:
        - /dev/sdh
        num_log: 1
        num_data: 2
        num_metadata: 1
      name: cvg-01
      type: ios
    hostname: data1-node3
    name: data1-node3
    node_id: ccc8700fe6797ed532e311b0615c32a7
    num_components: 3
    num_cvg: 1
    storage_set: storage-set-1
    type: data_node/1
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
  4eb5285f0ec23864604123b79de13e38:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: csm
      num_services: 1
      services:
      - agent
      version: 2.0.0-5072
    hostname: ssc-vm-rhev4-2905.colo.seagate.com
    name: control-node
    node_id: 8efd697708a8f7e428d3fd520c180795
    num_components: 2
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    storage_set: storage-set-1
    type: control_node
  5a230afb7355d23155bcc58328bbe03a:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: hare
      version: 2.0.0-5072
    - name: rgw
      version: 2.0.0-5073
      num_services: 1
      services:
      - rgw_s3
    hostname: server-node3
    name: server-node3
    node_id: fff8700fe6797ed532e311b0615c32a7
    num_components: 3
    storage_set: storage-set-1
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    type: server_node
  8581ff0c598abccbd020e3b72f2cd334:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: hare
    - name: rgw
      version: 2.0.0-5073
      num_services: 1
      services:
      - rgw_s3
    hostname: server-node2
    name: server-node2
    node_id: eee340f79047df9bb52fa460615c32a5
    num_components: 3
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    storage_set: storage-set-1
    type: server_node
  a41d31a905d3892ba80bc0604041d4af:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: motr
      version: 2.0.0-5060
      num_services: 1
      services:
      - io
    - name: hare
      version: 2.0.0-5072
    cvg:
    - devices:
        data:
        - /dev/sdc
        - /dev/sdd
        metadata:
        - /dev/sdb
        log:
        - /dev/sdh
        num_log: 1
        num_data: 2
        num_metadata: 1
      name: cvg-01
      type: ios
    hostname: data1-node1
    name: data1-node1
    node_id: aaa120a9e051d103c164f605615c32a4
    num_components: 3
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    num_cvg: 1
    storage_set: storage-set-1
    type: data_node/1
  aab7732613c4483da0784d14e5ad917d:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: hare
      version: 2.0.0-5072
    - name: rgw
      version: 2.0.0-5073
      num_services: 1
      services:
      - rgw_s3
    hostname: server-node1
    name: server-node1
    node_id: ddd120a9e051d103c164f605615c32a4
    num_components: 3
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    storage_set: storage-set-1
    type: server_node
  b5e8544fd1b3ec3764fff509652a8d07:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: motr
      version: 2.0.0-5060
      num_services: 1
      services:
      - io
    - name: hare
      version: 2.0.0-5072
    cvg:
    - devices:
        data:
        - /dev/sdf
        - /dev/sdg
        metadata:
        - /dev/sde
        log:
        - /dev/sdi
        num_log: 1
        num_data: 2
        num_metadata: 1
      name: cvg-02
      type: ios
    hostname: data2-node3
    name: data2-node3
    node_id: cca8700fe6797ed532e311b0615c32a7
    num_components: 3
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    num_cvg: 1
    storage_set: storage-set-1
    type: data_node/2
  c4a32f1a4a3a43c1c65563511d9536b0:
    cluster_id: 0007ec45379e36d9fa089a3d615c32a3
    components:
    - name: utils
      version: 2.0.0-5058
    - name: motr
      version: 2.0.0-5060
      num_services: 1
      services:
      - io
    - name: hare
      version: 2.0.0-5072
    cvg:
    - devices:
        data:
        - /dev/sdf
        - /dev/sdg
        metadata:
        - /dev/sde
        log:
        - /dev/sdi
        num_log: 1
        num_data: 2
        num_metadata: 1
      name: cvg-02
      type: ios
    hostname: data2-node1
    name: data2-node1
    node_id: eee120a9e051d103c164f605615c32a4
    num_components: 3
    provisioning:
      phase: deployment
      status: success
      version: 2.0.0-846
    num_cvg: 1
    storage_set: storage-set-1
    type: data_node/2
