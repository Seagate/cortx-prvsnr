/**
* Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
*
* This program is free software: you can redistribute it and/or modify
* it under the terms of the GNU Affero General Public License as published
* by the Free Software Foundation, either version 3 of the License, or
* (at your option) any later version.
* This program is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
* GNU Affero General Public License for more details.
* You should have received a copy of the GNU Affero General Public License
* along with this program. If not, see <https://www.gnu.org/licenses/>.
* For any questions about this software or licensing,
* please email opensource@seagate.com or cortx-questions@seagate.com.
*/

node {
    properties([
        buildDiscarder(logRotator(artifactDaysToKeepStr: '', artifactNumToKeepStr: '', daysToKeepStr: '5', numToKeepStr: '5')),
        parameters([
            string(defaultValue: '', description: 'FQDN of VM to destroy on.', name: 'HOST_NAME', trim: true),
            password(description: 'root user password for the target node.', name: 'PASSWORD')
        ])
    ])

    def remote = [:]
    remote.name = "srvnode-1"
    remote.host = HOST_NAME
    remote.user = 'root'
    remote.password = PASSWORD
    remote.allowAnyHosts = true
    withEnv(["SSHPASS=${PASSWORD}"]) {
        ansiColor('xterm') {

            stage("SSH Connectivity Check") {
                if (PASSWORD.isEmpty()) {
                    error "Target VM password cannot be empty."
                }
                sshCommand remote: remote, command: "exit"
                echo "Successfully connected to VM ${HOST_NAME}."
            }

            stage("Teardown: HA") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states ha || true
            """
            echo "Successful teardown: HA"
            }

            stage("Teardown: Control-Path") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states controlpath || true
            """
            echo "Successful teardown: Control-Path"
            }

            stage("Teardown: IO-Path") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states iopath || true
            """
            echo "Successful teardown: IO-Path"
            }

            stage("Teardown: Foundation") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states utils || true
            """
            echo "Successful teardown: Foundation"
            }

            stage("Teardown: 3rd_Party") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states prereq || true
            """
            echo "Successful teardown: 3rd-Party"
            }

            stage("Teardown: Platform") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states system || true
            """
            echo "Successful teardown: Platform"
            }

            stage("Teardown: Provisioner Bootstrap") {
            sshCommand remote: remote, command: """
                command -v provisioner &>/dev/null && provisioner destroy --states bootstrap || true
            """
            echo "Successful teardown: Bootstrap"
            }

            stage("Storage reclaim") {
                sshCommand remote: remote, command: """
                    systemctl status lnet >/dev/null && systemctl stop lnet || true
                    # Wipe the MBR of metadata volume
                    for vggroup in \$(vgdisplay | egrep "vg_srvnode-"|tr -s ' '|cut -d' ' -f 4); do
                        echo "Removing volume group \${vggroup}"
                        vgremove --force \${vggroup}
                    done
                    partprobe

                    device_list=\$(lsblk -nd -o NAME -e 11|grep -v sda|sed 's|sd|/dev/sd|g')
                    for device in \${device_list}
                    do
                        wipefs --all \${device}
                    done
                """
                echo "Successful storage reclaim"
            }

            stage("Stop services") {
                sshCommand remote: remote, command: """
                    systemctl status glustersharedstorage >/dev/null && systemctl stop glustersharedstorage || true
                    systemctl status glusterfsd >/dev/null && systemctl stop glusterfsd || true
                    systemctl status glusterd >/dev/null && systemctl stop glusterd || true
                    systemctl status salt-minion >/dev/null && systemctl stop salt-minion || true
                    systemctl status salt-master >/dev/null && systemctl stop salt-master || true
                """
                echo "Sucessfully stopped salt and gluster services."
            }

            stage("Uninstall rpms") {
                sshCommand remote: remote, command: """
                    yum erase -y cortx-prvsnr cortx-prvsnr-cli      # Cortx Provisioner packages
                    # Remove cortx-py-utils
                    pip3 uninstall -y cortx-py-utils
                    yum erase -y glusterfs-fuse glusterfs-server glusterfs     # Gluster FS packages
                    yum erase -y salt-minion salt-master salt-api   # Salt packages
                    yum erase -y python36-m2crypto                  # Salt dependency
                    yum erase -y python36-cortx-prvsnr              # Cortx Provisioner API packages
                    # Brute clean for any cortx rpm packages
                    yum erase -y *cortx*
                    # Cleanup pip packages
                    pip3 freeze|xargs pip3 uninstall -y
                    # Re-condition yum db
                    yum autoremove -y
                    yum clean all
                """
                echo "Successfully uninstalled rpms."
            }

            stage("Cleanup bricks and other directories") {
                sshCommand remote: remote, command: """
                    test -e /var/cache/yum && rm -rf /var/cache/yum
                    # Cleanup pip config
                    test -e /etc/pip.conf && rm -f /etc/pip.conf
                    test -e ~/.cache/pip && rm -rf ~/.cache/pip
                    # Cortx software dirs
                    test -e /opt/seagate/cortx && rm -rf /opt/seagate/cortx
                    test -e /opt/seagate/cortx_configs && rm -rf /opt/seagate/cortx_configs
                    test -e /opt/seagate && rm -rf /opt/seagate
                    test -e /etc/csm && rm -rf /etc/csm
                    # Bricks cleanup
                    test -e /var/lib/seagate && rm -rf /var/lib/seagate
                    test -e /srv/glusterfs && rm -rf /srv/glusterfs
                    test -e /var/lib/glusterd && rm -rf /var/lib/glusterd || true
                    # Cleanup Salt
                    test -e /var/cache/salt && rm -rf /var/cache/salt
                    test -e /etc/salt && rm -rf /etc/salt
                    # Cleanup Provisioner profile directory
                    test -e /opt/isos && rm -rf /opt/isos || true
                    test -e /root/.provisioner && rm -rf /root/.provisioner || true
                    test -e /etc/yum.repos.d/RELEASE_FACTORY.INFO && rm -f /etc/yum.repos.d/RELEASE_FACTORY.INFO || true
                    #test -e /etc/yum.repos.d && pushd /etc/yum.repos.d;rm -fv !("redhat.repo");popd || true
                    test -e /root/.ssh && rm -rf /root/.ssh || true
                """
                echo "Successfully removed."
            }

            stage("Cleanup logs") {
                sshCommand remote: remote, command: """
                    rm -rf /var/log/seagate
                """
                echo "Successfully removed logs from the node ${remote.host}."
            }
        }

        // sh label: '', script: "mkdir -p archives/provisioner_cluster.json"
        // archiveArtifacts artifacts: '/opt/seagate/cortx_configs/provisioner_cluster.json,/var/log/seagate/provisioner/setup.log', followSymlinks: false
    }
}

