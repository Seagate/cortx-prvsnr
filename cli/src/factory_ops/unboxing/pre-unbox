#!/bin/bash

# This is the pre-unbox script for Seagate Cortx Lyve Drive Rack
# This script is expected to be run after the Cortx hardware is
# set up as per the instruction mentioned in Cortx LDR User Guide.
# The script may prompt user for input, if required.
# Please keep following data handy before running the script:
# 1. Storage Controller IP addresses (2 nos - one for each controller) 
# 2. Hostnames for Sever A and hostnames for Server B, this will be
#    required if DNS/DHCP is not configured in the lab for auto assignment
#    of hostname using mac addresses.

# Following steps are performed as part of the script:
# 1. Run cleanup 
    # 1. Check if subscription manager is enabled
    # 2. Disable and cleanup the subscription 
    # 3. Remove all Seagate internal repos from /etc/yum.repos.d
# 2. Check if the volumes are being listed on both the nodes - lsblk
    # 1. Run 'lsblk -S | grep sas | wc -l' command on both the servers
    # 2. The output should be equal to $nvols (32)
    # 3. If the output is not equal to $nvols (32) then error out with diagnostic steps.
# 3. Get the controller IPs from user as an input
    # 1.  Prompt user to provide IP address for controller A
    # 2.  Prompt user to provide IP address for controller B
    # 3.  Update the IP addresses in storage_enclosure pillar
    # 3.1 If the ips provided by user matches the current values in pillar, do nothing.
# 4. check if the hostname contains 'seagate.com', if yes, take input for new hostname
    # 1 Get the current hostname
    # 2 Check if the hostname has .seagate.com sub-string
    # 3 If yes, get the new hostnames from user, else do nothing.
    # 4 Print the new names and ask for confirmation
    #    4.1 If user don't confirm, then exit and ask user to rerun the command.
    #    4.2 Proceed if user confirms the hostname
    # 5 Set the hostnames on the servers
    #    5.1 restart hostname service
    #    5.2 check the hostname again to ensure it's set correctly
    #    5.3 if hostname is not set to new hostname then try one more time 
    #    5.4 exit after providing the diagnostic steps
# 5. update hostnames in local dns (/etc/hosts)
    # grab the IP address for MGMT interface from 'ip a' output
    # Check if hostnames are getting resolved to mgmt ip addresses
    # Add the new ip & hostnames to /etc/hosts
        # 1. Is mgmt_ip_A already added in /etc/hosts on node A?
        #     1.1 Yes: mgmt_ip_A is added in /etc/hosts
        #            Is the entry against hostname_A? 
        #              Yes: We are good, do nothing. 
        #              NO: Update the hostname against the mgmt_ip_A
        #     1.2 No: add the new entry in /etc/hosts for mgmt_ip_A
        # 2. Repeat for mgmt_ip_B on node A.
        # 3. Repeat 1 and 2 for node B
# 6. Update hostnames in ssh config files
    # 1. Update new hostnames in /root/.ssh/config file on Server A
    # 2. scp /root/.ssh/config file from Server A to Server B
    # 3. Check ssh connectivity from server A to server B using alias/minion id (srvnode-2)
    # 4. Check ssh connectivity from server B to server A using alias/minion id (srvnode-1)
    # 5. Check if the ssh works without password from Server A to Server B using new hostname
    # 6. Check if the ssh works without password from Server B to Server A using new hostname
# 7. Update minion files on both the nodes to point master to server A hostname
    # 1. Update master host in /etc/salt/minion file with hostname of Server A 
    # 2. Update master host in /etc/salt/minion file with hostname of Server B 
    # 3. Restart salt-minion service on Server A
    # 4. Restart salt-minion service on Server B
    # 5. Restart salt-master service on Server A
    # 6. Check if 'salt '*' test.ping' works
# 8. Run the unboxing script

set -euE
export LOG_FILE="${LOG_FILE:-/var/log/seagate/provisioner/pre-unboxing-hf.log}"
mkdir -p $(dirname "${LOG_FILE}")

PRVSNR_ROOT="/opt/seagate/eos-prvsnr"

# Global variables which can be set externally using environment variables
default_hostname_nodeA="${default_hostname_nodeA:-"cortx-node-a"}"
default_hostname_nodeB="${cortx-node-b:-"cortx-node-b"}"
pvt_ip_a="${pvt_ip_a:-192.168.0.1}"
pvt_ip_b="${pvt_ip_b:-192.168.0.2}"
#TODO: The no of volumes for Beta are 32.
nvols="${nvols:-32}" # The default value needs to be changed to 64 for cross connect
pillar_root="${pillar_root:-/opt/seagate/eos-prvsnr/pillar/user/groups/all}"

# Global variables, don't change these values.
ssh_cmd="ssh -i /root/.ssh/id_rsa_prvsnr -o \"StrictHostKeyChecking no\""
scp_cmd="scp -i /root/.ssh/id_rsa_prvsnr -o \"StrictHostKeyChecking no\""
pvt_ips=("${pvt_ip_a}" "${pvt_ip_b}")
minion_ids=("srvnode-1" "srvnode-2")
server_names=("Server A" "Server B") #Required for logging messages

# Variables for stpring inputs from user
ctrl_ip_a=
ctrl_ip_b=
new_hostname_nodeA=
new_hostname_nodeB=

# These variables will be updated in /etc/hosts, ssh config and salt config
hostname_A=
hostname_B=

function err_handler {
    echo "\
**************************FAILED!!********************************
For detailed error logs, please see: $LOG_FILE
******************************************************************" | tee -a ${LOG_FILE}
}

function intrpt_handler {
    echo "\
Received Ctrl-c signal, exiting Gracefully.
For detailed logs, please see: $LOG_FILE" | tee -a ${LOG_FILE}
}

trap err_handler ERR
trap intrpt_handler SIGTERM SIGINT

sub_manager_check()
{
    _pvt_ip="${1:-$pvt_ip_a}"
    _server=${2:-"Server A"}

    $ssh_cmd $_pvt_ip 'grep -q "Red Hat" /etc/*-release' || {
        echo "$_server is not a RedHat system" | tee -a ${LOG_FILE}
        subscription_enabled=false
        return
    }

    echo "Checking if RHEL subscription manager is enabled on $_server" | tee -a ${LOG_FILE}
    subc_list=`$ssh_cmd $_pvt_ip 'subscription-manager list' | grep Status: | awk '{ print $2 }'`
    subc_status=`$ssh_cmd $_pvt_ip 'subscription-manager status' | grep "Overall Status:" | awk '{ print $3 }'`
    if echo "$subc_list" | grep -q "Subscribed"; then
        if [[  "$subc_status" == "Current" ]]; then
            echo "RedHat subscription manager is enabled on $_server." | tee -a ${LOG_FILE}
            subscription_enabled=true
        else
            echo "RedHat subscription manager is disabled on $_server." 2>&1 | tee -a ${LOG_FILE}
            subscription_enabled=false
        fi
    fi
}

sub_manager_cleanup()
{
    _pvt_ip="${1:-$pvt_ip_a}"
    _server=${2:-"Server A"}

    $ssh_cmd $_pvt_ip 'grep -q "Red Hat" /etc/*-release' || {
        echo "$_server is not a RedHat system" | tee -a ${LOG_FILE}
        return
    }
    echo "Cleaning up the subscription manager on $_server" | tee -a ${LOG_FILE}

    echo "Running the subscription-manager auto-attach on $_server" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip 'subscription-manager auto-attach --disable || true'  | tee -a ${LOG_FILE}

    echo "Running subscription-manager remove --all on $_server" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip 'subscription-manager remove --all || true' | tee -a ${LOG_FILE}

    echo "Running subscription-manager unregister on $_server" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip 'subscription-manager unregister || true' | tee -a ${LOG_FILE}

    echo "Running subscription-manager clean on $_server" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip 'subscription-manager clean || true' | tee -a ${LOG_FILE}

    echo "Running subscription-manager config --rhsm.manage_repos=0 on $_server" | tee -a ${LOG_FILE}
    $ssh_cmd $_pvt_ip 'subscription-manager config --rhsm.manage_repos=0' | tee -a ${LOG_FILE}
}

seagate_refs_cleanup()
{
    # 1. Check if subscription manager is enabled
    # 2. Disable and cleanup the subscription 
    # 3. Remove all Seagate internal repos from /etc/yum.repos.d

    for i in "${!pvt_ips[*]}"; do
        subscription_enabled=false
        sub_manager_check "${pvt_ips[$i]}" "${server_names[$i]}"
        if [[ "$subscription_enabled" == true ]]; then
            sub_manager_cleanup "${pvt_ips[$i]}" "${server_names[$i]}"
        fi

        echo "Removing following repos from ${server_names[$i]}" | tee -a ${LOG_FILE}
        $ssh_cmd ${pvt_ips[$i]} 'grep -lE "seagate.com" /etc/yum.repos.d/*.repo' | tee -a ${LOG_FILE}
        $ssh_cmd ${pvt_ips[$i]} 'for file in `grep -lE "seagate.com" /etc/yum.repos.d/*.repo`; do rm -f "$file"; done'
        echo "Cleaning yum cache on ${server_names[$i]}" | tee -a ${LOG_FILE}
        $ssh_cmd ${pvt_ips[$i]} 'yum clean all'
        echo "Done" | tee -a ${LOG_FILE}
    done
}

luns_availability_check()
{
    # 1. Run 'lsblk -S | grep sas | wc -l' command on both the servers
    # 2. The output should be equal to $nvols
    # 3. If the output is not equal to $nvols then error out with diagnostic steps.

    echo "Checking if the luns from storage enclosure are available" | tee -a ${LOG_FILE}
    for i in "${!pvt_ips[*]}"; do
        $ssh_cmd ${pvt_ips[$i]} 'lsblk -S| grep sas' | tee -a ${LOG_FILE}
        _nluns=$($ssh_cmd ${pvt_ips[$i]} 'lsblk -S | grep sas | wc -l')
        echo "No of luns on server ${server_names[$i]} are: $_nluns" | tee -a ${LOG_FILE}
        if [[ $_nluns -eq 0 || $_nluns -ne $nvols ]]; then
            echo "Error: The no of luns from storage enclosure ($_nluns) are not correct on ${server_names[$i]}." | tee -a ${LOG_FILE}
            echo "Try following steps before running the command again:\
1. Please make sure the storage enclosre is connected to servers as shown in the user guide.
2. If the connections to the storage enclosure are correct then try rebooting the ${server_names[$i]} once
3. After the reboot of server(s) run 'lsblk -S | grep sas | wc -l' to check the no of volumes available
4. Once the output of 'lsblk -S | grep sas | wc -l' is $nvols on both the servers run this command '$0' again" | tee -a ${LOG_FILE}
            exit 1
        fi
        echo "The number of luns ($_nluns) are correct on ${server_names[$i]}"  | tee -a ${LOG_FILE}
    done
}

ctrl_ip_addr_get_set()
{
    # 1.  Prompt user to provide IP address for controller A
    # 2.  Prompt user to provide IP address for controller B
    # 3.  Update the IP addresses in storage_enclosure pillar
    # 3.1 If the ips provided by user matches the current values in pillar, do nothing.

    echo "Please provide the IP addresses for storage enclosure (should be already configured)" | tee -a ${LOG_FILE}
    _input_recd=0
    while $_input_received -eq 0; do
        read -p "Enter IP address configured for Controller A: " ctrl_ip_a
        if [[ -z $ctrl_ip_a ]]; then
            echo "Pleaes provide valid input"; | tee -a ${LOG_FILE}
        else
            echo "IP address received for Controller A: $ctrl_ip_a" >> ${LOG_FILE}
            _input_recd=1
        fi
    done

    #TODO: remove duplicate code for controller B, add it in for loop.
    _input_recd=0
    while $_input_received -eq 0; do
        read -p "Enter IP address configured for Controller B: " ctrl_ip_b
        if [[ -z $ctrl_ip_b ]]; then
            echo "Pleaes provide valid input"; | tee -a ${LOG_FILE}
        else
            echo "IP address received for Controller B: $ctrl_ip_b" >> ${LOG_FILE}
            _input_recd=1
        fi
    done

    #Update the storage_enclosure pillar with new controller IP.
    _storage_encl_sls_path=${pillar_root}/storage_enclosure.sls
    echo "Updating IP addresses of storage enclosure in Salt pillar" |tee -a ${LOG_FILE}

    echo "Updating storage enclosure pillar with new controller A ip address" | tee -a ${LOG_FILE}
    _current_ip=$(grep -A1 primary_mc ${_storage_encl_sls_path} | tail -1 | awk '{ print $2 }')
    cat ${_storage_encl_sls_path} | sed -e "s/$_current_ip/$ctrl_ip_a/" > /tmp/ctrla-sed.tmp
    diff_lines=$(diff ${_storage_encl_sls_path} /tmp/ctrla-sed.tmp | grep "<" | wc -l)
    #Check and ensure there is only one line change in the diff
    if [[ $diff_lines -eq 1 ]]; then
        # Only one line is changed, as expected.
        # Take backup of the original file &
        # Update the file generated by sed with new ip
        yes | cp -f ${_storage_encl_sls_path} ${_storage_encl_sls_path}.bkp
        yes | cp -f /tmp/ctrla-sed.tmp ${_storage_encl_sls_path}
    elif [[ $diff_lines -eq 0 ]]; then
        # old ip and new ip match, prbably due to rerun of 
        # the command or updating the pillar manually
        echo "The ip address provided for controller A is already present in storage enclosure pillar, ignoring" | tee -a ${LOG_FILE}
    else
        # sed didn't work as expected (was supposed to change only one line)
        echo "ERROR: Could not update the storage enclosure pillar with new controller A ip address" | tee -a ${LOG_FILE}
        echo "ERROR: Please update it manually in ${_storage_encl_sls_path} and run the command again" | tee -a ${LOG_FILE}
        exit 1
    fi
    echo "Done" | tee -a ${LOG_FILE}

    echo "Updating storage enclosure pillar with new controller B ip address" | tee -a ${LOG_FILE}
    _current_ip=$(grep -A1 secondary_mc ${_storage_encl_sls_path} | tail -1 | awk '{ print $2 }')
    cat ${_storage_encl_sls_path} | sed -e "s/$_current_ip/$ctrl_ip_b/" > /tmp/ctrlb-sed.tmp
    diff_lines=$(diff ${_storage_encl_sls_path} /tmp/ctrlb-sed.tmp | grep "<" | wc -l)
    #Check and ensure there is only one line change in the diff
    if [[ $diff_lines -eq 1 ]]; then
        # Only one line is changed, as expected.
        # Take backup of the original file &
        # Update the file generated by sed with new ip
        yes | cp -f ${_storage_encl_sls_path} ${_storage_encl_sls_path}.bkp
        yes | cp -f /tmp/ctrlb-sed.tmp ${_storage_encl_sls_path}
    elif [[ $diff_lines -eq 0 ]]; then
        # old ip and new ip match, prbably due to rerun of 
        # the command or updating the pillar manually
        echo "The ip address provided for controller B is already present in storage enclosure pillar, ignoring" | tee -a ${LOG_FILE}
    else
        # sed didn't work as expected (was supposed to change only one line)
        echo "ERROR: Could not update the storage enclosure pillar with new controller B ip address" | tee -a ${LOG_FILE}
        echo "ERROR: Please update it manually in ${_storage_encl_sls_path} and run the command again" | tee -a ${LOG_FILE}
        exit 1
    fi
    echo "Done" | tee -a ${LOG_FILE}
}

new_hostname_set()
{
    # Set the hostname on both the servers
    # Set the hostname only if user has provided the new hostname
    # After hostname is set, sleep for 30s
    # Check the hostname again, and ensure it is the new hostname 

    for i in "${!pvt_ips[*]}"; do
        echo "Setting the new hostname provided by user" | tee -a ${LOG_FILE}
        if [[ $i -eq 0 && ! -z $new_hostname_nodeA ]]; then
            echo "Setting the hostname($new_hostname_nodeA) for server A" | tee -a ${LOG_FILE}
            _new_hostname=$new_hostname_nodeA
        elif [[ $i -eq 1 && ! -z $new_hostname_nodeB ]]; then
            echo "Setting the hostname($new_hostname_nodeB) for server B" | tee -a ${LOG_FILE}
            _new_hostname=$new_hostname_nodeB
        else
            echo "DEBUG: no hostname provided by user for ${server_names[$i]}, skipping" >> ${LOG_FILE}
            continue
        fi
        $ssh_cmd ${pvt_ips[$i]} 'hostnamectl --set-hostnmae --static --transient --pretty $_new_hostname' 2>&1 | tee -a ${LOG_FILE}
        echo "Restarting the hostnamed service" | tee -a ${LOG_FILE}
        systemctl restart systemd-hostnamed 2>&1 | tee -a ${LOG_FILE}
        echo "Waiting for the new hostname to get updated in system for ${server_names[$i]}" | tee -a ${LOG_FILE}
        sleep 30

        #TODO: Which service to restart to get the hostname updated in customer's DNS server?

        # Get the current hostname again from the system
        _hostname=$($ssh_cmd ${pvt_ips[$i]} 'hostnamectl status' | grep "Static hostname" | awk '{ print $3 }')
        # Check if the current hostname is same as new hostname provided by user
        if [[ "$_hostname" == "$_new_hostname" ]]; then
            echo "New hostname($_new_hostname) is set successfully for ${server_names[$i]}" | tee -a ${LOG_FILE}
        else
            # DNS is not allowing to change the hostname
            # Change the attribute of hostname file
            echo "Warning: The hostname provided ($_new_hostname) could not be set on ${server_names[$i]}" | tee -a ${LOG_FILE}
            echo "Retrying.." | tee -a ${LOG_FILE}
            $ssh_cmd ${pvt_ips[$i]} 'chattr -i /etc/hostname' 2>&1 | tee -a ${LOG_FILE}
            $ssh_cmd ${pvt_ips[$i]} 'hostnamectl --set-hostnmae --static --transient --pretty ${_new_hostname}' 2>&1 | tee -a ${LOG_FILE}
            echo "Restarting the hostnamed service" | tee -a ${LOG_FILE}
            systemctl restart systemd-hostnamed 2>&1 | tee -a ${LOG_FILE}
            echo "Waiting for the new hostname to get updated in system for ${server_names[$i]}" | tee -a ${LOG_FILE}
            sleep 30
            # Check again if the hostname provided by user got updated in the system
            _hostname=$($ssh_cmd ${pvt_ips[$i]} 'hostnamectl status' | grep "Static hostname" | awk '{ print $3 }')
            if [[ "$_hostname" == "$_new_hostname" ]]; then
                echo "New hostname($_new_hostname) is set successfully for ${server_names[$i]}" | tee -a ${LOG_FILE}
            else
                #TODO: Give one more try with 
                # 1. nmcli general hostname $_new_hostname
                # 2. sysctl kernel.hostname=$_new_hostname
                echo "Error: The hostname provided ($_new_hostname) could not be set on ${server_names[$i]}
Please do the following steps:
1. Check whether the dns/dhclient settings in the lab are causing problems to update the hostname.
2. If yes, please set the hostnames through DNS using mac addresses mentioned in Quick install information sheet provided by Seagate.
3. If changing DNS is not possible or it's not working try to change the hostname manually using following commands:
   1. hostnamectl --set-hostnmae --static --transient --pretty ${_new_hostname}
   2. nmcli general hostname $_new_hostname
   3. sysctl kernel.hostname=$_new_hostname
   NOTE: Run these commands one at a time, if command no. 1 don't work then only try command no. 2 and so on.
         Also wait for couple of minutes after the command is run and check 'hostnamectl status' again to ensure the hostname indeed has changed.
4. If the above commands also don't work it's mostly the network settings in the lab that is causing the problem.
   To bypass the network settings:
   1. Remove the network cables from the Servers to switch
   2. Set the the hostnames again using commands mentioned in step 3 above
5. If the hostnames are updated successfully rerun the pre-unbox command again to proceed

The pre-unbox command did not complete, exiting." | tee -a ${$LOG_FILE}
                exit 1
            fi
        fi
        if [[ $i -eq 0 ]]; then hostname_A=${_hostname}; else hostname_B=${_hostname}; fi 
    done
}

new_hostname_get()
{
    _tnode="${1:-"Server A"}"
    # Prompt user for the new hostname (suggest sample hostname as an example)

    if [[ $_tnode == "Server A" ]]; then
        read -p "Enter new hostname for server A(press enter to keep default [$default_hostname_nodeA]): " new_hostname_nodeA
        new_hostname_nodeA=${new_hostname_nodeA:-$default_hostname_nodeA}
    else
        read -p "Enter new hostname for server B(press enter to keep default [$default_hostname_nodeB]): " new_hostname_nodeA
        new_hostname_nodeB=${new_hostname_nodeB:-$default_hostname_nodeB}
    fi
}

hostname_validate_get_set()
{
    # 1 Get the current hostname
    # 2 Check if the hostname has .seagate.com
    # 3 If yes, get the new hostnames from user, else do nothing.
    # 4 Print the new names and ask for confirmation
    #    4.1 If user don't confirm, then exit and ask user to rerun the command.
    #    4.2 Proceed if user confirms the hostname
    # 5 Set the hostnames on the servers
    #    5.1 restart hostname service
    #    5.2 check the hostname again to ensure it's set correctly
    #    5.3 if hostname is not set to new hostname then try one more time 
    #    5.4 exit after providing the diagnostic steps

    _seagate_hname=false

    echo "Validating the current hostnames on both servers" | tee -a ${LOG_FILE}
    for i in "${!pvt_ips[*]}"; do
        echo "Running hostname check on ${server_names[$i]}" >> ${LOG_FILE}
        _hname=$($ssh_cmd ${pvt_ips[$i]} 'hostnamectl status' | grep "Static hostname" | awk '{ print $3 }')
        echo $_hname | grep -q "seagate.com" && _seagate_hname=true || _seagate_hname=false
        if [[ $_seagate_hname == "true" ]]; then
            echo "The hostname of ${server_names[$i]} contains the seagate domain name" >> ${LOG_FILE}
            new_hostname_get "${server_names[$i]}"
        else
            echo "The current hostname for ${server_names[$i]} looks good" | tee -a ${LOG_FILE}
            if [[ "${server_names[$i]}" == "Server A" ]]; then
                hostname_A=${_hname}
                echo "hostname of server A: ${hostname_A}" | tee -a ${LOG_FILE}
            else
                hostname_B=${_hname}
                echo "hostname of server B: ${hostname_B}" | tee -a ${LOG_FILE}
            fi
        fi
    done

    # Log the new hostname provided by user
    if [[ ! -z "$new_hostname_nodeA" || ! -z "$new_hostname_nodeA" ]]; then
        echo "New hostnames provided by user:" | tee -a ${LOG_FILE}
        if [[ ! -z "$new_hostname_nodeA" ]]; then
            echo "server A: "$new_hostname_nodeA"" | tee -a ${LOG_FILE}
        fi
        if [[ ! -z "$new_hostname_nodeB" ]]; then
            echo "server B: "$new_hostname_nodeB"" | tee -a ${LOG_FILE}
        fi
    fi

    new_hostname_set

}

dns_update_hostnames()
{
    # grab the IP address for MGMT interface from 'ip a' output
    # Check if hostnames are getting resolved to mgmt ip addresses
    # Add the new ip & hostnames to /etc/hosts

    # Get management interface name from pillar
    local mgmt_if=$(grep -m1 -A3 -P "mgmt_nw:" ${pillar_root}/cluster.sls|tail -1|cut -d'-' -f2|tr -d "[:space:]")

    # Get management interface ip address for node1
    local mgmt_ip_A=$((ip addr show dev ${mgmt_if}|grep inet|grep -v inet6|grep -Po "\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}" || echo "ERROR: IP address missing for ${mgmt_if} on Server A" || (tee -a ${LOG_FILE}; exit 1))|head -1)
    echo "Management IP (Server A) : ${mgmt_ip_A}" | tee -a ${LOG_FILE}

    # Get management ip address for node2
    local mgmt_ip_B=$(($ssh_cmd ${pvt_ip_b} "ip addr show dev ${mgmt_if}|grep inet|grep -v inet6|grep -Po \"\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}\"" || (echo "ERROR: IP address missing for ${mgmt_if} on srvnode-2" | tee -a ${LOG_FILE}; echo 1))|head -1)
    echo "Management IP (Server B) : ${mgmt_ip_B}" | tee -a ${LOG_FILE}

    # Check if entries in /etc/hosts needs to be added.
    # The entry needs to be added only if the hostnames are NOT getting resolved from each other
    echo "Checking if server B ($hostname_B) is reachable from server A ($hostname_A) over DNS" | tee -a ${LOG_FILE}
    if ping -c1 -W2  $hostname_B > /dev/null; then
        # Ping is successfull, no need to add entry in /etc/hosts
        echo "Server B ($hostname_B) is reachable from ServerA ($hostname_A) over DNS" | tee -a ${LOG_FILE}
        _ping_status_A_to_B=true
    else
        # Ping failed, need to add entry in /etc/hosts
        echo "ERROR: Server B ($hostname_B) is not reachable from server A ($hostname_A) over DNS" |tee -a ${LOG_FILE}
        _ping_status_A_to_B=false
    fi

    echo "Checking if server A ($hostname_A) is reachable from Server B ($hostname_B) over DNS" | tee -a ${LOG_FILE}
    if $ssh_cmd ${pvt_ip_b} "ping -c1 -W2  $hostname_A" > /dev/null; then
        # Ping is successfull, no need to add entry in /etc/hosts
        echo "Server A ($hostname_A) is reachable from ServerB ($hostname_B) over DNS" | tee -a ${LOG_FILE}
        _ping_status_B_to_A=true
    else
        # Ping failed, need to add entry in /etc/hosts
        echo "ERROR: Server A ($hostname_A) is not reachable from server B ($hostname_B) over DNS" |tee -a ${LOG_FILE}
        _ping_status_B_to_A=false
    fi

    # Steps to add entries in /etc/hosts
    # 1. Is mgmt_ip_A already added in /etc/hosts on node A?
    #     1.1 Yes: mgmt_ip_A is added in /etc/hosts
    #            Is the entry against hostname_A? 
    #              Yes: We are good, do nothing. 
    #              NO: Update the hostname against the mgmt_ip_A
    #     1.2 No: add the new entry in /etc/hosts for mgmt_ip_A
    # 2. Repeat for mgmt_ip_B on node A.
    # 3. Repeat 1 and 2 for node B
    # TODO: Remove duplicate code.

    # node A
    echo "Checking DNS entries in /etc/hosts on server A for management IP of server A (${mgmt_ip_A})" | tee -a ${LOG_FILE}
    # 1. Is mgmt_ip_A already added in /etc/hosts on node A?
    if grep -q "${mgmt_ip_A}" /etc/hosts ; then
        # 1.1 mgmt_ip_A is added in /etc/hosts
        if grep "${mgmt_ip_A}" /etc/hosts | grep "${hostname_A}"; then
            # mgmt_ip_A is added in /etc/hosts and is against hostname_A
            echo "Server A: /etc/hosts already has dns entry for management IP of server A (${mgmt_ip_A}) against hostname of Server A (${hostname_A})" | tee -a ${LOG_FILE}
        else
            # Entry for ${mgmt_ip_A} is there in /etc/hosts but it's not against hostname_A
            # Update the hostname against the existing entry.
            echo "Server A: Warning: /etc/hosts already has entry for management IP (${mgmt_ip_A}) of server A (${hostname_A}) but against wrong/old hostname" | tee -a  ${LOG_FILE}
            #TODO: Update the entry in /etc/hosts. Remove and add it?
        fi
    else
        # 1.2 /etc/hosts does not has the entry, add it
        echo "Server A: Adding entry in /etc/hosts for management IP (${mgmt_ip_A}) of server A ($hostname_A)" | tee -a  ${LOG_FILE}
        echo "$mgmt_ip_A $hostname_A" >> /etc/hosts
        echo "Done" | tee -a ${LOG_FILE}
    fi

    # 2. Is mgmt_ip_B already added in /etc/hosts on node A?
    echo "Checking DNS entries in /etc/hosts on server A for management IP of Server B (${mgmt_ip_B})" | tee -a ${LOG_FILE}
    if grep -q "${mgmt_ip_B}" /etc/hosts ; then
        # 2.1 mgmt_ip_B is added in /etc/hosts
        if grep "${mgmt_ip_B}" /etc/hosts | grep "${hostname_B}"; then
            # mgmt_ip_B is added in /etc/hosts and is against hostname_B
            echo "Server A: /etc/hosts already has dns entry for management IP of server B (${mgmt_ip_B}) against hostname of Server B (${hostname_B})" | tee -a ${LOG_FILE}
        else
            # Entry for ${mgmt_ip_B} is there in /etc/hosts but it's not against hostname_B
            # Update the hostname against the existing entry.
            echo "Server A: Warning: /etc/hosts already has entry for management IP (${mgmt_ip_B}) of server B (${hostname_B}) but against wrong/old hostname" | tee -a  ${LOG_FILE}
            #TODO: Update the entry in /etc/hosts. Remove and add it?
        fi
    else
        # 2.2 /etc/hosts does not has the entry, add it
        echo "Server A: Adding entry in /etc/hosts for management IP (${mgmt_ip_B}) of server B ($hostname_B)" | tee -a  ${LOG_FILE}
        echo "$mgmt_ip_B $hostname_B" >> /etc/hosts
        echo "Done" | tee -a ${LOG_FILE}
    fi

    # Repeat for node B
    echo "Checking DNS entries in /etc/hosts on server B for management IP of node A (${mgmt_ip_A})" | tee -a ${LOG_FILE}
    # 1. Is mgmt_ip_A already added in /etc/hosts on node B?
    if $ssh_cmd ${pvt_ip_b} 'grep -q \"${mgmt_ip_A}\" /etc/hosts' ; then
        # 1.1 mgmt_ip_A is added in /etc/hosts
        if $ssh_cmd ${pvt_ip_b} 'grep \"${mgmt_ip_A}\" /etc/hosts | grep \"${hostname_A}\"'; then
            # mgmt_ip_A is added in /etc/hosts and is against hostname_A
            echo "Server B: /etc/hosts already has dns entry for management IP  of server A (${mgmt_ip_A}) against hostname of Server A (${hostname_A})" | tee -a ${LOG_FILE}
        else
            # Entry for ${mgmt_ip_A} is there in /etc/hosts but it's not against hostname_A
            # Update the hostname against the existing entry.
            echo "Server B: Warning: /etc/hosts already has entry for management IP (${mgmt_ip_A}) of server A (${hostname_A}) but against wrong/old hostname" | tee -a  ${LOG_FILE}
            #TODO: Update the entry in /etc/hosts. Remove and add it?
        fi
    else
        # 1.2 /etc/hosts does not has the entry, add it
        echo "Server B: Adding entry in /etc/hosts for management IP (${mgmt_ip_A}) of server A ($hostname_A)" | tee -a  ${LOG_FILE}
        echo "$mgmt_ip_A $hostname_A" > /tmp/dns_a.tmp
        $scp_cmd /tmp/dns_a.tmp srvnode2:/tmp/dns_a.tmp
        $ssh_cmd ${pvt_ip_b} 'cat /tmp/dns_a.tmp >> /etc/hosts' >> /etc/hosts
        echo "Done" | tee -a ${LOG_FILE}
    fi

    # Repeat for mgmtIP_B on node B
    echo "Checking DNS entries in /etc/hosts on server B for management IP of node B (${mgmt_ip_B})" | tee -a ${LOG_FILE}
    # 2. Is mgmt_ip_B already added in /etc/hosts on node B?
    if $ssh_cmd ${pvt_ip_b} 'grep -q \"${mgmt_ip_B}\" /etc/hosts' ; then
        # 2.1 mgmt_ip_B is added in /etc/hosts
        if $ssh_cmd ${pvt_ip_b} 'grep \"${mgmt_ip_B}\" /etc/hosts | grep \"${hostname_B}\"'; then
            # mgmt_ip_B is added in /etc/hosts and is against hostname_B
            echo "Server B: /etc/hosts already has dns entry for management IP  of server B (${mgmt_ip_B}) against hostname of Server B (${hostname_B})" | tee -a ${LOG_FILE}
        else
            # Entry for ${mgmt_ip_B} is there in /etc/hosts but it's not against hostname_B
            # Update the hostname against the existing entry.
            echo "Server B: Warning: /etc/hosts already has entry for management IP (${mgmt_ip_B}) of server B (${hostname_B}) but against wrong/old hostname" | tee -a  ${LOG_FILE}
            #TODO: Update the entry in /etc/hosts. Remove and add it?
        fi
    else
        # 2.2 /etc/hosts does not has the entry, add it
        echo "Server B: Adding entry in /etc/hosts for management IP (${mgmt_ip_B}) of server B ($hostname_B)" | tee -a  ${LOG_FILE}
        echo "$mgmt_ip_B $hostname_B" > /tmp/dns_b.tmp
        $scp_cmd /tmp/dns_b.tmp srvnode2:/tmp/dns_b.tmp
        $ssh_cmd ${pvt_ip_b} 'cat /tmp/dns_b.tmp >> /etc/hosts' >> /etc/hosts
        echo "Done" | tee -a ${LOG_FILE}
    fi
}

ssh_config_update()
{
    # 1. Update new hostnames in /root/.ssh/config file on Server A
    # 2. scp /root/.ssh/config file from Server A to Server B
    # 3. Check ssh connectivity from server A to server B using alias/minion id (srvnode-2)
    # 4. Check ssh connectivity from server B to server A using alias/minion id (srvnode-1)
    # 5. Check if the ssh works without password from Server A to Server B using new hostname
    # 6. Check if the ssh works without password from Server B to Server A using new hostname

    echo "Updating server A hostname (${hostname_A}) in ssh config file of server A" | tee -a ${LOG_FILE}
    local line_to_replace=$(grep -m1 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
    sed -i "s|Host srvnode-1.*|Host srvnode-1 ${hostname_A}|" /root/.ssh/config
    sed -i "${line_to_replace}s|HostName.*|HostName ${hostname_A}|" /root/.ssh/config
    echo "Done" | tee -a ${LOG_FILE}

    # Replace Server B entry
    echo "Updating server B hostname (${hostname_B}) in ssh config file of server A" | tee -a ${LOG_FILE}
    local line_to_replace=$(grep -m2 -noP "HostName" /root/.ssh/config|tail -1|cut -d: -f1)
    sed -i "s|Host srvnode-2.*|Host srvnode-2 ${hostname_B}|" /root/.ssh/config
    sed -i "${line_to_replace}s|HostName.*|HostName ${hostname_B}|" /root/.ssh/config
    echo "Done" | tee -a ${LOG_FILE}

    echo "Copying the ssh config file from Server A to Server B" | tee -a ${LOG_FILE}
    $scp_cmd /root/.ssh/config srvnode-2:/root/.ssh/config
    echo "Done"

    # Check if the ssh works without password from Server A to Server B
    echo "Checking if the ssh works without password from Server A to Server B" | tee -a ${LOG_FILE}
    ssh -q -o "ConnectTimeout=5" srvnode-2 exit || {
        echo "ERROR: Server B is not reachable using alias (srvnode-2) over ssh" | tee -a ${LOG_FILE}
        exit 1
    }
    echo "Done." | tee -a ${LOG_FILE}
    # Check if the ssh works without password from Server B to Server A
    echo "Checking if the ssh works without password from Server B to Server A" | tee -a ${LOG_FILE}
    ssh -q -o "ConnectTimeout=5" srvnode-2 \
        "ssh -q -o ConnectTimeout=5 srvnode-1 exit; exit" || {
        echo "Server A is not reachable from Server B over ssh" | tee -a ${LOG_FILE}
        exit 1
    }
    echo "Done." | tee -a ${LOG_FILE}

    # Check if the ssh works without password from Server A to Server B using DNS hostname
    echo "Checking if the ssh works without password from Server A to Server B using new hostname" | tee -a ${LOG_FILE}
    ssh -q -o "ConnectTimeout=5" $hostname_B exit || {
        echo "ERROR: Server B($hostname_B) is not reachable from Server A($hostname_A) using hostname($hostname_B) over ssh" | tee -a ${LOG_FILE}
        exit 1
    }
    echo "Done." | tee -a ${LOG_FILE}
    # Check if the ssh works without password from Server B to Server A using DNS hostname
    echo "Checking if the ssh works without password from Server B to Server A using new hostname" | tee -a ${LOG_FILE}
    ssh -q -o "ConnectTimeout=5" $hostname_B \
        "ssh -q -o ConnectTimeout=5 $hostname_A exit; exit" || {
        echo "Server A($hostname_A) is not reachable from Server B($hostname_B)  using hostname over ssh" | tee -a ${LOG_FILE}
        exit 1
    }
    echo "Done." | tee -a ${LOG_FILE}
}

minion_config_update()
{
    # 1. Update master host in /etc/salt/minion file with hostname of Server A 
    # 2. Update master host in /etc/salt/minion file with hostname of Server B 
    # 3. Restart salt-minion service on Server A
    # 4. Restart salt-minion service on Server B
    # 5. Restart salt-master service on Server A
    # 6. Check if 'salt '*' test.ping' works

    local line_to_replace=$(grep -m1 -noP "master: " /etc/salt/minion|tail -1|cut -d: -f1)
    echo "Setting Salt master (${hostname_A}) on server A (${hostname_A})" | tee -a ${LOG_FILE}
    yes | cp -f /etc/salt/minion /etc/salt/minion.bkp
    sed -i "${line_to_replace}s|^master:.*|master: ${hostname_A}|" /etc/salt/minion
    echo "Done" | tee -a ${LOG_FILE}

    echo "Setting Salt master (${hostname_A}) on server B (${hostname_B})" | tee -a ${LOG_FILE}
    #TODO: Should we just scp minion file from Server A to server B?
    yes | cp -f /etc/salt/minion /etc/salt/minion.bkp
    $ssh_cmd ${pvt_ip_b} "sed -i \"${line_to_replace}s|^master:.*|master: ${hostname_A}|\" /etc/salt/minion"
    echo "Done" | tee -a ${LOG_FILE}

    echo "Restarting salt-minion on Server A" | tee -a ${LOG_FILE}
    systemctl restart salt-minion
    echo "Done" | tee -a ${LOG_FILE}

    echo "Restarting salt-minion on Server B" | tee -a ${LOG_FILE}
    $ssh_cmd ${pvt_ip_b} "systemctl restart salt-minion"
    echo "Done" | tee -a ${LOG_FILE}
    sleep 5

    # Check if salt '*' test.ping works
    echo "Waiting for minion on Server A to become ready" | tee -a ${LOG_FILE}
    try=1; max_tries=10
    until salt -t 1 srvnode-1 test.ping >/dev/null 2>&1
    do
        if [[ "$try" -gt "$max_tries" ]]; then
            echo "
ERROR: minion srvnode-1 seems still not ready after $max_tries attempts.\
Check following things:
1. restart salt-master service on Server A [systemctl restart salt-master]
2. restart salt-minion service on Server A [systemctl restart salt-minion]
3. restart salt-minion service on Server B [ssh srvnode-2 'systemctl restart salt-minion']
4. check the status again [salt '*' test.ping]
5. if step 4 works, rerun the pre-unbox command
pre-unbox script did not run successfully, exiting." | tee -a ${LOG_FILE}
            #TODO: Retry after restarting services?
            exit 1
        fi
        echo -n "." | tee -a ${LOG_FILE}
        try=$(( $try + 1 ))
    done
    echo "Minion on Server A started successfully" | tee -a ${LOG_FILE}

    echo "Waiting for minion on Server B to become ready" | tee -a ${LOG_FILE}
    try=1; max_tries=10
    until salt -t 1 srvnode-2 test.ping >/dev/null 2>&1
    do
        if [[ "$try" -gt "$max_tries" ]]; then
            echo "
ERROR: minion srvnode-2 seems still not ready after $max_tries attempts.\
Check following things:
1. restart salt-master service on Server A [systemctl restart salt-master]
2. restart salt-minion service on Server A [systemctl restart salt-minion]
3. restart salt-minion service on Server B [ssh srvnode-2 'systemctl restart salt-minion']
4. check the status again [salt '*' test.ping]
5. if step 4 works, rerun the pre-unbox command
pre-unbox script did not run successfully, exiting." | tee -a ${LOG_FILE}
            #TODO: Retry after restarting services?
            exit 1
        fi
        echo -n "." | tee -a ${LOG_FILE}
        try=$(( $try + 1 ))
    done
    echo "Minion on Server B started successfully" | tee -a ${LOG_FILE}
}

main()
{
    time_stamp=$(date)
    echo "******** [$time_stamp] Running $0 *********" >> ${LOG_FILE}
    echo "DEBUG: Private IP address Node A: $pvt_ip_a" >> ${LOG_FILE}
    echo "DEBUG: Private IP address Node B: $pvt_ip_b" >> ${LOG_FILE}
    #1. Run cleanup 
    echo "Capturing output of `ip a` on Server A for analysis (if needed)" | tee -a ${LOG_FILE}
    ip a | tee -a ${LOG_FILE}

    echo "Capturing output of `ip a` on Server B for analysis (if needed)" | tee -a ${LOG_FILE}
    $ssh_cmd ${pvt_ip_b} 'ip a' | tee -a ${LOG_FILE}

    seagate_refs_cleanup

    #2. Check if the volumes are being listed on both the nodes - lsblk
    luns_availability_check

    #3. Get the controller IPs from user as an input
    #TODO: Remove this when enclosure communication is switched to in-band.
    ctrl_ip_addr_get_set

    #4. check if the hostname contains 'seagate.com', if yes, take input for new hostname
    hostname_validate_get_set

    #5. update hostnames in local dns (/etc/hosts)
    dns_update_hostnames

    #6. Update hostnames in ssh config files
    ssh_config_update

    #7. Update minion files on both the nodes to point master to server A hostname
    minion_config_update

    #TODO:
    # 1. Print and confirm the hostnames from user.
    # 2. Add idempotency changes for ssh_config_update() & minion_config_update()
    #    1. Check if hostname_A and hostname_B matches with existing entries 
    #    2. If yes, skip updating the config files
    # 3. Update existing entry in /etc/hosts
}

main "$@"